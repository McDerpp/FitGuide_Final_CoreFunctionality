{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Bidirectional, BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard,ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "import copy\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "# import mediapipe as mp\n",
    "from IPython.display import clear_output\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from tensorflow_model_optimization.quantization.keras import quantize_model\n",
    "from collections import Counter\n",
    "import random as rand\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def txt_pre_process(txt_file,label,simplify=False,simplify_level=14 ):\n",
    "    label_array = []\n",
    "    temp_feature_data = []\n",
    "    temp_sequence_data = []\n",
    "    batch_data = []\n",
    "\n",
    "    with open(str(txt_file), 'r') as file:\n",
    "\n",
    "        for line in file:\n",
    "            values = line.strip().split('|')\n",
    "\n",
    "            temp_feature_data = []\n",
    "\n",
    "            for value in values:\n",
    "                float_value = str(value)\n",
    "\n",
    "                #FIRST PART OF THE SEQUENCE\n",
    "                if float_value == 'START':\n",
    "                    temp_sequence_data=[]\n",
    "\n",
    "                elif float_value == 'END':\n",
    "                    batch_data.append(temp_sequence_data)\n",
    "                    label_array.append(label)\n",
    "\n",
    "\n",
    "                elif float_value != '' and float_value != 'START':\n",
    "                    if simplify:\n",
    "                        float_value = round(float(value),simplify_level)\n",
    "                    else:\n",
    "                        float_value = float(value)\n",
    "                    temp_feature_data.append(float_value)\n",
    "\n",
    "            if temp_feature_data!=[]:\n",
    "                temp_sequence_data.append(temp_feature_data)\n",
    "\n",
    "    label_array = np.array(label_array)\n",
    "    return [batch_data,label_array]\n",
    "\n",
    "#--------------------------------------------------------------------------- paddingV1 --------------------------------------------------------------------------------\n",
    "# padding can be improved probably...by using sequence\n",
    "# minor issue:\n",
    "# > is whether sequences had exceeded the intended number of sequences but is still right (it was performed right but slower(by an acceptable margin)) - not resolved\n",
    "#    = temporary fix was just to truncate everything if it had exceeded the intended number of sequence for the sake of running it for now\n",
    "#    = a reliable solution in theory could be that to randomly truncate in between the first and end sequence, in this way relevant data can be captured\n",
    "def padding(pre_processed_input,optional_maxLength=0):\n",
    "    padded_sequences = []\n",
    "    if optional_maxLength != 0:\n",
    "        max_length = optional_maxLength\n",
    "    else:\n",
    "        max_length = max(len(sequence) for sequence in pre_processed_input)\n",
    "\n",
    "    for sequence in pre_processed_input:\n",
    "        padding_length = max_length - len(sequence)\n",
    "        if padding_length >= 0:\n",
    "            padded_sequence = np.pad(sequence, ((0, padding_length), (0, 0)), mode='constant')\n",
    "\n",
    "        else:\n",
    "            padded_sequence = sequence[:max_length]\n",
    "        padded_sequences.append(padded_sequence)\n",
    "    padded_sequences = np.array(padded_sequences)\n",
    "\n",
    "    return padded_sequences\n",
    "\n",
    "#--------------------------------------------------------------------------- paddingV1 --------------------------------------------------------------------------------\n",
    "\n",
    "# this is to merge correct executions and wrong executions and randomize their input and label\n",
    "# positions of input and its corresponding label are the same\n",
    "# introducing noise/wrong input makes the model more robust\n",
    "def concatenate_randomize_batches(base_input,base_label,concat_input,concat_label):\n",
    "    combined_inputs = np.concatenate((base_input,concat_input), axis = 0)\n",
    "    combined_label = np.concatenate((base_label,concat_label), axis = 0)\n",
    "    indices = np.random.permutation(len(combined_inputs))\n",
    "    randomized_inputs = combined_inputs[indices]\n",
    "    randomized_label = combined_label[indices]\n",
    "    return [randomized_inputs,randomized_label]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def tally_sequence(sequence_array):\n",
    "    tally_number = []\n",
    "    tally_ctr = []\n",
    "\n",
    "    for x in sequence_array:\n",
    "        temp = len(x)\n",
    "        if temp not in tally_number:\n",
    "            tally_number.append(temp)\n",
    "            tally_ctr.append(1)\n",
    "        else:\n",
    "            for y in range(len(tally_number)) :\n",
    "                if temp == tally_number[y]:\n",
    "                    tally_ctr[y] = tally_ctr[y] + 1\n",
    "\n",
    "    tally_max = 0\n",
    "    tally_number_arranged = []\n",
    "    tally_ctr_arranged = []\n",
    "\n",
    "    for x in range(len(tally_number)):\n",
    "        # print(len(tally_ctr))\n",
    "        tally_max = max(tally_ctr)\n",
    "        for y in range(len(tally_number)):\n",
    "            if tally_ctr[y] == tally_max:\n",
    "                tally_number_arranged.append(tally_number[y])\n",
    "                tally_ctr_arranged.append(tally_ctr[y])\n",
    "                tally_ctr.pop(y)\n",
    "                tally_number.pop(y)\n",
    "                break\n",
    "\n",
    "    total_ctr = 0\n",
    "    for x in tally_ctr:\n",
    "        total_ctr = total_ctr + x\n",
    "\n",
    "\n",
    "    for x in range(len(tally_number_arranged)):\n",
    "        print(tally_number_arranged[x],'-->',tally_ctr_arranged[x])\n",
    "\n",
    "\n",
    "# outlier detection and removal (currently being used)\n",
    "def common_length_sequence(sequences_array,threshold = 5):\n",
    "    temp = []\n",
    "\n",
    "    data = [len(seq) for seq in sequences_array]\n",
    "    data_frequency = Counter(data)\n",
    "    most_common_data = data_frequency.most_common()\n",
    "    outlier_frequencies = [value for value, freq in data_frequency.items() if freq < threshold]\n",
    "    most_common_values = [value for value, freq in most_common_data if freq >= threshold]\n",
    "\n",
    "    print(\"Most Common Data Points:\", most_common_values)\n",
    "    print(\"Outlier Frequencies:\", outlier_frequencies)\n",
    "\n",
    "    for x in sequences_array:\n",
    "        if len(x) in most_common_values:\n",
    "            temp.append(x)\n",
    "    print('-------------------applied frequency outlier detection-------------------')\n",
    "    print(\"original num -> \", len(sequences_array))\n",
    "    print(\"current num -> \", len(temp))\n",
    "    print(\"removed num -> \", len(sequences_array) - len(temp))\n",
    "    return temp\n",
    "\n",
    "# outlier detection and removal (currently being used)\n",
    "def apply_z_score(sequences_array,z_score_threshold = 1):\n",
    "    data_points = []\n",
    "    included_datapoints = []\n",
    "    updated_sequences =[]\n",
    "\n",
    "    for x in sequences_array:\n",
    "        temp = len(x)\n",
    "        if temp not in data_points:\n",
    "            data_points.append(temp)\n",
    "\n",
    "    data = np.array(data_points)\n",
    "    mean_value = np.mean(data)\n",
    "    standard_deviation = np.std(data)\n",
    "    z_scores = (data - mean_value) / standard_deviation\n",
    "    for x in range(len(z_scores)):\n",
    "        if np.abs(z_scores[x]) <= z_score_threshold:\n",
    "            included_datapoints.append(data[x])\n",
    "\n",
    "\n",
    "    for x in sequences_array:\n",
    "        if len(x) in included_datapoints:\n",
    "            updated_sequences.append(x)\n",
    "    print('-------------------applied z-score outlier detection-------------------')\n",
    "    print(\"datapoints included -> \", included_datapoints)\n",
    "    print(\"original num -> \", len(sequences_array))\n",
    "    print(\"current num -> \", len(updated_sequences))\n",
    "    print(\"removed num -> \", len(sequences_array) - len(updated_sequences))\n",
    "\n",
    "    return updated_sequences\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def paddingV2(sequences_array_input,optional_maxlength = 0):\n",
    "    sequences_array = copy.deepcopy(sequences_array_input)\n",
    "\n",
    "\n",
    "    output = []\n",
    "    max_length = 0\n",
    "    if optional_maxlength == 0:\n",
    "        max_length = max(len(sequence) for sequence in sequences_array)\n",
    "        expanded_max_length = int(max_length+ ((max_length) * .10))\n",
    "    else:\n",
    "        expanded_max_length = optional_maxlength\n",
    "\n",
    "    # sequence = np.array(sequences_array)\n",
    "\n",
    "\n",
    "    padding_length_before = 0\n",
    "    padding_length_after = 0\n",
    "\n",
    "    for seq in sequences_array:\n",
    "        # print(seq)\n",
    "        for x in range(expanded_max_length-len(seq)+1):\n",
    "            padding_length_before = x\n",
    "            padding_length_after = expanded_max_length - len(seq) - x\n",
    "            padded_sequence = np.pad(seq, ((padding_length_before, padding_length_after),(0,0)), mode='constant')\n",
    "            output.append(padded_sequence)\n",
    "\n",
    "            # print(padded_sequence)\n",
    "    print('------------------------applied paddingV2------------------------')\n",
    "    print('max_length -> ', max_length)\n",
    "    print('expanded_max_length -> ', expanded_max_length)\n",
    "    print('original num set of sequences -> ', len(sequences_array))\n",
    "    print('final num set of sequences -> ', len(output))\n",
    "\n",
    "    output = np.array(output)\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def convert_tf_to_tflite(tf_model,input_shape,test_dataset,name,id_number,validation_loss,validation_accuracy):\n",
    "  model = tf.keras.models.load_model(tf_model)\n",
    "\n",
    "  run_model = tf.function(lambda x: model(x))\n",
    "  # This is important, let's fix the input size.\n",
    "  BATCH_SIZE = input_shape[0]\n",
    "  STEPS = input_shape[1]\n",
    "  INPUT_SIZE = input_shape[2]\n",
    "  concrete_func = run_model.get_concrete_function(\n",
    "      tf.TensorSpec([BATCH_SIZE, STEPS, INPUT_SIZE], model.inputs[0].dtype))\n",
    "\n",
    "  # model directory.\n",
    "  MODEL_DIR = \"keras_lstm\"\n",
    "  model.save(MODEL_DIR, save_format=\"tf\", signatures=concrete_func)\n",
    "\n",
    "  converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR)\n",
    "  tflite_model = converter.convert()\n",
    "\n",
    "\n",
    "  # Run the model with TensorFlow to get expected results.\n",
    "  TEST_CASES = 10\n",
    "\n",
    "  # Run the model with TensorFlow Lite\n",
    "  interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "  interpreter.allocate_tensors()\n",
    "  input_details = interpreter.get_input_details()\n",
    "  output_details = interpreter.get_output_details()\n",
    "\n",
    "  for i in range(TEST_CASES):\n",
    "    expected = model.predict(test_dataset[i:i+1])\n",
    "    interpreter.set_tensor(input_details[0][\"index\"], test_dataset[i:i+1, :, :])\n",
    "    interpreter.invoke()\n",
    "    result = interpreter.get_tensor(output_details[0][\"index\"])\n",
    "\n",
    "    # Assert if the result of TFLite model is consistent with the TF model.\n",
    "    np.testing.assert_almost_equal(expected, result, decimal=5)\n",
    "    print(\"Done. The result of TensorFlow matches the result of TensorFlow Lite.\")\n",
    "\n",
    "    interpreter.reset_all_variables()\n",
    "\n",
    "\n",
    "\n",
    "  temp = 'converted_model_'\n",
    "\n",
    "  temp3 = temp + str(name) + id_number + \"(loss_\"+ str(round(validation_loss,3)) +\")\" + \"(acc_\"+  str(round(validation_accuracy,3 )) + \")\" + '.tflite'\n",
    "  print(\"path is -->\",temp3 )\n",
    "  # Save the TFLite model to a file\n",
    "  with open(temp3, \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "  return temp3\n",
    "  # with open(\"converted_model.tflite\", \"wb\") as f:\n",
    "  #     f.write(tflite_model)\n",
    "\n",
    "\n",
    "# this data augmentation replaces padded index with random inputs\n",
    "def populate_0_input(correct_data_input,noise_data_input):\n",
    "    correct_data = copy.deepcopy(correct_data_input)\n",
    "    noise_data = copy.deepcopy(noise_data_input)\n",
    "\n",
    "    print(len(correct_data))\n",
    "    index = 10\n",
    "    temp = []\n",
    "    temp_compilation = []\n",
    "    ctr = 0\n",
    "    rand_modifier =0\n",
    "\n",
    "    for set_sequence in tqdm(correct_data, desc=\"populate_0_input\", leave=True):\n",
    "        rand_modifier = rand.randint(0,len(noise_data))\n",
    "\n",
    "        for x in range(len(set_sequence)):\n",
    "            ctr = ctr + 1\n",
    "            if set_sequence[x][0] == 0:\n",
    "                temp.append(noise_data[rand_modifier-1][rand.randint(0,len(noise_data[rand_modifier-1])-1)])\n",
    "\n",
    "            else:\n",
    "                temp.append(set_sequence[x])\n",
    "\n",
    "        temp_compilation.append(temp)\n",
    "        temp =[]\n",
    "\n",
    "\n",
    "    return temp_compilation\n",
    "\n",
    "\n",
    "\n",
    "class CustomEarlyStopping(Callback):\n",
    "  def __init__(self, accuracy_threshold=0.95, loss_threshold=0.10):\n",
    "      super(CustomEarlyStopping, self).__init__()\n",
    "      self.accuracy_threshold = accuracy_threshold\n",
    "      self.loss_threshold = loss_threshold\n",
    "\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "      if logs is None:\n",
    "          logs = {}\n",
    "\n",
    "      if logs.get('val_accuracy') is None or logs.get('val_loss') is None:\n",
    "          return\n",
    "\n",
    "      if logs.get('val_accuracy') >= self.accuracy_threshold and logs.get('val_loss') <= self.loss_threshold:\n",
    "          self.model.stop_training = True\n",
    "          print(f\"\\nTraining stopped as validation accuracy reached {logs.get('val_accuracy'):.4f} \"\n",
    "                f\"and validation loss reached {logs.get('val_loss'):.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CustomEarlyStoppingV2(Callback):\n",
    "    def __init__(self, accuracy_threshold=0.95, loss_threshold=0.10, patience=None):\n",
    "        super(CustomEarlyStopping, self).__init__()\n",
    "        self.accuracy_threshold = accuracy_threshold\n",
    "        self.loss_threshold = loss_threshold\n",
    "        self.patience = patience\n",
    "        self.wait = 0  # Counter for patience\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "\n",
    "        if logs.get('val_accuracy') is None or logs.get('val_loss') is None:\n",
    "            return\n",
    "\n",
    "        if logs.get('val_accuracy') >= self.accuracy_threshold and logs.get('val_loss') <= self.loss_threshold:\n",
    "            self.model.stop_training = True\n",
    "            print(f\"\\nTraining stopped as validation accuracy reached {logs.get('val_accuracy'):.4f} \"\n",
    "                  f\"and validation loss reached {logs.get('val_loss'):.4f}\")\n",
    "        else:\n",
    "            if self.patience is not None and self.patience > 0:\n",
    "                current_val_loss = logs.get('val_loss')\n",
    "                if current_val_loss is not None:\n",
    "                    if current_val_loss < self.best:\n",
    "                        self.best = current_val_loss\n",
    "                        self.wait = 0\n",
    "                    else:\n",
    "                        self.wait += 1\n",
    "                        if self.wait >= self.patience:\n",
    "                            self.model.stop_training = True\n",
    "                            print(f\"\\nTraining stopped due to lack of improvement for {self.patience} epochs.\")\n",
    "                            self.restore_best_weights()\n",
    "\n",
    "# this data augmentation augments data to simulate if the sequence were follow,whether there is unecessary movements in between.\n",
    "# this would also reinforce the model to detect certain action that were classified as correct instead of incorrect\n",
    "# this augmentation is augmenting each sequence of a list of sequence\n",
    "# 1 set of sequence(example-> 1 push up):\n",
    "# [seq1,seq2,seq3,seq4]\n",
    "# [seq1,seq2,NOISE_SEQ3,seq4]\n",
    "def data_aug_sensitivity(sequence_array_list_input,noise_sequence_list_input,num_data_aug = 3,num_aug_in_1_seq = 3,noise_seq_len = 2):\n",
    "  sequence_array_list = copy.deepcopy(sequence_array_list_input)\n",
    "  noise_sequence_list = copy.deepcopy(noise_sequence_list_input)\n",
    "\n",
    "  compile = []\n",
    "  temp_seq = []\n",
    "  temp_storage = []\n",
    "  temp_rand = []\n",
    "  num = 0\n",
    "  ctr1111 = 0\n",
    "  temp_rand2 = 0\n",
    "  temp_rand3 = 0\n",
    "\n",
    "# per sequences\n",
    "  for sequence in tqdm(sequence_array_list, desc=\"data_aug_seq_sensitivity\", leave=True):\n",
    "    # loops for the number of data augmentation per sequence\n",
    "    for ctr in range(num_data_aug):\n",
    "      # loops for the amount of number of augmentation in the sequence(loops to get random index)\n",
    "      while len(temp_rand)!=num_aug_in_1_seq:\n",
    "        num = rand.randint(0,len(sequence)-1)\n",
    "        if num in temp_rand:\n",
    "          continue\n",
    "        else:\n",
    "          temp_rand.append(num)\n",
    "\n",
    "      #actual augmentation of the sequence\n",
    "      temp_seq = sequence.copy()\n",
    "      # store in a temp variable and to be edited\n",
    "\n",
    "      # number of augmentation to be done in a sequence\n",
    "      for ctr1 in range(len(temp_rand)):\n",
    "\n",
    "        # number of sequence to be expanded(index + number of noise_seq_len)\n",
    "        for ctr2 in range(noise_seq_len):\n",
    "          temp_rand2 = rand.randint(0,len(noise_sequence_list)-1)\n",
    "          temp_rand3 = rand.randint(0,len(noise_sequence_list[0])-1)\n",
    "\n",
    "          if (temp_rand[ctr1] + ctr2) < len(temp_seq):\n",
    "            temp_seq[temp_rand[ctr1] + ctr2] = noise_sequence_list[temp_rand2][temp_rand3]\n",
    "\n",
    "          else:\n",
    "            continue\n",
    "\n",
    "      # for test1 in sequence\n",
    "      compile.append(temp_seq)\n",
    "      temp_seq = []\n",
    "      temp_rand = []\n",
    "\n",
    "\n",
    "  return compile\n",
    "\n",
    "\n",
    "def data_aug_seq_sensitivity(sequence_array_list_input,num_to_aug=2,num_coor_edit=3,num_sequence_edit=2):\n",
    "  sequence_array_list = copy.deepcopy(sequence_array_list_input)\n",
    "\n",
    "  compile = []\n",
    "  temp = []\n",
    "  rand_coor = []\n",
    "\n",
    "\n",
    "  for ctr in tqdm(range(num_to_aug), desc=\"data_aug_coor_sensitivity\", leave=True):\n",
    "    for sequence in sequence_array_list:\n",
    "      for ctr3 in range(num_sequence_edit):\n",
    "        what_sequence = rand.randint(0,len(sequence)-1)\n",
    "        for ctr2 in range(num_coor_edit):\n",
    "          what_coor = rand.randint(0,len(sequence[0])-1)\n",
    "          rand_coor = rand.randint(0,9999999999)\n",
    "          rand_coor = rand_coor / (10 ** len(str(rand_coor)))\n",
    "          print(sequence[what_sequence][what_coor],'---',rand_coor)\n",
    "          sequence[what_sequence][what_coor]=rand_coor\n",
    "      compile.append(sequence)\n",
    "  return compile\n",
    "\n",
    "\n",
    "def data_aug_coor_sensitivity(sequence_array_list_input,num_coor_edit=45,num_sequence_edit=8):\n",
    "  sequence_array_list = copy.deepcopy(sequence_array_list_input)\n",
    "\n",
    "  compile = []\n",
    "  temp = []\n",
    "  rand_coor = []\n",
    "  temp_seq = []\n",
    "\n",
    "\n",
    "\n",
    "  # for ctr in tqdm(range(num_to_aug), desc=\"data_aug_coor_sensitivity\", leave=True):\n",
    "  for sequence in sequence_array_list:\n",
    "    print('------------------------------------------------------------------------')\n",
    "    # print(len(sequence))\n",
    "    temp_seq = sequence.copy()\n",
    "    for ctr3 in range(num_sequence_edit):\n",
    "      what_sequence = rand.randint(0,len(sequence)-1)\n",
    "      num_coor_edit = rand.randint(int(num_coor_edit*.65),num_coor_edit)\n",
    "      print(\"----\")\n",
    "\n",
    "      for ctr2 in range(num_coor_edit):\n",
    "        what_coor = rand.randint(0,len(sequence[0])-1)\n",
    "        # rand_coor = rand.randint(0,9999999999)\n",
    "        rand_coor = rand.randint(0,999)\n",
    "        rand_coor = rand_coor / (10 ** len(str(rand_coor)))\n",
    "        # print(temp_seq[what_sequence][what_coor],'---',rand_coor)\n",
    "        temp_seq[what_sequence][what_coor]=rand_coor\n",
    "    compile.append(sequence)\n",
    "  return compile\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_training_history(history):\n",
    "    epochs = range(1, len(history.history['loss']) + 1)\n",
    "\n",
    "    # Plotting training and validation loss\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history.history['loss'], label='Training Loss')\n",
    "    plt.plot(epochs, history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plotting training and validation accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(epochs, history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# After the training loop\n",
    "# plot_training_history(history)\n",
    "\n",
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "# coordinate adversary sensetivity1\n",
    "# augments data to make model more acceptable to flaws\n",
    "\n",
    "def coorAdvSens1(sequence_array_list_input,num_aug = 6,sensetivity = 0.025,sensetivity_optional_range = 0):\n",
    "  sequence_array_list = copy.deepcopy(sequence_array_list_input)\n",
    "\n",
    "  temp_allowance = 0.5\n",
    "\n",
    "  tempExecution = []\n",
    "  tempSequence = []\n",
    "  tempFinalList = []\n",
    "  temp = 0\n",
    "\n",
    "\n",
    "  for ctr in range(num_aug):\n",
    "    for execution in sequence_array_list_input:\n",
    "      for sequence in execution:\n",
    "        for individual_coor in sequence:\n",
    "          if sensetivity_optional_range == 0:\n",
    "            temp = round(rand.uniform(individual_coor - sensetivity, individual_coor + sensetivity), 8)\n",
    "            # print(individual_coor, \"<---->\", temp)\n",
    "\n",
    "          else:\n",
    "            determiner = rand.randint(0,1)\n",
    "            if determiner == 0:\n",
    "              temp = round(rand.uniform(individual_coor + sensetivity + temp_allowance,individual_coor + sensetivity+sensetivity_optional_range ), 8)\n",
    "              # print(inidividual_coor, \"<---->\", temp)\n",
    "            else:\n",
    "              temp = round(rand.uniform(individual_coor - sensetivity - sensetivity_optional_range,individual_coor - sensetivity - temp_allowance ), 8)\n",
    "              # print(inidividual_coor, \"<---->\", temp)\n",
    "\n",
    "\n",
    "\n",
    "            # if temp >= 1.0 :\n",
    "            #   temp = 1\n",
    "            # elif temp <= 0.00000000:\n",
    "            #   temp =  0.00000001\n",
    "\n",
    "\n",
    "          tempSequence.append(temp)\n",
    "        temp = []\n",
    "        temp_x_value = []\n",
    "        temp_y_value = []\n",
    "\n",
    "        temp_x_value_normalized = []\n",
    "        temp_y_value_normalized = []\n",
    "\n",
    "        tempSequence_normalized = []\n",
    "\n",
    "        tempCtr = 0\n",
    "        for coordinates in range(int(len(tempSequence)/2)):\n",
    "          temp_x_value.append(tempSequence[tempCtr])\n",
    "          tempCtr = tempCtr + 1\n",
    "          temp_y_value.append(tempSequence[tempCtr])\n",
    "          tempCtr = tempCtr + 1\n",
    "\n",
    "\n",
    "\n",
    "        x_min_value = min(temp_x_value)\n",
    "        x_max_value = max(temp_x_value)\n",
    "\n",
    "        y_min_value = min(temp_y_value)\n",
    "        y_max_value = max(temp_y_value)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        for content in temp_x_value:\n",
    "\n",
    "          scaled_values = (content - x_min_value) / (x_max_value - x_min_value)\n",
    "          temp_x_value_normalized.append(scaled_values)\n",
    "          if scaled_values < 0:\n",
    "            print(\"--------------------------\")\n",
    "            print(\"x_min_value --->\",x_min_value)\n",
    "            print(\"x_max_value --->\",x_max_value)\n",
    "            print(\"contett --->\" ,content)\n",
    "\n",
    "\n",
    "        for content in temp_y_value:\n",
    "          scaled_values = (content - y_min_value) / (y_max_value - y_min_value)\n",
    "\n",
    "          temp_y_value_normalized.append(scaled_values)\n",
    "          # if scaled_values < 0:\n",
    "          #   print(y_min_value,y_max_value,content)\n",
    "\n",
    "        x_value_ctr = 0\n",
    "        y_value_ctr = 0\n",
    "\n",
    "        for ctr in range(66):\n",
    "          if ctr % 2 == 0:\n",
    "            tempSequence_normalized.append(temp_x_value_normalized[x_value_ctr])\n",
    "            x_value_ctr = x_value_ctr + 1\n",
    "          else:\n",
    "            tempSequence_normalized.append(temp_y_value_normalized[y_value_ctr])\n",
    "            y_value_ctr = y_value_ctr + 1\n",
    "\n",
    "\n",
    "\n",
    "        tempExecution.append(tempSequence)\n",
    "        tempSequence_normalized = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        tempSequence = []\n",
    "\n",
    "      tempFinalList.append(tempExecution)\n",
    "      tempExecution = []\n",
    "  tempFinalList.extend(sequence_array_list)\n",
    "  print('\\n-------------------applied coorAdvSens1-------------------')\n",
    "  print('initial len --> ',len(sequence_array_list))\n",
    "  print('final len --> ',len(tempFinalList))\n",
    "\n",
    "\n",
    "  return tempFinalList\n",
    "\n",
    "\n",
    "\n",
    "def sequenceAdvSens1(sequence_array_list_input,num_aug = 2,sensetivity = 0.1):\n",
    "  sequence_array_list = copy.deepcopy(sequence_array_list_input)\n",
    "\n",
    "  allowance_temp = 0.5\n",
    "\n",
    "  base_num_seq_aug = int(len(sequence_array_list[0]) * sensetivity)\n",
    "  print(\"base num aug ---> \",base_num_seq_aug)\n",
    "  if base_num_seq_aug < 1:\n",
    "    base_num_seq_aug = 1\n",
    "\n",
    "  temp_rand_seq_index = []\n",
    "  rand_index = 0\n",
    "  tempSequence = []\n",
    "  tempFinalList = []\n",
    "  replacement = []\n",
    "  temp = 0\n",
    "\n",
    "  for x in range(66):\n",
    "    replacement.append(0)\n",
    "\n",
    "  for ctr in tqdm(range(num_aug), desc=\"sequenceAdvSens1\"):\n",
    "    for execution in sequence_array_list:\n",
    "      temp_execution = execution\n",
    "      for ctr in range(base_num_seq_aug):\n",
    "        while rand_index in temp_rand_seq_index:\n",
    "          rand_index = rand.randrange(1, len(temp_execution))\n",
    "        temp_rand_seq_index.append(rand_index)\n",
    "\n",
    "      for index in temp_rand_seq_index:\n",
    "        temp_execution[index] = replacement\n",
    "      tempFinalList.append(temp_execution)\n",
    "      temp_rand_seq_index = []\n",
    "\n",
    "\n",
    "  tempFinalList.extend(sequence_array_list)\n",
    "\n",
    "  print('-------------------applied coorAdvSens1-------------------')\n",
    "  print('initial len --> ',len(sequence_array_list))\n",
    "  print('final len --> ',len(tempFinalList))\n",
    "  return tempFinalList\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load and preprocess input image\n",
    "import numpy as np\n",
    "\n",
    "# testing data leakage\n",
    "def checking_inputs(correct_input,wrong_input,model_path_param):\n",
    "  temp_padding_array = []\n",
    "\n",
    "  for x in range(66):\n",
    "    temp_padding_array.append(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  base_data = correct_input\n",
    "  base_data_noise = wrong_input\n",
    "  print(len(base_data))\n",
    "  print(len(base_data_noise))\n",
    "\n",
    "  compiled_data = []\n",
    "  compiled_data.append(base_data)\n",
    "  compiled_data.append(base_data_noise)\n",
    "\n",
    "  temp_seq = []\n",
    "  temp_execution = []\n",
    "\n",
    "  ctr_compiled_data = 0\n",
    "  correct_ctr = 0\n",
    "  wrong_ctr = 0\n",
    "\n",
    "  correct_threshod = 0.9\n",
    "\n",
    "\n",
    "  # Load TFLite model\n",
    "  interpreter = tf.lite.Interpreter(model_path=model_path_param)\n",
    "  interpreter.allocate_tensors()\n",
    "  input_tensor_index = interpreter.get_input_details()[0]['index']\n",
    "  shape_needed = interpreter.get_input_details()[0][\"shape\"][1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# convertin to float32\n",
    "  for content in compiled_data:\n",
    "    temp_final = []\n",
    "\n",
    "    if ctr_compiled_data == 0:\n",
    "      print(\"----------------correct inputs----------------\")\n",
    "    else:\n",
    "      print(\"----------------wrong inputs----------------\")\n",
    "    for execution in content:\n",
    "      while len(execution) > shape_needed:\n",
    "        execution.pop()\n",
    "\n",
    "      while len(execution) < shape_needed:\n",
    "        execution.append(temp_padding_array)\n",
    "\n",
    "      for sequence in execution:\n",
    "        for coordinates in sequence:\n",
    "          temp_seq.append(np.float32(coordinates))\n",
    "        temp_execution.append(temp_seq)\n",
    "        temp_seq = []\n",
    "      temp_final.append(temp_execution)\n",
    "      temp_execution=[]\n",
    "\n",
    "    print(\"--->?\",len(temp_final))\n",
    "    for temp_final_content in temp_final:\n",
    "      temp_inference = temp_final_content\n",
    "\n",
    "      input_data = temp_final_content\n",
    "      input_data = np.array(input_data)\n",
    "      input_data = np.reshape(input_data, (1, 5, 66))\n",
    "\n",
    "      # Set input tensor\n",
    "      interpreter.set_tensor(input_tensor_index, input_data)\n",
    "\n",
    "      # Run inference\n",
    "      interpreter.invoke()\n",
    "\n",
    "      # Get output tensor\n",
    "      output_tensor_index = interpreter.get_output_details()[0]['index']\n",
    "      output_data = interpreter.get_tensor(output_tensor_index)\n",
    "      print(output_data)\n",
    "\n",
    "\n",
    "\n",
    "      # print(ctr_compiled_data)\n",
    "      if ctr_compiled_data == 0 and output_data >= correct_threshod:\n",
    "        correct_ctr = correct_ctr + 1\n",
    "\n",
    "      elif ctr_compiled_data == 1 and output_data < correct_threshod:\n",
    "        wrong_ctr = wrong_ctr + 1\n",
    "\n",
    "    ctr_compiled_data = ctr_compiled_data + 1\n",
    "\n",
    "\n",
    "  print(\"correct_input --> \",correct_ctr,\"/\",len(base_data))\n",
    "  print(\"wrong_input --> \",wrong_ctr,\"/\",len(base_data_noise))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'txt_pre_process' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\CLARK\\Documents\\fitguidef\\pythonScriptTrain\\trainScript.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/CLARK/Documents/fitguidef/pythonScriptTrain/trainScript.ipynb#W2sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m   convert_tf_to_tflite(\u001b[39m'\u001b[39m\u001b[39m/content/testingModel\u001b[39m\u001b[39m'\u001b[39m,[\u001b[39m1\u001b[39m,\u001b[39mlen\u001b[39m(aug3[\u001b[39m0\u001b[39m]),\u001b[39mlen\u001b[39m(aug3[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m])], X_test,\u001b[39m'\u001b[39m\u001b[39mwhole_model\u001b[39m\u001b[39m'\u001b[39m,id_num,val_loss,val_accuracy)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/CLARK/Documents/fitguidef/pythonScriptTrain/trainScript.ipynb#W2sZmlsZQ%3D%3D?line=119'>120</a>\u001b[0m \u001b[39m# uncomment this to run training model as a whole(this means that this model can determine the motion as a whole but not specify what part if correct or not)\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/CLARK/Documents/fitguidef/pythonScriptTrain/trainScript.ipynb#W2sZmlsZQ%3D%3D?line=120'>121</a>\u001b[0m streamlined_process(\u001b[39m'\u001b[39;49m\u001b[39m/content/drive/MyDrive/Colab Notebooks/correct_new_2.txt\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39m/content/drive/MyDrive/Colab Notebooks/wrong_new_2.txt\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32md:\\CLARK\\Documents\\fitguidef\\pythonScriptTrain\\trainScript.ipynb Cell 3\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/CLARK/Documents/fitguidef/pythonScriptTrain/trainScript.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstreamlined_process\u001b[39m(correct_execution,noise_data):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/CLARK/Documents/fitguidef/pythonScriptTrain/trainScript.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m   id_num \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(rand\u001b[39m.\u001b[39mrandint(\u001b[39m1000\u001b[39m,\u001b[39m9999\u001b[39m))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/CLARK/Documents/fitguidef/pythonScriptTrain/trainScript.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m   base_data \u001b[39m=\u001b[39m txt_pre_process(correct_execution,\u001b[39m1\u001b[39m,\u001b[39mFalse\u001b[39;00m,\u001b[39m4\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/CLARK/Documents/fitguidef/pythonScriptTrain/trainScript.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m   base_data_noise \u001b[39m=\u001b[39m txt_pre_process(noise_data,\u001b[39m0\u001b[39m,\u001b[39mFalse\u001b[39;00m,\u001b[39m4\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/CLARK/Documents/fitguidef/pythonScriptTrain/trainScript.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m   loop \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'txt_pre_process' is not defined"
     ]
    }
   ],
   "source": [
    "# this is as a whole exercise\n",
    "def streamlined_process(correct_execution,noise_data):\n",
    "  id_num = str(rand.randint(1000,9999))\n",
    "  base_data = txt_pre_process(correct_execution,1,False,4)\n",
    "  base_data_noise = txt_pre_process(noise_data,0,False,4)\n",
    "\n",
    "  loop = 0\n",
    "\n",
    "  temp_correct_start=[]\n",
    "  temp_correct_wrong=[]\n",
    "\n",
    "  temp_wrong_store_train = []\n",
    "\n",
    "  data = base_data[0]\n",
    "  data_noise = base_data_noise[0]\n",
    "\n",
    "  best_val_loss = float('inf')\n",
    "  best_val_accuracy = 0.0\n",
    "  best_model = None\n",
    "\n",
    "\n",
    "\n",
    "  print('')\n",
    "  print('----------------------------correct data augmentation ----------------------------')\n",
    "  aug = common_length_sequence(data)\n",
    "  aug2 = apply_z_score(aug,1)\n",
    "  aug3 = paddingV2(aug2)\n",
    "  aug4 = populate_0_input(aug3,data_noise)\n",
    "  aug4 = np.array(aug4)\n",
    "  aug4 = aug4.reshape(-1,len(aug3[0]),len(aug3[0][0]))\n",
    "  combined_inputs = np.concatenate((aug4,aug3), axis = 0)\n",
    "  combined_inputs = aug4\n",
    "  print('concat -> ', len(combined_inputs))\n",
    "\n",
    "\n",
    "  print('')\n",
    "  print('----------------------------data noise data augmentation ----------------------------')\n",
    "  aug_noise_data1 = paddingV2(data_noise,len(aug3[0]))\n",
    "  aug_noise_data2 = populate_0_input(aug_noise_data1,data_noise)\n",
    "\n",
    "# testing...\n",
    "  # data_aug_coor_sensitivity()\n",
    "\n",
    "# original\n",
    "  # aug_noise_data3 = aug_noise_data2[0:len(combined_inputs)]\n",
    "\n",
    "# testing(temporary)\n",
    "  aug_noise_data3 = aug_noise_data2[:]\n",
    "\n",
    "  aug_noise_data4 = np.array(aug_noise_data3)\n",
    "\n",
    "  aug_noise_data5 = aug_noise_data4.reshape(-1,len(aug_noise_data4[0]),len(aug_noise_data4[0][0]))\n",
    "\n",
    "# testing(temporary)\n",
    "  aug_noise_data7 = data_aug_coor_sensitivity(aug_noise_data5,num_sequence_edit=int(len(aug_noise_data5[0])*.70))\n",
    "\n",
    "  # aug_noise_data6 = data_aug_sensitivity(aug4,aug_noise_data5,1,2,2)\n",
    "  aug_noise_data6 = data_aug_sensitivity(aug4,aug_noise_data5,1,2,int(len(combined_inputs)*.35))\n",
    "\n",
    "# original\n",
    "  # aug_noise_data5 = np.concatenate((aug_noise_data6,aug_noise_data5), axis = 0)\n",
    "\n",
    "# testing(temporary)\n",
    "  aug_noise_data5 = np.concatenate((aug_noise_data6,aug_noise_data5,aug_noise_data7), axis = 0)\n",
    "\n",
    "\n",
    "  print('aug_noise_data5--->',aug_noise_data5.shape)\n",
    "\n",
    "\n",
    "  loop = int(len(aug_noise_data5)/len(combined_inputs))\n",
    "\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(len(aug4[0]), return_sequences=True, activation='relu',  input_shape=(len(aug4[0]), len(aug4[0][0]))))\n",
    "  model.add(LSTM(len(aug4[0]) + int(len(aug4[0]) - int(len(aug4[0]) * .4)), return_sequences=True,  activation='relu'))\n",
    "  model.add(Bidirectional(LSTM(len(aug4[0]) - int(len(aug4[0]) - int(len(aug4[0]) * .4)), return_sequences=True, dropout=0.3, recurrent_dropout=0.3, activation='relu')))\n",
    "  model.add(LSTM(len(aug4[0]) - int(len(aug4[0]) - int(len(aug4[0]) * .4)), return_sequences=False, activation='relu'))\n",
    "  # model.add(BatchNormalization())\n",
    "  model.add(Dense(len(aug4[0]) - int(len(aug4[0]) - int(len(aug4[0]) * .3)), activation='relu'))\n",
    "  model.add(Dense(1,activation='sigmoid'))\n",
    "  custom_early_stopping = CustomEarlyStopping(accuracy_threshold=0.97, loss_threshold=0.05)\n",
    "  model.compile(optimizer = 'Adam' , loss = 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  for x in range(loop):\n",
    "    temp_wrong_store_train = aug_noise_data5[x*len(combined_inputs):len(combined_inputs)*(x+1)]\n",
    "\n",
    "    aug3_label = np.ones(len(combined_inputs))\n",
    "    aug_noise_label = np.zeros(len(temp_wrong_store_train))\n",
    "\n",
    "    rand_batches=concatenate_randomize_batches(combined_inputs,aug3_label,temp_wrong_store_train,aug_noise_label)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(rand_batches[0], rand_batches[1], test_size=0.3, random_state=42)\n",
    "\n",
    "    # history=model.fit(X_train, y_train, epochs=200, batch_size =256  , validation_data=(X_test, y_test), callbacks=[custom_early_stopping])\n",
    "    history=model.fit(X_train, y_train, epochs=200, batch_size =256  , validation_data=(X_test, y_test), callbacks=[custom_early_stopping])\n",
    "\n",
    "\n",
    "    val_loss = min(history.history['val_loss'])\n",
    "    val_accuracy = max(history.history['val_accuracy'])\n",
    "    temp_wrong_store_train=[]\n",
    "\n",
    "\n",
    "    if val_loss < best_val_loss and val_accuracy > best_val_accuracy:\n",
    "      best_model = model.get_weights()\n",
    "      best_val_loss = val_loss\n",
    "      best_val_accuracy = val_accuracy\n",
    "\n",
    "    # testing(temporary)\n",
    "    model.set_weights(best_model)\n",
    "\n",
    "  X_train = X_train.astype(np.float32)\n",
    "  X_test = X_test.astype(np.float32)\n",
    "  model.save('testingModel')\n",
    "  convert_tf_to_tflite('/content/testingModel',[1,len(aug3[0]),len(aug3[0][0])], X_test,'whole_model',id_num,val_loss,val_accuracy)\n",
    "\n",
    "\n",
    "\n",
    "# uncomment this to run training model as a whole(this means that this model can determine the motion as a whole but not specify what part if correct or not)\n",
    "streamlined_process('/content/drive/MyDrive/Colab Notebooks/correct_new_2.txt','/content/drive/MyDrive/Colab Notebooks/wrong_new_2.txt')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is for the individual models\n",
    "# this is experimental\n",
    "def streamlined_process(correct_execution,noise_data,loop=5):\n",
    "  id_num = str(rand.randint(1000,9999))\n",
    "  base_data = txt_pre_process(correct_execution,1,False,4)\n",
    "  base_data_noise = txt_pre_process(noise_data,0,False,4)\n",
    "\n",
    "  temp_correct_start=[]\n",
    "  temp_correct_wrong=[]\n",
    "\n",
    "  data = base_data[0]\n",
    "  data_noise = base_data_noise[0]\n",
    "\n",
    "\n",
    "\n",
    "  print('')\n",
    "  print('----------------------------correct data augmentation ----------------------------')\n",
    "  aug = common_length_sequence(data)\n",
    "  aug2 = apply_z_score(aug,1)\n",
    "  aug3 = paddingV2(aug2)\n",
    "  aug4 = populate_0_input(aug3,data_noise)\n",
    "  aug4 = np.array(aug4)\n",
    "  aug4 = aug4.reshape(-1,len(aug3[0]),len(aug3[0][0]))\n",
    "  combined_inputs = np.concatenate((aug4,aug3), axis = 0)\n",
    "  combined_inputs= aug4\n",
    "  print('concat -> ', len(combined_inputs))\n",
    "\n",
    "\n",
    "\n",
    "  print('')\n",
    "  print('----------------------------data noise data augmentation ----------------------------')\n",
    "  aug_noise_data1 = paddingV2(data_noise,len(aug3[0]))\n",
    "  aug_noise_data2 = populate_0_input(aug_noise_data1,data_noise)\n",
    "  aug_noise_data3 = aug_noise_data2[0:len(combined_inputs)]\n",
    "  aug_noise_data4 = np.array(aug_noise_data3)\n",
    "  aug_noise_data5 = aug_noise_data4.reshape(-1,len(aug_noise_data4[0]),len(aug_noise_data4[0][0]))\n",
    "\n",
    "  aug_noise_data6 = data_aug_sensitivity(aug4,aug_noise_data5,1,1,int(len(combined_inputs)*.35))\n",
    "  aug_noise_data5 = np.concatenate((aug_noise_data6,aug_noise_data5), axis = 0)\n",
    "  # aug_noise_data6 = data_aug_coor_sensitivity(aug4)\n",
    "  # aug_noise_data5 = np.concatenate((aug_noise_data6,aug_noise_data5), axis = 0)\n",
    "\n",
    "\n",
    "  correct_data_set = []\n",
    "  noise_data_set = []\n",
    "  rand_batches = []\n",
    "\n",
    "  print('testtesttesttest=======>',len(aug_noise_data5))\n",
    "\n",
    "\n",
    "  for x in range(14):\n",
    "    correct_data_set.append([])\n",
    "    noise_data_set.append([])\n",
    "    rand_batches.append([])\n",
    "\n",
    "\n",
    "  for exercise in combined_inputs:\n",
    "\n",
    "    left_upper_arm = []\n",
    "    left_lower_arm = []\n",
    "    left_hand = []\n",
    "    right_upper_arm = []\n",
    "    right_lower_arm = []\n",
    "    right_hand = []\n",
    "    left_upper_leg = []\n",
    "    left_lower_leg = []\n",
    "    left_feet = []\n",
    "    right_upper_leg = []\n",
    "    right_lower_leg = []\n",
    "    right_feet = []\n",
    "    head =[]\n",
    "    body = []\n",
    "\n",
    "\n",
    "\n",
    "#------------------------ generating the correct input of certain part------------------------------\n",
    "    for sequence in exercise:\n",
    "      # 11,13\n",
    "      left_upper_arm.append([sequence[22],sequence[23],sequence[26],sequence[27]])\n",
    "      # 13,15\n",
    "      left_lower_arm.append([sequence[26],sequence[27],sequence[30],sequence[31]])\n",
    "      # 15,17,19,21\n",
    "      left_hand.append([sequence[30],sequence[31],sequence[34],sequence[35],sequence[38],sequence[39],sequence[42],sequence[43]])\n",
    "\n",
    "      # 12,14\n",
    "      right_upper_arm.append([sequence[24],sequence[25],sequence[28],sequence[29]])\n",
    "      # 14,16\n",
    "      right_lower_arm.append([sequence[28],sequence[29],sequence[32],sequence[33]])\n",
    "      # 16,18,20,22\n",
    "      right_hand.append([sequence[32],sequence[33],sequence[36],sequence[37],sequence[40],sequence[41],sequence[44],sequence[45]])\n",
    "\n",
    "      # 23,25\n",
    "      left_upper_leg.append([sequence[46],sequence[47],sequence[50],sequence[51]])\n",
    "      # 25,27\n",
    "      left_lower_leg.append([sequence[50],sequence[51],sequence[54],sequence[55]])\n",
    "      # 27,29,31\n",
    "      left_feet.append([sequence[54],sequence[55],sequence[58],sequence[59],sequence[62],sequence[63]])\n",
    "\n",
    "      # 24,26\n",
    "      right_upper_leg.append([sequence[48],sequence[49],sequence[52],sequence[53]])\n",
    "      # 26,28\n",
    "      right_lower_leg.append([sequence[52],sequence[53],sequence[56],sequence[57]])\n",
    "      # 28,30,32\n",
    "      right_feet.append([sequence[56],sequence[57],sequence[60],sequence[61],sequence[64],sequence[65]])\n",
    "\n",
    "      # 11,12,23,24\n",
    "      body.append([sequence[22],sequence[23],sequence[24],sequence[25],sequence[46],sequence[47],sequence[48],sequence[49]])\n",
    "      # 7,8,9,10\n",
    "      head.append([sequence[14],sequence[15],sequence[16],sequence[17],sequence[18],sequence[19],sequence[20],sequence[21]])\n",
    "\n",
    "\n",
    "    correct_data_set[0].append(left_upper_arm)\n",
    "    correct_data_set[1].append(left_lower_arm)\n",
    "    correct_data_set[2].append(left_hand)\n",
    "\n",
    "    correct_data_set[3].append(right_upper_arm)\n",
    "    correct_data_set[4].append(right_lower_arm)\n",
    "    correct_data_set[5].append(right_hand)\n",
    "\n",
    "    correct_data_set[6].append(left_upper_leg)\n",
    "    correct_data_set[7].append(left_lower_leg)\n",
    "    correct_data_set[8].append(left_feet)\n",
    "\n",
    "    correct_data_set[9].append(right_upper_leg)\n",
    "    correct_data_set[10].append(right_lower_leg)\n",
    "    correct_data_set[11].append(right_feet)\n",
    "\n",
    "    correct_data_set[12].append(body)\n",
    "    correct_data_set[13].append(head)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  for x in range(len(correct_data_set)):\n",
    "    correct_data_set[x] = np.array(correct_data_set[x])\n",
    "\n",
    "  # for x in range(len(noise_data_set)):\n",
    "  #   noise_data_set[x] = np.array(noise_data_set[x])\n",
    "\n",
    "  print('len of correct data set ---->',len(correct_data_set[0]))\n",
    "  print('len of noise data set ---->',len(noise_data_set[0]))\n",
    "\n",
    "  correct_data_set_label = np.ones(len(combined_inputs))\n",
    "  noise_data_set_label = np.zeros(len(aug_noise_data5))\n",
    "\n",
    "  # for x in range(len(correct_data_set)):\n",
    "  #   rand_batches[x]=concatenate_randomize_batches(correct_data_set[x],correct_data_set_label,noise_data_set[x],noise_data_set_label)\n",
    "\n",
    "\n",
    "\n",
    "  loop = int(len(aug_noise_data5)/len(combined_inputs))\n",
    "\n",
    "\n",
    "  data_set_name=['left_upper_arm','left_lower_arm','left_hand','right_upper_arm','right_lower_arm','right_hand','left_upper_leg','left_lower_leg','left_feet','right_upper_leg','right_lower_leg','right_feet','head','body']\n",
    "  for x in range(len(data_set_name)-1):\n",
    "    print('progress -> ',x,'/',len(data_set_name)-1)\n",
    "\n",
    "\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_accuracy = 0.0\n",
    "    best_model = None\n",
    "\n",
    "    for y in range(loop):\n",
    "\n",
    "      noise_data_set=[]\n",
    "\n",
    "\n",
    "    #------------------------------ generating noise ---------------------------------------\n",
    "      for exercise in aug_noise_data5[len(correct_data_set[x])*y:len(correct_data_set[x]) + (len(correct_data_set[x])*y)]:\n",
    "      # for exercise in aug_noise_data5[0:450]:\n",
    "        # print('batchin noise ->',len(correct_data_set[x])*y,'<==--==>',len(correct_data_set[x]) + (len(correct_data_set[x])*y))\n",
    "\n",
    "\n",
    "        left_upper_arm = []\n",
    "        left_lower_arm = []\n",
    "        left_hand = []\n",
    "        right_upper_arm = []\n",
    "        right_lower_arm = []\n",
    "        right_hand = []\n",
    "        left_upper_leg = []\n",
    "        left_lower_leg = []\n",
    "        left_feet = []\n",
    "        right_upper_leg = []\n",
    "        right_lower_leg = []\n",
    "        right_feet = []\n",
    "        head =[]\n",
    "        body = []\n",
    "\n",
    "\n",
    "\n",
    "        for sequence in exercise:\n",
    "          # 11,13\n",
    "          # print(sequence[22])\n",
    "          if x == 0:\n",
    "            left_upper_arm.append([sequence[22],sequence[23],sequence[26],sequence[27]])\n",
    "          # 13,15\n",
    "          if x == 1:\n",
    "            left_lower_arm.append([sequence[26],sequence[27],sequence[30],sequence[31]])\n",
    "          # 15,17,19,21\n",
    "          if x == 2:\n",
    "            left_hand.append([sequence[30],sequence[31],sequence[34],sequence[35],sequence[38],sequence[39],sequence[42],sequence[43]])\n",
    "\n",
    "          # 12,14\n",
    "          if x == 3:\n",
    "            right_upper_arm.append([sequence[24],sequence[25],sequence[28],sequence[29]])\n",
    "          # 14,16\n",
    "          if x == 4:\n",
    "            right_lower_arm.append([sequence[28],sequence[29],sequence[32],sequence[33]])\n",
    "          # 16,18,20,22\n",
    "          if x == 5:\n",
    "            right_hand.append([sequence[32],sequence[33],sequence[36],sequence[37],sequence[40],sequence[41],sequence[44],sequence[45]])\n",
    "\n",
    "          # 23,25\n",
    "          if x == 6:\n",
    "            left_upper_leg.append([sequence[46],sequence[47],sequence[50],sequence[51]])\n",
    "          # 25,27\n",
    "          if x == 7:\n",
    "            left_lower_leg.append([sequence[50],sequence[51],sequence[54],sequence[55]])\n",
    "          # 27,29,31\n",
    "          if x == 8:\n",
    "            left_feet.append([sequence[54],sequence[55],sequence[58],sequence[59],sequence[62],sequence[63]])\n",
    "\n",
    "          # 24,26\n",
    "          if x == 9:\n",
    "            right_upper_leg.append([sequence[48],sequence[49],sequence[52],sequence[53]])\n",
    "          # 26,28\n",
    "          if x == 10:\n",
    "            right_lower_leg.append([sequence[52],sequence[53],sequence[56],sequence[57]])\n",
    "          # 28,30,32\n",
    "          if x == 11:\n",
    "            right_feet.append([sequence[56],sequence[57],sequence[60],sequence[61],sequence[64],sequence[65]])\n",
    "\n",
    "          # 11,12,23,24\n",
    "          if x == 12:\n",
    "            body.append([sequence[22],sequence[23],sequence[24],sequence[25],sequence[46],sequence[47],sequence[48],sequence[49]])\n",
    "          # 7,8,9,10\n",
    "          if x == 13:\n",
    "            head.append([sequence[14],sequence[15],sequence[16],sequence[17],sequence[18],sequence[19],sequence[20],sequence[21]])\n",
    "\n",
    "        if x == 0:\n",
    "          noise_data_set.append(left_upper_arm)\n",
    "        if x == 1:\n",
    "          noise_data_set.append(left_lower_arm)\n",
    "        if x == 2:\n",
    "          noise_data_set.append(left_hand)\n",
    "\n",
    "        if x == 3:\n",
    "          noise_data_set.append(right_upper_arm)\n",
    "        if x == 4:\n",
    "          noise_data_set.append(right_lower_arm)\n",
    "        if x == 5:\n",
    "          noise_data_set.append(right_hand)\n",
    "\n",
    "        if x == 6:\n",
    "          noise_data_set.append(left_upper_leg)\n",
    "        if x == 7:\n",
    "          noise_data_set.append(left_lower_leg)\n",
    "        if x == 8:\n",
    "          noise_data_set.append(left_feet)\n",
    "\n",
    "\n",
    "        if x == 9:\n",
    "          noise_data_set.append(right_upper_leg)\n",
    "        if x == 10:\n",
    "          noise_data_set.append(right_lower_leg)\n",
    "        if x == 11:\n",
    "          noise_data_set.append(right_feet)\n",
    "\n",
    "\n",
    "        if x == 12:\n",
    "          noise_data_set.append(body)\n",
    "        if x == 13:\n",
    "          noise_data_set.append(head)\n",
    "\n",
    "# ===================================================================================================================================================\n",
    "\n",
    "      print('correct->',len(correct_data_set[x]),'  incorrect->',len(noise_data_set[x]))\n",
    "\n",
    "      rand_batches=concatenate_randomize_batches(correct_data_set[x],correct_data_set_label,noise_data_set,noise_data_set_label)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "      print('loop -> ',y,'/',loop)\n",
    "      X_train, X_test, y_train, y_test = train_test_split(rand_batches[0], rand_batches[1], test_size=0.2, random_state=42)\n",
    "\n",
    "      # history = model.fit(X_train[y*len(rand_batches[x][0]):len(rand_batches[x][0])*(y+1)], y_train[y*len(rand_batches[x][1]):len(rand_batches[x][1])*(y+1)], epochs=200, batch_size =128 , validation_data=(X_test[y*len(rand_batches[x][0]):len(rand_batches[x][0])*(y+1)], y_test[y*len(rand_batches[x][1]):len(rand_batches[x][1])*(y+1)]), callbacks=[custom_early_stopping])\n",
    "\n",
    "      if y == 0:\n",
    "        model_base_modifier = 10\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(model_base_modifier+len(aug4[0]), return_sequences=True, activation='relu',  input_shape=(len(correct_data_set[x][0]), len(correct_data_set[x][0][0]))))\n",
    "        model.add(LSTM(model_base_modifier+len(aug4[0]) + int(len(aug4[0]) - int(len(aug4[0]) * .4)), return_sequences=True,  activation='relu' ))\n",
    "        model.add(Bidirectional(LSTM(model_base_modifier+len(aug4[0]) - int(len(aug4[0]) - int(len(aug4[0]) * .5)), return_sequences=True, dropout=0.4, recurrent_dropout=0.4, activation='relu')))\n",
    "        model.add(LSTM(model_base_modifier+len(aug4[0]) - int(len(aug4[0]) - int(len(aug4[0]) * .4)), return_sequences=False,  activation='relu'))\n",
    "        # model.add(BatchNormalization())\n",
    "        model.add(Dense(model_base_modifier+len(aug4[0]) - int(len(aug4[0]) - int(len(aug4[0]) * .4)), activation='relu'))\n",
    "\n",
    "        model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "        custom_early_stopping = CustomEarlyStopping(accuracy_threshold=0.97, loss_threshold=0.05)\n",
    "        model.compile(optimizer = 'Adam' , loss = 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "      history = model.fit(X_train, y_train, epochs=150, batch_size =128 , validation_data=(X_test, y_test), callbacks=[custom_early_stopping])\n",
    "\n",
    "      val_loss = min(history.history['val_loss'])\n",
    "      val_accuracy = max(history.history['val_accuracy'])\n",
    "\n",
    "      if val_loss < best_val_loss and val_accuracy > best_val_accuracy:\n",
    "        best_model = model.get_weights()\n",
    "        best_val_loss = val_loss\n",
    "        best_val_accuracy = val_accuracy\n",
    "\n",
    "      model.set_weights(best_model)\n",
    "\n",
    "\n",
    "\n",
    "    model.save('testingModel')\n",
    "\n",
    "\n",
    "    # X_train = X_train[x].astype(np.float32)\n",
    "    # X_test = X_test[x].astype(np.float32)\n",
    "\n",
    "    # convert_tf_to_tflite('/content/testingModel',[1,len(X_train[0]),len(X_train[0][0])], X_test,data_set_name[x],id_num,val_loss,val_accuracy)\n",
    "    # convert_tf_to_tflite('/content/testingModel',[1,len(correct_data_set[x][0]),len(correct_data_set[x][0][0])], X_test,data_set_name[x],id_num,val_loss,val_accuracy)\n",
    "\n",
    "\n",
    "    X_train = X_train.astype(np.float32)\n",
    "    X_test = X_test.astype(np.float32)\n",
    "    model.save('testingModel')\n",
    "\n",
    "    print('param 1 ->',len(noise_data_set[0]) )\n",
    "    print('param 1 ->',len(noise_data_set[0][0]))\n",
    "    print('current x ->',x)\n",
    "    convert_tf_to_tflite('/content/testingModel',[1,len(noise_data_set[0]),len(noise_data_set[0][0])], X_test,data_set_name[x],id_num,best_val_loss,best_val_accuracy)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    clear_output(wait=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# uncomment this to run the individual landmarks training training(this means that it can determine individual parts of the body if spefic parts are incorrect or correct)\n",
    "# streamlined_process('/content/drive/MyDrive/Colab Notebooks/correct_new_2.txt','/content/drive/MyDrive/Colab Notebooks/wrong_new_2.txt')\n",
    "3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\CLARK\\Documents\\fitguidef\\pythonScriptTrain\\trainScript.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CLARK/Documents/fitguidef/pythonScriptTrain/trainScript.ipynb#W4sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m black_background \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(image_size, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39muint8)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CLARK/Documents/fitguidef/pythonScriptTrain/trainScript.ipynb#W4sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(temp1) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CLARK/Documents/fitguidef/pythonScriptTrain/trainScript.ipynb#W4sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39m# Convert coordinates to integers only when drawing\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/CLARK/Documents/fitguidef/pythonScriptTrain/trainScript.ipynb#W4sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     start_point \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\u001b[39mmap\u001b[39;49m(\u001b[39mint\u001b[39;49m, skeletal_coordinates[i]))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CLARK/Documents/fitguidef/pythonScriptTrain/trainScript.ipynb#W4sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     end_point \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mint\u001b[39m, skeletal_coordinates[i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m]))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CLARK/Documents/fitguidef/pythonScriptTrain/trainScript.ipynb#W4sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     cv2\u001b[39m.\u001b[39mline(black_background, start_point, end_point, (\u001b[39m255\u001b[39m, \u001b[39m255\u001b[39m, \u001b[39m255\u001b[39m), \u001b[39m2\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Mock skeletal coordinates (replace this with your actual data)\n",
    "base_data = txt_pre_process('firstExerciseFitguide(correct).txt',1,False,4)\n",
    "\n",
    "\n",
    "\n",
    "# skeletal_coordinates = np.array([[50, 50], [50, 100], [100, 100], [100, 50]])\n",
    "skeletal_coordinates = base_data[0][0][0]\n",
    "tempCtr = 0\n",
    "temp1=[]\n",
    "\n",
    "for x in range(33):\n",
    "    temp1.append([skeletal_coordinates[tempCtr],skeletal_coordinates[tempCtr+1]])\n",
    "    tempCtr = tempCtr + 2\n",
    "\n",
    "\n",
    "image_size = (200, 200, 3)\n",
    "black_background = np.zeros(image_size, dtype=np.uint8)\n",
    "\n",
    "for i in range(len(temp1) - 1):\n",
    "    # Convert coordinates to integers only when drawing\n",
    "    start_point = tuple(map(int, skeletal_coordinates[i]))\n",
    "    end_point = tuple(map(int, skeletal_coordinates[i + 1]))\n",
    "\n",
    "    cv2.line(black_background, start_point, end_point, (255, 255, 255), 2)\n",
    "\n",
    "# Create a black background image\n",
    "\n",
    "\n",
    "# Draw lines on the black background based on skeletal coordinates\n",
    "# for i in range(len(skeletal_coordinates) - 1):\n",
    "#     cv2.line(black_background, tuple(skeletal_coordinates[i]), tuple(skeletal_coordinates[i + 1]), (255, 255, 255), 2)\n",
    "\n",
    "# Display the image\n",
    "cv2.imshow('Skeletal Image', black_background)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.59652624\n",
      "0.02667674\n",
      "0.58421618\n",
      "0.00168468\n",
      "0.59267123\n",
      "0.00101575\n",
      "0.59962849\n",
      "0.0\n",
      "0.54127683\n",
      "0.00120362\n",
      "0.51851573\n",
      "0.00065259\n",
      "0.49172178\n",
      "0.00029337\n",
      "0.57099637\n",
      "0.00786889\n",
      "0.41215745\n",
      "0.01199619\n",
      "0.59732561\n",
      "0.04739432\n",
      "0.54323311\n",
      "0.0482398\n",
      "0.73056635\n",
      "0.13932864\n",
      "0.19639467\n",
      "0.14795567\n",
      "0.86508043\n",
      "0.31035553\n",
      "0.08993779\n",
      "0.32513696\n",
      "0.9544216\n",
      "0.47049416\n",
      "0.03687205\n",
      "0.48484136\n",
      "1.0\n",
      "0.52071026\n",
      "0.0\n",
      "0.53177139\n",
      "0.95038218\n",
      "0.52437813\n",
      "0.04261503\n",
      "0.53557986\n",
      "0.91838214\n",
      "0.50537482\n",
      "0.07422279\n",
      "0.51945688\n",
      "0.66757966\n",
      "0.4674395\n",
      "0.34961041\n",
      "0.47650171\n",
      "0.67116715\n",
      "0.71119287\n",
      "0.38583393\n",
      "0.70923163\n",
      "0.69751153\n",
      "0.91788413\n",
      "0.45857493\n",
      "0.9158063\n",
      "0.67752156\n",
      "0.94017888\n",
      "0.49370885\n",
      "0.93859664\n",
      "0.75245725\n",
      "0.99957656\n",
      "0.3469685\n",
      "1.0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 42\u001b[0m\n\u001b[0;32m     40\u001b[0m temp1 \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m33\u001b[39m):\n\u001b[1;32m---> 42\u001b[0m     temp1\u001b[38;5;241m.\u001b[39mappend([\u001b[38;5;28mint\u001b[39m(\u001b[43mtemp_H_seq\u001b[49m\u001b[43m[\u001b[49m\u001b[43my\u001b[49m\u001b[43m]\u001b[49m[tempCtr]\u001b[38;5;241m*\u001b[39mwidth),\u001b[38;5;28mint\u001b[39m(temp_H_seq[y][tempCtr\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39mheight)])\n\u001b[0;32m     43\u001b[0m     tempCtr \u001b[38;5;241m=\u001b[39m tempCtr \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(temp1))\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "height = 500\n",
    "width = 200\n",
    "image_size = (height, width, 3)\n",
    "\n",
    "highestSeq = []\n",
    "temp_H_seq = []\n",
    "\n",
    "black_background = np.zeros(image_size, dtype=np.uint8)\n",
    "\n",
    "\n",
    "base_data = txt_pre_process('firstExerciseFitguide(correct).txt', 1, False, 4)\n",
    "base_data = base_data[0][40]\n",
    "temp1=[]\n",
    "tempCtr = 0\n",
    "# print(\"len-->\",len(base_data))\n",
    "# print(len(base_data[0]))\n",
    "\n",
    "# for sequenceList in base_data:\n",
    "#     print(sequenceList)\n",
    "    \n",
    "#     print(sequenceList,\"----\",highestSeq)\n",
    "#     if len(sequenceList) >= highestSeq:\n",
    "#         highestSeq = sequenceList\n",
    "\n",
    "for sequenceList in base_data[0]:\n",
    "    print(sequenceList)\n",
    "    # if len(sequenceList) == highestSeq:\n",
    "    #     temp_H_seq.append(sequenceList)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for y in range(len(base_data[0])):\n",
    "    black_background = np.zeros(image_size, dtype=np.uint8)\n",
    "\n",
    "    tempCtr =0\n",
    "    temp1 = []\n",
    "    for x in range(33):\n",
    "        temp1.append([int(temp_H_seq[y][tempCtr]*width),int(temp_H_seq[y][tempCtr+1]*height)])\n",
    "        tempCtr = tempCtr + 2\n",
    "        print(len(temp1))\n",
    "\n",
    "    # left foot\n",
    "    cv2.line(black_background, temp1[32], temp1[30], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[30], temp1[28], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[32], temp1[28], (255, 255, 255), 2)\n",
    "\n",
    "    # right Foot\n",
    "    cv2.line(black_background, temp1[27], temp1[31], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[31], temp1[29], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[29], temp1[27], (255, 255, 255), 2)\n",
    "\n",
    "    # left leg\n",
    "    cv2.line(black_background, temp1[28], temp1[26], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[26], temp1[24], (255, 255, 255), 2)\n",
    "\n",
    "    # right leg\n",
    "    cv2.line(black_background, temp1[27], temp1[25], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[25], temp1[23], (255, 255, 255), 2)\n",
    "\n",
    "    # body\n",
    "    cv2.line(black_background, temp1[24], temp1[23], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[23], temp1[11], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[11], temp1[12], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[12], temp1[24], (255, 255, 255), 2)\n",
    "\n",
    "    # left arm\n",
    "    cv2.line(black_background, temp1[12], temp1[14], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[14], temp1[16], (255, 255, 255), 2)\n",
    "\n",
    "    # left hand\n",
    "    cv2.line(black_background, temp1[16], temp1[18], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[18], temp1[20], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[20], temp1[16], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[16], temp1[22], (255, 255, 255), 2)\n",
    "\n",
    "    # right arm\n",
    "    cv2.line(black_background, temp1[11], temp1[13], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[13], temp1[15], (255, 255, 255), 2)\n",
    "\n",
    "    # right hand\n",
    "    cv2.line(black_background, temp1[15], temp1[17], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[17], temp1[19], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[19], temp1[15], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[15], temp1[21], (255, 255, 255), 2)\n",
    "\n",
    "    # face\n",
    "    cv2.line(black_background, temp1[7], temp1[3], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[3], temp1[2], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[2], temp1[1], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[0], temp1[4], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[0], temp1[4], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[4], temp1[5], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[5], temp1[6], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[6], temp1[7], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[7], temp1[8], (255, 255, 255), 2)\n",
    "\n",
    "    # mouth\n",
    "    cv2.line(black_background, temp1[10], temp1[9], (255, 255, 255), 2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    cv2.imshow('Skeletal Image', black_background)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "[[116, 16]]\n",
      "[[116, 16], [138, 12]]\n",
      "[[116, 16], [138, 12], [134, 34]]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(base_data)):\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m33\u001b[39m):\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;66;03m# temp1.append([int(base_data[y][tempCtr]*width),int(base_data[y][tempCtr+1]*height)])\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m         temp1\u001b[38;5;241m.\u001b[39mappend([\u001b[38;5;28mint\u001b[39m(\u001b[43mbase_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43my\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtempCtr\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m width), \u001b[38;5;28mint\u001b[39m(base_data[y][tempCtr][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m height)])\n\u001b[0;32m     23\u001b[0m         tempCtr \u001b[38;5;241m=\u001b[39m tempCtr \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;28mprint\u001b[39m(temp1)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "height = 500\n",
    "width = 200\n",
    "image_size = (height, width, 3)\n",
    "\n",
    "black_background = np.zeros(image_size, dtype=np.uint8)\n",
    "\n",
    "\n",
    "base_data = txt_pre_process('firstExerciseFitguide(correct).txt', 1, False, 4)\n",
    "base_data = base_data[0]\n",
    "temp1=[]\n",
    "tempCtr = 0\n",
    "\n",
    "print(len(base_data))\n",
    "\n",
    "\n",
    "for y in range(len(base_data)):\n",
    "    for x in range(33):\n",
    "        # temp1.append([int(base_data[y][tempCtr]*width),int(base_data[y][tempCtr+1]*height)])\n",
    "        temp1.append([int(base_data[y][tempCtr][0] * width), int(base_data[y][tempCtr][1] * height)])\n",
    "\n",
    "        tempCtr = tempCtr + 2\n",
    "        print(temp1)\n",
    "\n",
    "    # left foot\n",
    "    cv2.line(black_background, temp1[32], temp1[30], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[30], temp1[28], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[32], temp1[28], (255, 255, 255), 2)\n",
    "\n",
    "    # right Foot\n",
    "    cv2.line(black_background, temp1[27], temp1[31], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[31], temp1[29], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[29], temp1[27], (255, 255, 255), 2)\n",
    "\n",
    "    # left leg\n",
    "    cv2.line(black_background, temp1[28], temp1[26], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[26], temp1[24], (255, 255, 255), 2)\n",
    "\n",
    "    # right leg\n",
    "    cv2.line(black_background, temp1[27], temp1[25], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[25], temp1[23], (255, 255, 255), 2)\n",
    "\n",
    "    # body\n",
    "    cv2.line(black_background, temp1[24], temp1[23], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[23], temp1[11], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[11], temp1[12], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[12], temp1[24], (255, 255, 255), 2)\n",
    "\n",
    "    # left arm\n",
    "    cv2.line(black_background, temp1[12], temp1[14], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[14], temp1[16], (255, 255, 255), 2)\n",
    "\n",
    "    # left hand\n",
    "    cv2.line(black_background, temp1[17], temp1[18], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[18], temp1[20], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[20], temp1[16], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[16], temp1[23], (255, 255, 255), 2)\n",
    "\n",
    "    # right arm\n",
    "    cv2.line(black_background, temp1[11], temp1[13], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[13], temp1[15], (255, 255, 255), 2)\n",
    "\n",
    "    # right hand\n",
    "    cv2.line(black_background, temp1[15], temp1[17], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[17], temp1[19], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[19], temp1[15], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[15], temp1[21], (255, 255, 255), 2)\n",
    "\n",
    "    # face\n",
    "    cv2.line(black_background, temp1[7], temp1[3], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[3], temp1[2], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[2], temp1[1], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[0], temp1[4], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[0], temp1[4], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[4], temp1[5], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[5], temp1[6], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[6], temp1[7], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[7], temp1[8], (255, 255, 255), 2)\n",
    "\n",
    "    # mouth\n",
    "    cv2.line(black_background, temp1[10], temp1[9], (255, 255, 255), 2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    cv2.imshow('Skeletal Image', black_background)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "height = 500\n",
    "width = 200\n",
    "image_size = (height, width, 3)\n",
    "\n",
    "\n",
    "# Assuming you have a 1D array of 66 coordinates\n",
    "base_data = txt_pre_process('firstExerciseFitguide(correct).txt', 1, False, 4)\n",
    "\n",
    "base_data = base_data[0]\n",
    "\n",
    "# for sequence in base_data[0]:\n",
    "#     for coordinates in sequence:\n",
    "#         # print(coordinates)\n",
    "\n",
    "check_execution = 0\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "for sequence in range(len(base_data[check_execution])-1):\n",
    "    temp1 = []\n",
    "    black_background = np.zeros(image_size, dtype=np.uint8)\n",
    "    \n",
    "#     # sequence\n",
    "    for coordinates in range(len(base_data[check_execution][sequence])-1):\n",
    "\n",
    "        tempCtr = 0\n",
    "\n",
    "        # coordinates\n",
    "        for x in range(33):\n",
    "            x = int(base_data[check_execution][sequence][tempCtr] * width)\n",
    "            tempCtr = tempCtr + 1\n",
    "            y = int(base_data[check_execution][sequence][tempCtr] * height)\n",
    "            tempCtr = tempCtr + 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "            temp1.append([x,y])\n",
    "\n",
    "        \n",
    "        print(\"test\")\n",
    "    # left foot\n",
    "    cv2.line(black_background, temp1[32], temp1[30], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[30], temp1[28], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[32], temp1[28], (255, 255, 255), 2)\n",
    "\n",
    "    # right Foot\n",
    "    cv2.line(black_background, temp1[27], temp1[31], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[31], temp1[29], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[29], temp1[27], (255, 255, 255), 2)\n",
    "\n",
    "    # left leg\n",
    "    cv2.line(black_background, temp1[28], temp1[26], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[26], temp1[24], (255, 255, 255), 2)\n",
    "\n",
    "    # right leg\n",
    "    cv2.line(black_background, temp1[27], temp1[25], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[25], temp1[23], (255, 255, 255), 2)\n",
    "\n",
    "    # body\n",
    "    cv2.line(black_background, temp1[24], temp1[23], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[23], temp1[11], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[11], temp1[12], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[12], temp1[24], (255, 255, 255), 2)\n",
    "\n",
    "    # left arm\n",
    "    cv2.line(black_background, temp1[12], temp1[14], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[14], temp1[16], (255, 255, 255), 2)\n",
    "\n",
    "    # left hand\n",
    "    cv2.line(black_background, temp1[17], temp1[18], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[18], temp1[20], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[20], temp1[16], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[16], temp1[23], (255, 255, 255), 2)\n",
    "\n",
    "    # right arm\n",
    "    cv2.line(black_background, temp1[11], temp1[13], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[13], temp1[15], (255, 255, 255), 2)\n",
    "\n",
    "    # right hand\n",
    "    cv2.line(black_background, temp1[15], temp1[17], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[17], temp1[19], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[19], temp1[15], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[15], temp1[21], (255, 255, 255), 2)\n",
    "\n",
    "    # face\n",
    "    cv2.line(black_background, temp1[7], temp1[3], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[3], temp1[2], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[2], temp1[1], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[0], temp1[4], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[0], temp1[4], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[4], temp1[5], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[5], temp1[6], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[6], temp1[7], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[7], temp1[8], (255, 255, 255), 2)\n",
    "\n",
    "    # mouth\n",
    "    cv2.line(black_background, temp1[10], temp1[9], (255, 255, 255), 2)\n",
    "\n",
    "\n",
    "\n",
    "    cv2.imshow('Skeletal Image', black_background)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------\n",
      "Interpolated Data:\n",
      "At x = 0.00, interpolated y = 1.00\n",
      "At x = 0.40, interpolated y = 1.98\n",
      "At x = 0.80, interpolated y = 1.57\n",
      "At x = 1.20, interpolated y = 1.99\n",
      "At x = 1.60, interpolated y = 3.12\n",
      "At x = 2.00, interpolated y = 2.50\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data with 5 data points\n",
    "x_values = np.array([0.0, 0.5, 1.0, 1.5, 2.0])\n",
    "y_values = np.array([1.0, 2.0, 1.5, 3.0, 2.5])\n",
    "\n",
    "# Create interpolation function\n",
    "interp_func = interp1d(x_values, y_values, kind='quadratic', fill_value='extrapolate')\n",
    "\n",
    "# Generate new x values for interpolation\n",
    "new_x_values = np.linspace(min(x_values), max(x_values), 6)  # adjust number of points as needed\n",
    "\n",
    "# for x in new_x_values:\n",
    "#     print(x)\n",
    "\n",
    "# Perform interpolation\n",
    "interpolated_y_values = interp_func(new_x_values)\n",
    "print('-----------------------------------------------')\n",
    "# for x in interpolated_y_values:\n",
    "#     print(x)\n",
    "\n",
    "# # Print the interpolated data\n",
    "print(\"Interpolated Data:\")\n",
    "for x, y in zip(new_x_values, interpolated_y_values):\n",
    "    print(f\"At x = {x:.2f}, interpolated y = {y:.2f}\")\n",
    "\n",
    "# # Plot original data\n",
    "# plt.scatter(x_values, y_values, color='blue', label='Original Data')\n",
    "\n",
    "# # Plot interpolated data\n",
    "# plt.plot(new_x_values, interpolated_y_values, color='red', label='Interpolated Data')\n",
    "\n",
    "# # Set labels and title\n",
    "# plt.xlabel('X values')\n",
    "# plt.ylabel('Y values')\n",
    "# plt.title('Linear Interpolation')\n",
    "\n",
    "# # Show legend\n",
    "# plt.legend()\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def txt_pre_process(txt_file,label,simplify=False,simplify_level=14 ):\n",
    "    label_array = []\n",
    "    temp_feature_data = []\n",
    "    temp_sequence_data = []\n",
    "    batch_data = []\n",
    "\n",
    "    with open(str(txt_file), 'r') as file:\n",
    "\n",
    "        for line in file:\n",
    "            values = line.strip().split('|')\n",
    "\n",
    "            temp_feature_data = []\n",
    "\n",
    "            for value in values:\n",
    "                float_value = str(value)\n",
    "\n",
    "                #FIRST PART OF THE SEQUENCE\n",
    "                if float_value == 'START':\n",
    "                    temp_sequence_data=[]\n",
    "\n",
    "                elif float_value == 'END':\n",
    "                    batch_data.append(temp_sequence_data)\n",
    "                    label_array.append(label)\n",
    "\n",
    "\n",
    "                elif float_value != '' and float_value != 'START':\n",
    "                    if simplify:\n",
    "                        float_value = round(float(value),simplify_level)\n",
    "                    else:\n",
    "                        float_value = float(value)\n",
    "                    temp_feature_data.append(float_value)\n",
    "\n",
    "            if temp_feature_data!=[]:\n",
    "                temp_sequence_data.append(temp_feature_data)\n",
    "\n",
    "    label_array = np.array(label_array)\n",
    "    return [batch_data,label_array]\n",
    "\n",
    "#--------------------------------------------------------------------------- paddingV1 --------------------------------------------------------------------------------\n",
    "# padding can be improved probably...by using sequence\n",
    "# minor issue:\n",
    "# > is whether sequences had exceeded the intended number of sequences but is still right (it was performed right but slower(by an acceptable margin)) - not resolved\n",
    "#    = temporary fix was just to truncate everything if it had exceeded the intended number of sequence for the sake of running it for now\n",
    "#    = a reliable solution in theory could be that to randomly truncate in between the first and end sequence, in this way relevant data can be captured\n",
    "def padding(pre_processed_input,optional_maxLength=0):\n",
    "    padded_sequences = []\n",
    "    if optional_maxLength != 0:\n",
    "        max_length = optional_maxLength\n",
    "    else:\n",
    "        max_length = max(len(sequence) for sequence in pre_processed_input)\n",
    "\n",
    "    for sequence in pre_processed_input:\n",
    "        padding_length = max_length - len(sequence)\n",
    "        if padding_length >= 0:\n",
    "            padded_sequence = np.pad(sequence, ((0, padding_length), (0, 0)), mode='constant')\n",
    "\n",
    "        else:\n",
    "            padded_sequence = sequence[:max_length]\n",
    "        padded_sequences.append(padded_sequence)\n",
    "    padded_sequences = np.array(padded_sequences)\n",
    "\n",
    "    return padded_sequences\n",
    "\n",
    "#--------------------------------------------------------------------------- paddingV1 --------------------------------------------------------------------------------\n",
    "\n",
    "# this is to merge correct executions and wrong executions and randomize their input and label\n",
    "# positions of input and its corresponding label are the same\n",
    "# introducing noise/wrong input makes the model more robust\n",
    "def concatenate_randomize_batches(base_input,base_label,concat_input,concat_label):\n",
    "    combined_inputs = np.concatenate((base_input,concat_input), axis = 0)\n",
    "    combined_label = np.concatenate((base_label,concat_label), axis = 0)\n",
    "    indices = np.random.permutation(len(combined_inputs))\n",
    "    randomized_inputs = combined_inputs[indices]\n",
    "    randomized_label = combined_label[indices]\n",
    "    return [randomized_inputs,randomized_label]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def tally_sequence(sequence_array):\n",
    "    tally_number = []\n",
    "    tally_ctr = []\n",
    "\n",
    "    for x in sequence_array:\n",
    "        temp = len(x)\n",
    "        if temp not in tally_number:\n",
    "            tally_number.append(temp)\n",
    "            tally_ctr.append(1)\n",
    "        else:\n",
    "            for y in range(len(tally_number)) :\n",
    "                if temp == tally_number[y]:\n",
    "                    tally_ctr[y] = tally_ctr[y] + 1\n",
    "\n",
    "    tally_max = 0\n",
    "    tally_number_arranged = []\n",
    "    tally_ctr_arranged = []\n",
    "\n",
    "    for x in range(len(tally_number)):\n",
    "        # print(len(tally_ctr))\n",
    "        tally_max = max(tally_ctr)\n",
    "        for y in range(len(tally_number)):\n",
    "            if tally_ctr[y] == tally_max:\n",
    "                tally_number_arranged.append(tally_number[y])\n",
    "                tally_ctr_arranged.append(tally_ctr[y])\n",
    "                tally_ctr.pop(y)\n",
    "                tally_number.pop(y)\n",
    "                break\n",
    "\n",
    "    total_ctr = 0\n",
    "    for x in tally_ctr:\n",
    "        total_ctr = total_ctr + x\n",
    "\n",
    "\n",
    "    for x in range(len(tally_number_arranged)):\n",
    "        print(tally_number_arranged[x],'-->',tally_ctr_arranged[x])\n",
    "\n",
    "\n",
    "# outlier detection and removal (currently being used)\n",
    "def common_length_sequence(sequences_array,threshold = 5):\n",
    "    temp = []\n",
    "\n",
    "    data = [len(seq) for seq in sequences_array]\n",
    "    data_frequency = Counter(data)\n",
    "    most_common_data = data_frequency.most_common()\n",
    "    outlier_frequencies = [value for value, freq in data_frequency.items() if freq < threshold]\n",
    "    most_common_values = [value for value, freq in most_common_data if freq >= threshold]\n",
    "\n",
    "    print(\"Most Common Data Points:\", most_common_values)\n",
    "    print(\"Outlier Frequencies:\", outlier_frequencies)\n",
    "\n",
    "    for x in sequences_array:\n",
    "        if len(x) in most_common_values:\n",
    "            temp.append(x)\n",
    "    print('-------------------applied frequency outlier detection-------------------')\n",
    "    print(\"original num -> \", len(sequences_array))\n",
    "    print(\"current num -> \", len(temp))\n",
    "    print(\"removed num -> \", len(sequences_array) - len(temp))\n",
    "    return temp\n",
    "\n",
    "# outlier detection and removal (currently being used)\n",
    "def apply_z_score(sequences_array,z_score_threshold = 1):\n",
    "    data_points = []\n",
    "    included_datapoints = []\n",
    "    updated_sequences =[]\n",
    "\n",
    "    for x in sequences_array:\n",
    "        temp = len(x)\n",
    "        if temp not in data_points:\n",
    "            data_points.append(temp)\n",
    "\n",
    "    data = np.array(data_points)\n",
    "    mean_value = np.mean(data)\n",
    "    standard_deviation = np.std(data)\n",
    "    z_scores = (data - mean_value) / standard_deviation\n",
    "    for x in range(len(z_scores)):\n",
    "        if np.abs(z_scores[x]) <= z_score_threshold:\n",
    "            included_datapoints.append(data[x])\n",
    "\n",
    "\n",
    "    for x in sequences_array:\n",
    "        if len(x) in included_datapoints:\n",
    "            updated_sequences.append(x)\n",
    "    print('-------------------applied z-score outlier detection-------------------')\n",
    "    print(\"datapoints included -> \", included_datapoints)\n",
    "    print(\"original num -> \", len(sequences_array))\n",
    "    print(\"current num -> \", len(updated_sequences))\n",
    "    print(\"removed num -> \", len(sequences_array) - len(updated_sequences))\n",
    "\n",
    "    return updated_sequences\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def paddingV2(sequences_array_input,optional_maxlength = 0):\n",
    "    sequences_array = copy.deepcopy(sequences_array_input)\n",
    "\n",
    "\n",
    "    output = []\n",
    "    max_length = 0\n",
    "    if optional_maxlength == 0:\n",
    "        max_length = max(len(sequence) for sequence in sequences_array)\n",
    "        expanded_max_length = int(max_length+ ((max_length) * .10))\n",
    "    else:\n",
    "        expanded_max_length = optional_maxlength\n",
    "\n",
    "    # sequence = np.array(sequences_array)\n",
    "\n",
    "\n",
    "    padding_length_before = 0\n",
    "    padding_length_after = 0\n",
    "\n",
    "    for seq in sequences_array:\n",
    "        # print(seq)\n",
    "        for x in range(expanded_max_length-len(seq)+1):\n",
    "            padding_length_before = x\n",
    "            padding_length_after = expanded_max_length - len(seq) - x\n",
    "            padded_sequence = np.pad(seq, ((padding_length_before, padding_length_after),(0,0)), mode='constant')\n",
    "            output.append(padded_sequence)\n",
    "\n",
    "            # print(padded_sequence)\n",
    "    print('------------------------applied paddingV2------------------------')\n",
    "    print('max_length -> ', max_length)\n",
    "    print('expanded_max_length -> ', expanded_max_length)\n",
    "    print('original num set of sequences -> ', len(sequences_array))\n",
    "    print('final num set of sequences -> ', len(output))\n",
    "\n",
    "    output = np.array(output)\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def convert_tf_to_tflite(tf_model,input_shape,test_dataset,name,id_number,validation_loss,validation_accuracy):\n",
    "  model = tf.keras.models.load_model(tf_model)\n",
    "\n",
    "  run_model = tf.function(lambda x: model(x))\n",
    "  # This is important, let's fix the input size.\n",
    "  BATCH_SIZE = input_shape[0]\n",
    "  STEPS = input_shape[1]\n",
    "  INPUT_SIZE = input_shape[2]\n",
    "  concrete_func = run_model.get_concrete_function(\n",
    "      tf.TensorSpec([BATCH_SIZE, STEPS, INPUT_SIZE], model.inputs[0].dtype))\n",
    "\n",
    "  # model directory.\n",
    "  MODEL_DIR = \"keras_lstm\"\n",
    "  model.save(MODEL_DIR, save_format=\"tf\", signatures=concrete_func)\n",
    "\n",
    "  converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR)\n",
    "  tflite_model = converter.convert()\n",
    "\n",
    "\n",
    "  # Run the model with TensorFlow to get expected results.\n",
    "  TEST_CASES = 10\n",
    "\n",
    "  # Run the model with TensorFlow Lite\n",
    "  interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "  interpreter.allocate_tensors()\n",
    "  input_details = interpreter.get_input_details()\n",
    "  output_details = interpreter.get_output_details()\n",
    "\n",
    "  for i in range(TEST_CASES):\n",
    "    expected = model.predict(test_dataset[i:i+1])\n",
    "    interpreter.set_tensor(input_details[0][\"index\"], test_dataset[i:i+1, :, :])\n",
    "    interpreter.invoke()\n",
    "    result = interpreter.get_tensor(output_details[0][\"index\"])\n",
    "\n",
    "    # Assert if the result of TFLite model is consistent with the TF model.\n",
    "    np.testing.assert_almost_equal(expected, result, decimal=5)\n",
    "    print(\"Done. The result of TensorFlow matches the result of TensorFlow Lite.\")\n",
    "\n",
    "    interpreter.reset_all_variables()\n",
    "\n",
    "\n",
    "\n",
    "  temp = 'converted_model_'\n",
    "\n",
    "  temp3 = temp + str(name) + id_number + \"(loss_\"+ str(round(validation_loss,3)) +\")\" + \"(acc_\"+  str(round(validation_accuracy,3 )) + \")\" + '.tflite'\n",
    "  print(\"path is -->\",temp3 )\n",
    "  # Save the TFLite model to a file\n",
    "  with open(temp3, \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "  return temp3\n",
    "  # with open(\"converted_model.tflite\", \"wb\") as f:\n",
    "  #     f.write(tflite_model)\n",
    "\n",
    "\n",
    "# this data augmentation replaces padded index with random inputs\n",
    "def populate_0_input(correct_data_input,noise_data_input):\n",
    "    correct_data = copy.deepcopy(correct_data_input)\n",
    "    noise_data = copy.deepcopy(noise_data_input)\n",
    "\n",
    "    print(len(correct_data))\n",
    "    index = 10\n",
    "    temp = []\n",
    "    temp_compilation = []\n",
    "    ctr = 0\n",
    "    rand_modifier =0\n",
    "\n",
    "    for set_sequence in tqdm(correct_data, desc=\"populate_0_input\", leave=True):\n",
    "        rand_modifier = rand.randint(0,len(noise_data))\n",
    "\n",
    "        for x in range(len(set_sequence)):\n",
    "            ctr = ctr + 1\n",
    "            if set_sequence[x][0] == 0:\n",
    "                temp.append(noise_data[rand_modifier-1][rand.randint(0,len(noise_data[rand_modifier-1])-1)])\n",
    "\n",
    "            else:\n",
    "                temp.append(set_sequence[x])\n",
    "\n",
    "        temp_compilation.append(temp)\n",
    "        temp =[]\n",
    "\n",
    "\n",
    "    return temp_compilation\n",
    "\n",
    "\n",
    "\n",
    "class CustomEarlyStopping(Callback):\n",
    "  def __init__(self, accuracy_threshold=0.95, loss_threshold=0.10):\n",
    "      super(CustomEarlyStopping, self).__init__()\n",
    "      self.accuracy_threshold = accuracy_threshold\n",
    "      self.loss_threshold = loss_threshold\n",
    "\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "      if logs is None:\n",
    "          logs = {}\n",
    "\n",
    "      if logs.get('val_accuracy') is None or logs.get('val_loss') is None:\n",
    "          return\n",
    "\n",
    "      if logs.get('val_accuracy') >= self.accuracy_threshold and logs.get('val_loss') <= self.loss_threshold:\n",
    "          self.model.stop_training = True\n",
    "          print(f\"\\nTraining stopped as validation accuracy reached {logs.get('val_accuracy'):.4f} \"\n",
    "                f\"and validation loss reached {logs.get('val_loss'):.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CustomEarlyStoppingV2(Callback):\n",
    "    def __init__(self, accuracy_threshold=0.95, loss_threshold=0.10, patience=None):\n",
    "        super(CustomEarlyStopping, self).__init__()\n",
    "        self.accuracy_threshold = accuracy_threshold\n",
    "        self.loss_threshold = loss_threshold\n",
    "        self.patience = patience\n",
    "        self.wait = 0  # Counter for patience\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "\n",
    "        if logs.get('val_accuracy') is None or logs.get('val_loss') is None:\n",
    "            return\n",
    "\n",
    "        if logs.get('val_accuracy') >= self.accuracy_threshold and logs.get('val_loss') <= self.loss_threshold:\n",
    "            self.model.stop_training = True\n",
    "            print(f\"\\nTraining stopped as validation accuracy reached {logs.get('val_accuracy'):.4f} \"\n",
    "                  f\"and validation loss reached {logs.get('val_loss'):.4f}\")\n",
    "        else:\n",
    "            if self.patience is not None and self.patience > 0:\n",
    "                current_val_loss = logs.get('val_loss')\n",
    "                if current_val_loss is not None:\n",
    "                    if current_val_loss < self.best:\n",
    "                        self.best = current_val_loss\n",
    "                        self.wait = 0\n",
    "                    else:\n",
    "                        self.wait += 1\n",
    "                        if self.wait >= self.patience:\n",
    "                            self.model.stop_training = True\n",
    "                            print(f\"\\nTraining stopped due to lack of improvement for {self.patience} epochs.\")\n",
    "                            self.restore_best_weights()\n",
    "\n",
    "# this data augmentation augments data to simulate if the sequence were follow,whether there is unecessary movements in between.\n",
    "# this would also reinforce the model to detect certain action that were classified as correct instead of incorrect\n",
    "# this augmentation is augmenting each sequence of a list of sequence\n",
    "# 1 set of sequence(example-> 1 push up):\n",
    "# [seq1,seq2,seq3,seq4]\n",
    "# [seq1,seq2,NOISE_SEQ3,seq4]\n",
    "def data_aug_sensitivity(sequence_array_list_input,noise_sequence_list_input,num_data_aug = 3,num_aug_in_1_seq = 3,noise_seq_len = 2):\n",
    "  sequence_array_list = copy.deepcopy(sequence_array_list_input)\n",
    "  noise_sequence_list = copy.deepcopy(noise_sequence_list_input)\n",
    "\n",
    "  compile = []\n",
    "  temp_seq = []\n",
    "  temp_storage = []\n",
    "  temp_rand = []\n",
    "  num = 0\n",
    "  ctr1111 = 0\n",
    "  temp_rand2 = 0\n",
    "  temp_rand3 = 0\n",
    "\n",
    "# per sequences\n",
    "  for sequence in tqdm(sequence_array_list, desc=\"data_aug_seq_sensitivity\", leave=True):\n",
    "    # loops for the number of data augmentation per sequence\n",
    "    for ctr in range(num_data_aug):\n",
    "      # loops for the amount of number of augmentation in the sequence(loops to get random index)\n",
    "      while len(temp_rand)!=num_aug_in_1_seq:\n",
    "        num = rand.randint(0,len(sequence)-1)\n",
    "        if num in temp_rand:\n",
    "          continue\n",
    "        else:\n",
    "          temp_rand.append(num)\n",
    "\n",
    "      #actual augmentation of the sequence\n",
    "      temp_seq = sequence.copy()\n",
    "      # store in a temp variable and to be edited\n",
    "\n",
    "      # number of augmentation to be done in a sequence\n",
    "      for ctr1 in range(len(temp_rand)):\n",
    "\n",
    "        # number of sequence to be expanded(index + number of noise_seq_len)\n",
    "        for ctr2 in range(noise_seq_len):\n",
    "          temp_rand2 = rand.randint(0,len(noise_sequence_list)-1)\n",
    "          temp_rand3 = rand.randint(0,len(noise_sequence_list[0])-1)\n",
    "\n",
    "          if (temp_rand[ctr1] + ctr2) < len(temp_seq):\n",
    "            temp_seq[temp_rand[ctr1] + ctr2] = noise_sequence_list[temp_rand2][temp_rand3]\n",
    "\n",
    "          else:\n",
    "            continue\n",
    "\n",
    "      # for test1 in sequence\n",
    "      compile.append(temp_seq)\n",
    "      temp_seq = []\n",
    "      temp_rand = []\n",
    "\n",
    "\n",
    "  return compile\n",
    "\n",
    "\n",
    "def data_aug_seq_sensitivity(sequence_array_list_input,num_to_aug=2,num_coor_edit=3,num_sequence_edit=2):\n",
    "  sequence_array_list = copy.deepcopy(sequence_array_list_input)\n",
    "\n",
    "  compile = []\n",
    "  temp = []\n",
    "  rand_coor = []\n",
    "\n",
    "\n",
    "  for ctr in tqdm(range(num_to_aug), desc=\"data_aug_coor_sensitivity\", leave=True):\n",
    "    for sequence in sequence_array_list:\n",
    "      for ctr3 in range(num_sequence_edit):\n",
    "        what_sequence = rand.randint(0,len(sequence)-1)\n",
    "        for ctr2 in range(num_coor_edit):\n",
    "          what_coor = rand.randint(0,len(sequence[0])-1)\n",
    "          rand_coor = rand.randint(0,9999999999)\n",
    "          rand_coor = rand_coor / (10 ** len(str(rand_coor)))\n",
    "          print(sequence[what_sequence][what_coor],'---',rand_coor)\n",
    "          sequence[what_sequence][what_coor]=rand_coor\n",
    "      compile.append(sequence)\n",
    "  return compile\n",
    "\n",
    "\n",
    "def data_aug_coor_sensitivity(sequence_array_list_input,num_coor_edit=45,num_sequence_edit=8):\n",
    "  sequence_array_list = copy.deepcopy(sequence_array_list_input)\n",
    "\n",
    "  compile = []\n",
    "  temp = []\n",
    "  rand_coor = []\n",
    "  temp_seq = []\n",
    "\n",
    "\n",
    "\n",
    "  # for ctr in tqdm(range(num_to_aug), desc=\"data_aug_coor_sensitivity\", leave=True):\n",
    "  for sequence in sequence_array_list:\n",
    "    print('------------------------------------------------------------------------')\n",
    "    # print(len(sequence))\n",
    "    temp_seq = sequence.copy()\n",
    "    for ctr3 in range(num_sequence_edit):\n",
    "      what_sequence = rand.randint(0,len(sequence)-1)\n",
    "      num_coor_edit = rand.randint(int(num_coor_edit*.65),num_coor_edit)\n",
    "      print(\"----\")\n",
    "\n",
    "      for ctr2 in range(num_coor_edit):\n",
    "        what_coor = rand.randint(0,len(sequence[0])-1)\n",
    "        # rand_coor = rand.randint(0,9999999999)\n",
    "        rand_coor = rand.randint(0,999)\n",
    "        rand_coor = rand_coor / (10 ** len(str(rand_coor)))\n",
    "        # print(temp_seq[what_sequence][what_coor],'---',rand_coor)\n",
    "        temp_seq[what_sequence][what_coor]=rand_coor\n",
    "    compile.append(sequence)\n",
    "  return compile\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_training_history(history):\n",
    "    epochs = range(1, len(history.history['loss']) + 1)\n",
    "\n",
    "    # Plotting training and validation loss\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history.history['loss'], label='Training Loss')\n",
    "    plt.plot(epochs, history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plotting training and validation accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(epochs, history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "# After the training loop\n",
    "# plot_training_history(history)\n",
    "\n",
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "# coordinate adversary sensetivity1\n",
    "# augments data to make model more acceptable to flaws\n",
    "\n",
    "def coorAdvSens1(sequence_array_list_input,num_aug = 6,sensetivity = 0.025,sensetivity_optional_range = 0):\n",
    "  sequence_array_list = copy.deepcopy(sequence_array_list_input)\n",
    "\n",
    "  temp_allowance = 0.5\n",
    "\n",
    "  tempExecution = []\n",
    "  tempSequence = []\n",
    "  tempFinalList = []\n",
    "  temp = 0\n",
    "\n",
    "\n",
    "  for ctr in range(num_aug):\n",
    "    for execution in sequence_array_list_input:\n",
    "      for sequence in execution:\n",
    "        for individual_coor in sequence:\n",
    "          if sensetivity_optional_range == 0:\n",
    "            temp = round(rand.uniform(individual_coor - sensetivity, individual_coor + sensetivity), 8)\n",
    "            # print(individual_coor, \"<---->\", temp)\n",
    "\n",
    "          else:\n",
    "            determiner = rand.randint(0,1)\n",
    "            if determiner == 0:\n",
    "              temp = round(rand.uniform(individual_coor + sensetivity + temp_allowance,individual_coor + sensetivity+sensetivity_optional_range ), 8)\n",
    "              # print(inidividual_coor, \"<---->\", temp)\n",
    "            else:\n",
    "              temp = round(rand.uniform(individual_coor - sensetivity - sensetivity_optional_range,individual_coor - sensetivity - temp_allowance ), 8)\n",
    "              # print(inidividual_coor, \"<---->\", temp)\n",
    "\n",
    "\n",
    "\n",
    "            # if temp >= 1.0 :\n",
    "            #   temp = 1\n",
    "            # elif temp <= 0.00000000:\n",
    "            #   temp =  0.00000001\n",
    "\n",
    "\n",
    "          tempSequence.append(temp)\n",
    "        temp = []\n",
    "        temp_x_value = []\n",
    "        temp_y_value = []\n",
    "\n",
    "        temp_x_value_normalized = []\n",
    "        temp_y_value_normalized = []\n",
    "\n",
    "        tempSequence_normalized = []\n",
    "\n",
    "        tempCtr = 0\n",
    "        for coordinates in range(int(len(tempSequence)/2)):\n",
    "          temp_x_value.append(tempSequence[tempCtr])\n",
    "          tempCtr = tempCtr + 1\n",
    "          temp_y_value.append(tempSequence[tempCtr])\n",
    "          tempCtr = tempCtr + 1\n",
    "\n",
    "\n",
    "\n",
    "        x_min_value = min(temp_x_value)\n",
    "        x_max_value = max(temp_x_value)\n",
    "\n",
    "        y_min_value = min(temp_y_value)\n",
    "        y_max_value = max(temp_y_value)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        for content in temp_x_value:\n",
    "\n",
    "          scaled_values = (content - x_min_value) / (x_max_value - x_min_value)\n",
    "          temp_x_value_normalized.append(scaled_values)\n",
    "          if scaled_values < 0:\n",
    "            print(\"--------------------------\")\n",
    "            print(\"x_min_value --->\",x_min_value)\n",
    "            print(\"x_max_value --->\",x_max_value)\n",
    "            print(\"contett --->\" ,content)\n",
    "\n",
    "\n",
    "        for content in temp_y_value:\n",
    "          scaled_values = (content - y_min_value) / (y_max_value - y_min_value)\n",
    "\n",
    "          temp_y_value_normalized.append(scaled_values)\n",
    "          # if scaled_values < 0:\n",
    "          #   print(y_min_value,y_max_value,content)\n",
    "\n",
    "        x_value_ctr = 0\n",
    "        y_value_ctr = 0\n",
    "\n",
    "        for ctr in range(66):\n",
    "          if ctr % 2 == 0:\n",
    "            tempSequence_normalized.append(temp_x_value_normalized[x_value_ctr])\n",
    "            x_value_ctr = x_value_ctr + 1\n",
    "          else:\n",
    "            tempSequence_normalized.append(temp_y_value_normalized[y_value_ctr])\n",
    "            y_value_ctr = y_value_ctr + 1\n",
    "\n",
    "\n",
    "\n",
    "        tempExecution.append(tempSequence)\n",
    "        tempSequence_normalized = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        tempSequence = []\n",
    "\n",
    "      tempFinalList.append(tempExecution)\n",
    "      tempExecution = []\n",
    "  tempFinalList.extend(sequence_array_list)\n",
    "  print('\\n-------------------applied coorAdvSens1-------------------')\n",
    "  print('initial len --> ',len(sequence_array_list))\n",
    "  print('final len --> ',len(tempFinalList))\n",
    "\n",
    "\n",
    "  return tempFinalList\n",
    "\n",
    "\n",
    "\n",
    "def sequenceAdvSens1(sequence_array_list_input,num_aug = 2,sensetivity = 0.1):\n",
    "  sequence_array_list = copy.deepcopy(sequence_array_list_input)\n",
    "\n",
    "  allowance_temp = 0.5\n",
    "\n",
    "  base_num_seq_aug = int(len(sequence_array_list[0]) * sensetivity)\n",
    "  print(\"base num aug ---> \",base_num_seq_aug)\n",
    "  if base_num_seq_aug < 1:\n",
    "    base_num_seq_aug = 1\n",
    "\n",
    "  temp_rand_seq_index = []\n",
    "  rand_index = 0\n",
    "  tempSequence = []\n",
    "  tempFinalList = []\n",
    "  replacement = []\n",
    "  temp = 0\n",
    "\n",
    "  for x in range(66):\n",
    "    replacement.append(0)\n",
    "\n",
    "  for ctr in tqdm(range(num_aug), desc=\"sequenceAdvSens1\"):\n",
    "    for execution in sequence_array_list:\n",
    "      temp_execution = execution\n",
    "      for ctr in range(base_num_seq_aug):\n",
    "        while rand_index in temp_rand_seq_index:\n",
    "          rand_index = rand.randrange(1, len(temp_execution))\n",
    "        temp_rand_seq_index.append(rand_index)\n",
    "\n",
    "      for index in temp_rand_seq_index:\n",
    "        temp_execution[index] = replacement\n",
    "      tempFinalList.append(temp_execution)\n",
    "      temp_rand_seq_index = []\n",
    "\n",
    "\n",
    "  tempFinalList.extend(sequence_array_list)\n",
    "\n",
    "  print('-------------------applied coorAdvSens1-------------------')\n",
    "  print('initial len --> ',len(sequence_array_list))\n",
    "  print('final len --> ',len(tempFinalList))\n",
    "  return tempFinalList\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load and preprocess input image\n",
    "import numpy as np\n",
    "\n",
    "# testing data leakage\n",
    "def checking_inputs(correct_input,wrong_input,model_path_param):\n",
    "  temp_padding_array = []\n",
    "\n",
    "  for x in range(66):\n",
    "    temp_padding_array.append(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  base_data = correct_input\n",
    "  base_data_noise = wrong_input\n",
    "  print(len(base_data))\n",
    "  print(len(base_data_noise))\n",
    "\n",
    "  compiled_data = []\n",
    "  compiled_data.append(base_data)\n",
    "  compiled_data.append(base_data_noise)\n",
    "\n",
    "  temp_seq = []\n",
    "  temp_execution = []\n",
    "\n",
    "  ctr_compiled_data = 0\n",
    "  correct_ctr = 0\n",
    "  wrong_ctr = 0\n",
    "\n",
    "  correct_threshod = 0.9\n",
    "\n",
    "\n",
    "  # Load TFLite model\n",
    "  interpreter = tf.lite.Interpreter(model_path=model_path_param)\n",
    "  interpreter.allocate_tensors()\n",
    "  input_tensor_index = interpreter.get_input_details()[0]['index']\n",
    "  shape_needed = interpreter.get_input_details()[0][\"shape\"][1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# convertin to float32\n",
    "  for content in compiled_data:\n",
    "    temp_final = []\n",
    "\n",
    "    if ctr_compiled_data == 0:\n",
    "      print(\"----------------correct inputs----------------\")\n",
    "    else:\n",
    "      print(\"----------------wrong inputs----------------\")\n",
    "    for execution in content:\n",
    "      while len(execution) > shape_needed:\n",
    "        execution.pop()\n",
    "\n",
    "      while len(execution) < shape_needed:\n",
    "        execution.append(temp_padding_array)\n",
    "\n",
    "      for sequence in execution:\n",
    "        for coordinates in sequence:\n",
    "          temp_seq.append(np.float32(coordinates))\n",
    "        temp_execution.append(temp_seq)\n",
    "        temp_seq = []\n",
    "      temp_final.append(temp_execution)\n",
    "      temp_execution=[]\n",
    "\n",
    "    print(\"--->?\",len(temp_final))\n",
    "    for temp_final_content in temp_final:\n",
    "      temp_inference = temp_final_content\n",
    "\n",
    "      input_data = temp_final_content\n",
    "      input_data = np.array(input_data)\n",
    "      input_data = np.reshape(input_data, (1, 5, 66))\n",
    "\n",
    "      # Set input tensor\n",
    "      interpreter.set_tensor(input_tensor_index, input_data)\n",
    "\n",
    "      # Run inference\n",
    "      interpreter.invoke()\n",
    "\n",
    "      # Get output tensor\n",
    "      output_tensor_index = interpreter.get_output_details()[0]['index']\n",
    "      output_data = interpreter.get_tensor(output_tensor_index)\n",
    "      print(output_data)\n",
    "\n",
    "\n",
    "\n",
    "      # print(ctr_compiled_data)\n",
    "      if ctr_compiled_data == 0 and output_data >= correct_threshod:\n",
    "        correct_ctr = correct_ctr + 1\n",
    "\n",
    "      elif ctr_compiled_data == 1 and output_data < correct_threshod:\n",
    "        wrong_ctr = wrong_ctr + 1\n",
    "\n",
    "    ctr_compiled_data = ctr_compiled_data + 1\n",
    "\n",
    "\n",
    "  print(\"correct_input --> \",correct_ctr,\"/\",len(base_data))\n",
    "  print(\"wrong_input --> \",wrong_ctr,\"/\",len(base_data_noise))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediapipe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
