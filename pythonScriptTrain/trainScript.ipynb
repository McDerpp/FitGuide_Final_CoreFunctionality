{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Bidirectional, BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard,ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "import copy\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "# import mediapipe as mp\n",
    "from IPython.display import clear_output\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from tensorflow_model_optimization.quantization.keras import quantize_model\n",
    "from collections import Counter\n",
    "import random as rand\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def txt_pre_process(txt_file,label,simplify=False,simplify_level=14 ):\n",
    "    label_array = []\n",
    "    temp_feature_data = []\n",
    "    temp_sequence_data = []\n",
    "    batch_data = []\n",
    "\n",
    "    with open(str(txt_file), 'r') as file:\n",
    "\n",
    "        for line in file:\n",
    "            values = line.strip().split('|')\n",
    "\n",
    "            temp_feature_data = []\n",
    "\n",
    "            for value in values:\n",
    "                float_value = str(value)\n",
    "\n",
    "                #FIRST PART OF THE SEQUENCE\n",
    "                if float_value == 'START':\n",
    "                    temp_sequence_data=[]\n",
    "\n",
    "                elif float_value == 'END':\n",
    "                    batch_data.append(temp_sequence_data)\n",
    "                    label_array.append(label)\n",
    "\n",
    "\n",
    "                elif float_value != '' and float_value != 'START':\n",
    "                    if simplify:\n",
    "                        float_value = round(float(value),simplify_level)\n",
    "                    else:\n",
    "                        float_value = float(value)\n",
    "                    temp_feature_data.append(float_value)\n",
    "\n",
    "            if temp_feature_data!=[]:\n",
    "                temp_sequence_data.append(temp_feature_data)\n",
    "\n",
    "    label_array = np.array(label_array)\n",
    "    return [batch_data,label_array]\n",
    "\n",
    "#--------------------------------------------------------------------------- paddingV1 --------------------------------------------------------------------------------\n",
    "# padding can be improved probably...by using sequence\n",
    "# minor issue:\n",
    "# > is whether sequences had exceeded the intended number of sequences but is still right (it was performed right but slower(by an acceptable margin)) - not resolved\n",
    "#    = temporary fix was just to truncate everything if it had exceeded the intended number of sequence for the sake of running it for now\n",
    "#    = a reliable solution in theory could be that to randomly truncate in between the first and end sequence, in this way relevant data can be captured\n",
    "def padding(pre_processed_input,optional_maxLength=0):\n",
    "    padded_sequences = []\n",
    "    if optional_maxLength != 0:\n",
    "        max_length = optional_maxLength\n",
    "    else:\n",
    "        max_length = max(len(sequence) for sequence in pre_processed_input)\n",
    "\n",
    "    for sequence in pre_processed_input:\n",
    "        padding_length = max_length - len(sequence)\n",
    "        if padding_length >= 0:\n",
    "            padded_sequence = np.pad(sequence, ((0, padding_length), (0, 0)), mode='constant')\n",
    "\n",
    "        else:\n",
    "            padded_sequence = sequence[:max_length]\n",
    "        padded_sequences.append(padded_sequence)\n",
    "    padded_sequences = np.array(padded_sequences)\n",
    "\n",
    "    return padded_sequences\n",
    "\n",
    "#--------------------------------------------------------------------------- paddingV1 --------------------------------------------------------------------------------\n",
    "\n",
    "# this is to merge correct executions and wrong executions and randomize their input and label\n",
    "# positions of input and its corresponding label are the same\n",
    "# introducing noise/wrong input makes the model more robust\n",
    "def concatenate_randomize_batches(base_input,base_label,concat_input,concat_label):\n",
    "    combined_inputs = np.concatenate((base_input,concat_input), axis = 0)\n",
    "    combined_label = np.concatenate((base_label,concat_label), axis = 0)\n",
    "    indices = np.random.permutation(len(combined_inputs))\n",
    "    randomized_inputs = combined_inputs[indices]\n",
    "    randomized_label = combined_label[indices]\n",
    "    return [randomized_inputs,randomized_label]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def tally_sequence(sequence_array):\n",
    "    tally_number = []\n",
    "    tally_ctr = []\n",
    "\n",
    "    for x in sequence_array:\n",
    "        temp = len(x)\n",
    "        if temp not in tally_number:\n",
    "            tally_number.append(temp)\n",
    "            tally_ctr.append(1)\n",
    "        else:\n",
    "            for y in range(len(tally_number)) :\n",
    "                if temp == tally_number[y]:\n",
    "                    tally_ctr[y] = tally_ctr[y] + 1\n",
    "\n",
    "    tally_max = 0\n",
    "    tally_number_arranged = []\n",
    "    tally_ctr_arranged = []\n",
    "\n",
    "    for x in range(len(tally_number)):\n",
    "        # print(len(tally_ctr))\n",
    "        tally_max = max(tally_ctr)\n",
    "        for y in range(len(tally_number)):\n",
    "            if tally_ctr[y] == tally_max:\n",
    "                tally_number_arranged.append(tally_number[y])\n",
    "                tally_ctr_arranged.append(tally_ctr[y])\n",
    "                tally_ctr.pop(y)\n",
    "                tally_number.pop(y)\n",
    "                break\n",
    "\n",
    "    total_ctr = 0\n",
    "    for x in tally_ctr:\n",
    "        total_ctr = total_ctr + x\n",
    "\n",
    "\n",
    "    for x in range(len(tally_number_arranged)):\n",
    "        print(tally_number_arranged[x],'-->',tally_ctr_arranged[x])\n",
    "\n",
    "\n",
    "# outlier detection and removal (currently being used)\n",
    "def common_length_sequence(sequences_array,threshold = 2):\n",
    "    temp = []\n",
    "\n",
    "    data = [len(seq) for seq in sequences_array]\n",
    "    data_frequency = Counter(data)\n",
    "    most_common_data = data_frequency.most_common()\n",
    "    outlier_frequencies = [value for value, freq in data_frequency.items() if freq < threshold]\n",
    "    most_common_values = [value for value, freq in most_common_data if freq >= threshold]\n",
    "\n",
    "    print(\"Most Common Data Points:\", most_common_values)\n",
    "    print(\"Outlier Frequencies:\", outlier_frequencies)\n",
    "\n",
    "    for x in sequences_array:\n",
    "        if len(x) in most_common_values:\n",
    "            temp.append(x)\n",
    "    print('-------------------applied frequency outlier detection-------------------')\n",
    "    print(\"original num -> \", len(sequences_array))\n",
    "    print(\"current num -> \", len(temp))\n",
    "    print(\"removed num -> \", len(sequences_array) - len(temp))\n",
    "    return temp\n",
    "\n",
    "# outlier detection and removal (currently being used)\n",
    "def apply_z_score(sequences_array,z_score_threshold = 1):\n",
    "    data_points = []\n",
    "    included_datapoints = []\n",
    "    updated_sequences =[]\n",
    "\n",
    "    for x in sequences_array:\n",
    "        temp = len(x)\n",
    "        if temp not in data_points:\n",
    "            data_points.append(temp)\n",
    "\n",
    "    data = np.array(data_points)\n",
    "    mean_value = np.mean(data)\n",
    "    standard_deviation = np.std(data)\n",
    "    z_scores = (data - mean_value) / standard_deviation\n",
    "    for x in range(len(z_scores)):\n",
    "        if np.abs(z_scores[x]) <= z_score_threshold:\n",
    "            included_datapoints.append(data[x])\n",
    "\n",
    "\n",
    "    for x in sequences_array:\n",
    "        if len(x) in included_datapoints:\n",
    "            updated_sequences.append(x)\n",
    "    print('-------------------applied z-score outlier detection-------------------')\n",
    "    print(\"datapoints included -> \", included_datapoints)\n",
    "    print(\"original num -> \", len(sequences_array))\n",
    "    print(\"current num -> \", len(updated_sequences))\n",
    "    print(\"removed num -> \", len(sequences_array) - len(updated_sequences))\n",
    "\n",
    "    return updated_sequences\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def paddingV2(sequences_array_input,optional_maxlength = 0):\n",
    "    sequences_array = copy.deepcopy(sequences_array_input)\n",
    "\n",
    "\n",
    "    output = []\n",
    "    max_length = 0\n",
    "    if optional_maxlength == 0:\n",
    "        max_length = max(len(sequence) for sequence in sequences_array)\n",
    "        expanded_max_length = int(max_length+ ((max_length) * .10))\n",
    "    else:\n",
    "        expanded_max_length = optional_maxlength\n",
    "\n",
    "    # sequence = np.array(sequences_array)\n",
    "\n",
    "    print(expanded_max_length)\n",
    "\n",
    "\n",
    "    padding_length_before = 0\n",
    "    padding_length_after = 0\n",
    "\n",
    "    for seq in sequences_array:\n",
    "        # print(seq)\n",
    "        for x in range(expanded_max_length-len(seq)+1):\n",
    "            padding_length_before = x\n",
    "            padding_length_after = expanded_max_length - len(seq) - x\n",
    "            padded_sequence = np.pad(seq, ((padding_length_before, padding_length_after),(0,0)), mode='constant')\n",
    "            output.append(padded_sequence)\n",
    "\n",
    "            # print(padded_sequence)\n",
    "    print('------------------------applied paddingV2------------------------')\n",
    "    print('max_length -> ', max_length)\n",
    "    print('expanded_max_length -> ', expanded_max_length)\n",
    "    print('original num set of sequences -> ', len(sequences_array))\n",
    "    print('final num set of sequences -> ', len(output))\n",
    "\n",
    "    output = np.array(output)\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def convert_tf_to_tflite(tf_model,input_shape,test_dataset,name,id_number,validation_loss,validation_accuracy):\n",
    "  model = tf.keras.models.load_model(tf_model)\n",
    "\n",
    "  run_model = tf.function(lambda x: model(x))\n",
    "  # This is important, let's fix the input size.\n",
    "  BATCH_SIZE = input_shape[0]\n",
    "  STEPS = input_shape[1]\n",
    "  INPUT_SIZE = input_shape[2]\n",
    "  concrete_func = run_model.get_concrete_function(\n",
    "      tf.TensorSpec([BATCH_SIZE, STEPS, INPUT_SIZE], model.inputs[0].dtype))\n",
    "\n",
    "  # model directory.\n",
    "  MODEL_DIR = \"keras_lstm\"\n",
    "  model.save(MODEL_DIR, save_format=\"tf\", signatures=concrete_func)\n",
    "\n",
    "  converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR)\n",
    "  tflite_model = converter.convert()\n",
    "\n",
    "\n",
    "  # Run the model with TensorFlow to get expected results.\n",
    "  TEST_CASES = 10\n",
    "\n",
    "  # Run the model with TensorFlow Lite\n",
    "  interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "  interpreter.allocate_tensors()\n",
    "  input_details = interpreter.get_input_details()\n",
    "  output_details = interpreter.get_output_details()\n",
    "\n",
    "  for i in range(TEST_CASES):\n",
    "    expected = model.predict(test_dataset[i:i+1])\n",
    "    interpreter.set_tensor(input_details[0][\"index\"], test_dataset[i:i+1, :, :])\n",
    "    interpreter.invoke()\n",
    "    result = interpreter.get_tensor(output_details[0][\"index\"])\n",
    "\n",
    "    # Assert if the result of TFLite model is consistent with the TF model.\n",
    "    np.testing.assert_almost_equal(expected, result, decimal=5)\n",
    "    print(\"Done. The result of TensorFlow matches the result of TensorFlow Lite.\")\n",
    "\n",
    "    interpreter.reset_all_variables()\n",
    "\n",
    "\n",
    "\n",
    "  temp = 'converted_model_'\n",
    "\n",
    "  temp3 = temp + str(name) + id_number + \"(loss_\"+ str(round(validation_loss,3)) +\")\" + \"(acc_\"+  str(round(validation_accuracy,3 )) + \")\" + '.tflite'\n",
    "  # Save the TFLite model to a file\n",
    "  with open(temp3, \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "  # with open(\"converted_model.tflite\", \"wb\") as f:\n",
    "  #     f.write(tflite_model)\n",
    "\n",
    "\n",
    "# this data augmentation replaces padded index with random inputs\n",
    "def populate_0_input(correct_data_input,noise_data_input):\n",
    "    correct_data = copy.deepcopy(correct_data_input)\n",
    "    noise_data = copy.deepcopy(noise_data_input)\n",
    "\n",
    "    print(len(correct_data))\n",
    "    index = 10\n",
    "    temp = []\n",
    "    temp_compilation = []\n",
    "    ctr = 0\n",
    "    rand_modifier =0\n",
    "\n",
    "    for set_sequence in tqdm(correct_data, desc=\"populate_0_input\", leave=True):\n",
    "        rand_modifier = rand.randint(0,len(noise_data))\n",
    "\n",
    "        for x in range(len(set_sequence)):\n",
    "            ctr = ctr + 1\n",
    "            if set_sequence[x][0] == 0:\n",
    "                temp.append(noise_data[rand_modifier-1][rand.randint(0,len(noise_data[rand_modifier-1])-1)])\n",
    "\n",
    "            else:\n",
    "                temp.append(set_sequence[x])\n",
    "\n",
    "        temp_compilation.append(temp)\n",
    "        temp =[]\n",
    "\n",
    "\n",
    "    return temp_compilation\n",
    "\n",
    "\n",
    "\n",
    "class CustomEarlyStopping(Callback):\n",
    "  def __init__(self, accuracy_threshold=0.95, loss_threshold=0.10):\n",
    "      super(CustomEarlyStopping, self).__init__()\n",
    "      self.accuracy_threshold = accuracy_threshold\n",
    "      self.loss_threshold = loss_threshold\n",
    "\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "      if logs is None:\n",
    "          logs = {}\n",
    "\n",
    "      if logs.get('val_accuracy') is None or logs.get('val_loss') is None:\n",
    "          return\n",
    "\n",
    "      if logs.get('val_accuracy') >= self.accuracy_threshold and logs.get('val_loss') <= self.loss_threshold:\n",
    "          self.model.stop_training = True\n",
    "          print(f\"\\nTraining stopped as validation accuracy reached {logs.get('val_accuracy'):.4f} \"\n",
    "                f\"and validation loss reached {logs.get('val_loss'):.4f}\")\n",
    "\n",
    "\n",
    "# this data augmentation augments data to simulate if the sequence were follow,whether there is unecessary movements in between.\n",
    "# this would also reinforce the model to detect certain action that were classified as correct instead of incorrect\n",
    "# this augmentation is augmenting each sequence of a list of sequence\n",
    "# 1 set of sequence(example-> 1 push up):\n",
    "# [seq1,seq2,seq3,seq4]\n",
    "# [seq1,seq2,NOISE_SEQ34,seq4]\n",
    "def data_aug_sensitivity(sequence_array_list_input,noise_sequence_list_input,num_data_aug = 3,num_aug_in_1_seq = 3,noise_seq_len = 2):\n",
    "  sequence_array_list = copy.deepcopy(sequence_array_list_input)\n",
    "  noise_sequence_list = copy.deepcopy(noise_sequence_list_input)\n",
    "\n",
    "  compile = []\n",
    "  temp_seq = []\n",
    "  temp_storage = []\n",
    "  temp_rand = []\n",
    "  num = 0\n",
    "  ctr1111 = 0\n",
    "  temp_rand2 = 0\n",
    "  temp_rand3 = 0\n",
    "\n",
    "# per sequences\n",
    "  for sequence in tqdm(sequence_array_list, desc=\"data_aug_seq_sensitivity\", leave=True):\n",
    "    # loops for the number of data augmentation per sequence\n",
    "    for ctr in range(num_data_aug):\n",
    "      # loops for the amount of number of augmentation in the sequence(loops to get random index)\n",
    "      while len(temp_rand)!=num_aug_in_1_seq:\n",
    "        num = rand.randint(0,len(sequence)-1)\n",
    "        if num in temp_rand:\n",
    "          continue\n",
    "        else:\n",
    "          temp_rand.append(num)\n",
    "\n",
    "      #actual augmentation of the sequence\n",
    "      temp_seq = sequence.copy()\n",
    "      # store in a temp variable and to be edited\n",
    "\n",
    "      # number of augmentation to be done in a sequence\n",
    "      for ctr1 in range(len(temp_rand)):\n",
    "\n",
    "        # number of sequence to be expanded(index + number of noise_seq_len)\n",
    "        for ctr2 in range(noise_seq_len):\n",
    "          temp_rand2 = rand.randint(0,len(noise_sequence_list)-1)\n",
    "          temp_rand3 = rand.randint(0,len(noise_sequence_list[0])-1)\n",
    "\n",
    "          if (temp_rand[ctr1] + ctr2) < len(temp_seq):\n",
    "            temp_seq[temp_rand[ctr1] + ctr2] = noise_sequence_list[temp_rand2][temp_rand3]\n",
    "\n",
    "\n",
    "          else:\n",
    "            continue\n",
    "\n",
    "\n",
    "      compile.append(temp_seq)\n",
    "      temp_seq = []\n",
    "      temp_rand = []\n",
    "\n",
    "\n",
    "  return compile\n",
    "\n",
    "\n",
    "def data_aug_seq_sensitivity(sequence_array_list_input,num_to_aug=2,num_coor_edit=3,num_sequence_edit=2):\n",
    "  sequence_array_list = copy.deepcopy(sequence_array_list_input)\n",
    "\n",
    "  compile = []\n",
    "  temp = []\n",
    "  rand_coor = []\n",
    "\n",
    "\n",
    "  for ctr in tqdm(range(num_to_aug), desc=\"data_aug_coor_sensitivity\", leave=True):\n",
    "    for sequence in sequence_array_list:\n",
    "      for ctr3 in range(num_sequence_edit):\n",
    "        what_sequence = rand.randint(0,len(sequence)-1)\n",
    "        for ctr2 in range(num_coor_edit):\n",
    "          what_coor = rand.randint(0,len(sequence[0])-1)\n",
    "          rand_coor = rand.randint(0,9999999999)\n",
    "          rand_coor = rand_coor / (10 ** len(str(rand_coor)))\n",
    "          sequence[what_sequence][what_coor]=rand_coor\n",
    "      compile.append(sequence)\n",
    "  return compile\n",
    "\n",
    "\n",
    "def data_aug_coor_sensitivity(sequence_array_list_input,num_coor_edit=45,num_sequence_edit=8):\n",
    "  sequence_array_list = copy.deepcopy(sequence_array_list_input)\n",
    "\n",
    "  compile = []\n",
    "  temp = []\n",
    "  rand_coor = []\n",
    "  temp_seq = []\n",
    "\n",
    "\n",
    "\n",
    "  # for ctr in tqdm(range(num_to_aug), desc=\"data_aug_coor_sensitivity\", leave=True):\n",
    "  for sequence in sequence_array_list:\n",
    "    temp_seq = sequence.copy()\n",
    "    for ctr3 in range(num_sequence_edit):\n",
    "      what_sequence = rand.randint(0,len(sequence)-1)\n",
    "      num_coor_edit = rand.randint(int(num_coor_edit*.65),num_coor_edit)\n",
    "      for ctr2 in range(num_coor_edit):\n",
    "        what_coor = rand.randint(0,len(sequence[0])-1)\n",
    "        # rand_coor = rand.randint(0,9999999999)\n",
    "        rand_coor = rand.randint(0,999)\n",
    "        rand_coor = rand_coor / (10 ** len(str(rand_coor)))\n",
    "        temp_seq[what_sequence][what_coor]=rand_coor\n",
    "    compile.append(sequence)\n",
    "  return compile\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'txt_pre_process' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\CLARK\\Documents\\fitguidef\\pythonScriptTrain\\trainScript.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/CLARK/Documents/fitguidef/pythonScriptTrain/trainScript.ipynb#W2sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m   convert_tf_to_tflite(\u001b[39m'\u001b[39m\u001b[39m/content/testingModel\u001b[39m\u001b[39m'\u001b[39m,[\u001b[39m1\u001b[39m,\u001b[39mlen\u001b[39m(aug3[\u001b[39m0\u001b[39m]),\u001b[39mlen\u001b[39m(aug3[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m])], X_test,\u001b[39m'\u001b[39m\u001b[39mwhole_model\u001b[39m\u001b[39m'\u001b[39m,id_num,val_loss,val_accuracy)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/CLARK/Documents/fitguidef/pythonScriptTrain/trainScript.ipynb#W2sZmlsZQ%3D%3D?line=119'>120</a>\u001b[0m \u001b[39m# uncomment this to run training model as a whole(this means that this model can determine the motion as a whole but not specify what part if correct or not)\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/CLARK/Documents/fitguidef/pythonScriptTrain/trainScript.ipynb#W2sZmlsZQ%3D%3D?line=120'>121</a>\u001b[0m streamlined_process(\u001b[39m'\u001b[39;49m\u001b[39m/content/drive/MyDrive/Colab Notebooks/correct_new_2.txt\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39m/content/drive/MyDrive/Colab Notebooks/wrong_new_2.txt\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32md:\\CLARK\\Documents\\fitguidef\\pythonScriptTrain\\trainScript.ipynb Cell 3\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/CLARK/Documents/fitguidef/pythonScriptTrain/trainScript.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstreamlined_process\u001b[39m(correct_execution,noise_data):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/CLARK/Documents/fitguidef/pythonScriptTrain/trainScript.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m   id_num \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(rand\u001b[39m.\u001b[39mrandint(\u001b[39m1000\u001b[39m,\u001b[39m9999\u001b[39m))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/CLARK/Documents/fitguidef/pythonScriptTrain/trainScript.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m   base_data \u001b[39m=\u001b[39m txt_pre_process(correct_execution,\u001b[39m1\u001b[39m,\u001b[39mFalse\u001b[39;00m,\u001b[39m4\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/CLARK/Documents/fitguidef/pythonScriptTrain/trainScript.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m   base_data_noise \u001b[39m=\u001b[39m txt_pre_process(noise_data,\u001b[39m0\u001b[39m,\u001b[39mFalse\u001b[39;00m,\u001b[39m4\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/CLARK/Documents/fitguidef/pythonScriptTrain/trainScript.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m   loop \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'txt_pre_process' is not defined"
     ]
    }
   ],
   "source": [
    "# this is as a whole exercise\n",
    "def streamlined_process(correct_execution,noise_data):\n",
    "  id_num = str(rand.randint(1000,9999))\n",
    "  base_data = txt_pre_process(correct_execution,1,False,4)\n",
    "  base_data_noise = txt_pre_process(noise_data,0,False,4)\n",
    "\n",
    "  loop = 0\n",
    "\n",
    "  temp_correct_start=[]\n",
    "  temp_correct_wrong=[]\n",
    "\n",
    "  temp_wrong_store_train = []\n",
    "\n",
    "  data = base_data[0]\n",
    "  data_noise = base_data_noise[0]\n",
    "\n",
    "  best_val_loss = float('inf')\n",
    "  best_val_accuracy = 0.0\n",
    "  best_model = None\n",
    "\n",
    "\n",
    "\n",
    "  print('')\n",
    "  print('----------------------------correct data augmentation ----------------------------')\n",
    "  aug = common_length_sequence(data)\n",
    "  aug2 = apply_z_score(aug,1)\n",
    "  aug3 = paddingV2(aug2)\n",
    "  aug4 = populate_0_input(aug3,data_noise)\n",
    "  aug4 = np.array(aug4)\n",
    "  aug4 = aug4.reshape(-1,len(aug3[0]),len(aug3[0][0]))\n",
    "  combined_inputs = np.concatenate((aug4,aug3), axis = 0)\n",
    "  combined_inputs = aug4\n",
    "  print('concat -> ', len(combined_inputs))\n",
    "\n",
    "\n",
    "  print('')\n",
    "  print('----------------------------data noise data augmentation ----------------------------')\n",
    "  aug_noise_data1 = paddingV2(data_noise,len(aug3[0]))\n",
    "  aug_noise_data2 = populate_0_input(aug_noise_data1,data_noise)\n",
    "\n",
    "# testing...\n",
    "  # data_aug_coor_sensitivity()\n",
    "\n",
    "# original\n",
    "  # aug_noise_data3 = aug_noise_data2[0:len(combined_inputs)]\n",
    "\n",
    "# testing(temporary)\n",
    "  aug_noise_data3 = aug_noise_data2[:]\n",
    "\n",
    "  aug_noise_data4 = np.array(aug_noise_data3)\n",
    "\n",
    "  aug_noise_data5 = aug_noise_data4.reshape(-1,len(aug_noise_data4[0]),len(aug_noise_data4[0][0]))\n",
    "\n",
    "# testing(temporary)\n",
    "  aug_noise_data7 = data_aug_coor_sensitivity(aug_noise_data5,num_sequence_edit=int(len(aug_noise_data5[0])*.70))\n",
    "\n",
    "  # aug_noise_data6 = data_aug_sensitivity(aug4,aug_noise_data5,1,2,2)\n",
    "  aug_noise_data6 = data_aug_sensitivity(aug4,aug_noise_data5,1,2,int(len(combined_inputs)*.35))\n",
    "\n",
    "# original\n",
    "  # aug_noise_data5 = np.concatenate((aug_noise_data6,aug_noise_data5), axis = 0)\n",
    "\n",
    "# testing(temporary)\n",
    "  aug_noise_data5 = np.concatenate((aug_noise_data6,aug_noise_data5,aug_noise_data7), axis = 0)\n",
    "\n",
    "\n",
    "  print('aug_noise_data5--->',aug_noise_data5.shape)\n",
    "\n",
    "\n",
    "  loop = int(len(aug_noise_data5)/len(combined_inputs))\n",
    "\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(len(aug4[0]), return_sequences=True, activation='relu',  input_shape=(len(aug4[0]), len(aug4[0][0]))))\n",
    "  model.add(LSTM(len(aug4[0]) + int(len(aug4[0]) - int(len(aug4[0]) * .4)), return_sequences=True,  activation='relu'))\n",
    "  model.add(Bidirectional(LSTM(len(aug4[0]) - int(len(aug4[0]) - int(len(aug4[0]) * .4)), return_sequences=True, dropout=0.3, recurrent_dropout=0.3, activation='relu')))\n",
    "  model.add(LSTM(len(aug4[0]) - int(len(aug4[0]) - int(len(aug4[0]) * .4)), return_sequences=False, activation='relu'))\n",
    "  # model.add(BatchNormalization())\n",
    "  model.add(Dense(len(aug4[0]) - int(len(aug4[0]) - int(len(aug4[0]) * .3)), activation='relu'))\n",
    "  model.add(Dense(1,activation='sigmoid'))\n",
    "  custom_early_stopping = CustomEarlyStopping(accuracy_threshold=0.97, loss_threshold=0.05)\n",
    "  model.compile(optimizer = 'Adam' , loss = 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  for x in range(loop):\n",
    "    temp_wrong_store_train = aug_noise_data5[x*len(combined_inputs):len(combined_inputs)*(x+1)]\n",
    "\n",
    "    aug3_label = np.ones(len(combined_inputs))\n",
    "    aug_noise_label = np.zeros(len(temp_wrong_store_train))\n",
    "\n",
    "    rand_batches=concatenate_randomize_batches(combined_inputs,aug3_label,temp_wrong_store_train,aug_noise_label)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(rand_batches[0], rand_batches[1], test_size=0.3, random_state=42)\n",
    "\n",
    "    # history=model.fit(X_train, y_train, epochs=200, batch_size =256  , validation_data=(X_test, y_test), callbacks=[custom_early_stopping])\n",
    "    history=model.fit(X_train, y_train, epochs=200, batch_size =256  , validation_data=(X_test, y_test), callbacks=[custom_early_stopping])\n",
    "\n",
    "\n",
    "    val_loss = min(history.history['val_loss'])\n",
    "    val_accuracy = max(history.history['val_accuracy'])\n",
    "    temp_wrong_store_train=[]\n",
    "\n",
    "\n",
    "    if val_loss < best_val_loss and val_accuracy > best_val_accuracy:\n",
    "      best_model = model.get_weights()\n",
    "      best_val_loss = val_loss\n",
    "      best_val_accuracy = val_accuracy\n",
    "\n",
    "    # testing(temporary)\n",
    "    model.set_weights(best_model)\n",
    "\n",
    "  X_train = X_train.astype(np.float32)\n",
    "  X_test = X_test.astype(np.float32)\n",
    "  model.save('testingModel')\n",
    "  convert_tf_to_tflite('/content/testingModel',[1,len(aug3[0]),len(aug3[0][0])], X_test,'whole_model',id_num,val_loss,val_accuracy)\n",
    "\n",
    "\n",
    "\n",
    "# uncomment this to run training model as a whole(this means that this model can determine the motion as a whole but not specify what part if correct or not)\n",
    "streamlined_process('/content/drive/MyDrive/Colab Notebooks/correct_new_2.txt','/content/drive/MyDrive/Colab Notebooks/wrong_new_2.txt')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is for the individual models\n",
    "# this is experimental\n",
    "def streamlined_process(correct_execution,noise_data,loop=5):\n",
    "  id_num = str(rand.randint(1000,9999))\n",
    "  base_data = txt_pre_process(correct_execution,1,False,4)\n",
    "  base_data_noise = txt_pre_process(noise_data,0,False,4)\n",
    "\n",
    "  temp_correct_start=[]\n",
    "  temp_correct_wrong=[]\n",
    "\n",
    "  data = base_data[0]\n",
    "  data_noise = base_data_noise[0]\n",
    "\n",
    "\n",
    "\n",
    "  print('')\n",
    "  print('----------------------------correct data augmentation ----------------------------')\n",
    "  aug = common_length_sequence(data)\n",
    "  aug2 = apply_z_score(aug,1)\n",
    "  aug3 = paddingV2(aug2)\n",
    "  aug4 = populate_0_input(aug3,data_noise)\n",
    "  aug4 = np.array(aug4)\n",
    "  aug4 = aug4.reshape(-1,len(aug3[0]),len(aug3[0][0]))\n",
    "  combined_inputs = np.concatenate((aug4,aug3), axis = 0)\n",
    "  combined_inputs= aug4\n",
    "  print('concat -> ', len(combined_inputs))\n",
    "\n",
    "\n",
    "\n",
    "  print('')\n",
    "  print('----------------------------data noise data augmentation ----------------------------')\n",
    "  aug_noise_data1 = paddingV2(data_noise,len(aug3[0]))\n",
    "  aug_noise_data2 = populate_0_input(aug_noise_data1,data_noise)\n",
    "  aug_noise_data3 = aug_noise_data2[0:len(combined_inputs)]\n",
    "  aug_noise_data4 = np.array(aug_noise_data3)\n",
    "  aug_noise_data5 = aug_noise_data4.reshape(-1,len(aug_noise_data4[0]),len(aug_noise_data4[0][0]))\n",
    "\n",
    "  aug_noise_data6 = data_aug_sensitivity(aug4,aug_noise_data5,1,1,int(len(combined_inputs)*.35))\n",
    "  aug_noise_data5 = np.concatenate((aug_noise_data6,aug_noise_data5), axis = 0)\n",
    "  # aug_noise_data6 = data_aug_coor_sensitivity(aug4)\n",
    "  # aug_noise_data5 = np.concatenate((aug_noise_data6,aug_noise_data5), axis = 0)\n",
    "\n",
    "\n",
    "  correct_data_set = []\n",
    "  noise_data_set = []\n",
    "  rand_batches = []\n",
    "\n",
    "  print('testtesttesttest=======>',len(aug_noise_data5))\n",
    "\n",
    "\n",
    "  for x in range(14):\n",
    "    correct_data_set.append([])\n",
    "    noise_data_set.append([])\n",
    "    rand_batches.append([])\n",
    "\n",
    "\n",
    "  for exercise in combined_inputs:\n",
    "\n",
    "    left_upper_arm = []\n",
    "    left_lower_arm = []\n",
    "    left_hand = []\n",
    "    right_upper_arm = []\n",
    "    right_lower_arm = []\n",
    "    right_hand = []\n",
    "    left_upper_leg = []\n",
    "    left_lower_leg = []\n",
    "    left_feet = []\n",
    "    right_upper_leg = []\n",
    "    right_lower_leg = []\n",
    "    right_feet = []\n",
    "    head =[]\n",
    "    body = []\n",
    "\n",
    "\n",
    "\n",
    "#------------------------ generating the correct input of certain part------------------------------\n",
    "    for sequence in exercise:\n",
    "      # 11,13\n",
    "      left_upper_arm.append([sequence[22],sequence[23],sequence[26],sequence[27]])\n",
    "      # 13,15\n",
    "      left_lower_arm.append([sequence[26],sequence[27],sequence[30],sequence[31]])\n",
    "      # 15,17,19,21\n",
    "      left_hand.append([sequence[30],sequence[31],sequence[34],sequence[35],sequence[38],sequence[39],sequence[42],sequence[43]])\n",
    "\n",
    "      # 12,14\n",
    "      right_upper_arm.append([sequence[24],sequence[25],sequence[28],sequence[29]])\n",
    "      # 14,16\n",
    "      right_lower_arm.append([sequence[28],sequence[29],sequence[32],sequence[33]])\n",
    "      # 16,18,20,22\n",
    "      right_hand.append([sequence[32],sequence[33],sequence[36],sequence[37],sequence[40],sequence[41],sequence[44],sequence[45]])\n",
    "\n",
    "      # 23,25\n",
    "      left_upper_leg.append([sequence[46],sequence[47],sequence[50],sequence[51]])\n",
    "      # 25,27\n",
    "      left_lower_leg.append([sequence[50],sequence[51],sequence[54],sequence[55]])\n",
    "      # 27,29,31\n",
    "      left_feet.append([sequence[54],sequence[55],sequence[58],sequence[59],sequence[62],sequence[63]])\n",
    "\n",
    "      # 24,26\n",
    "      right_upper_leg.append([sequence[48],sequence[49],sequence[52],sequence[53]])\n",
    "      # 26,28\n",
    "      right_lower_leg.append([sequence[52],sequence[53],sequence[56],sequence[57]])\n",
    "      # 28,30,32\n",
    "      right_feet.append([sequence[56],sequence[57],sequence[60],sequence[61],sequence[64],sequence[65]])\n",
    "\n",
    "      # 11,12,23,24\n",
    "      body.append([sequence[22],sequence[23],sequence[24],sequence[25],sequence[46],sequence[47],sequence[48],sequence[49]])\n",
    "      # 7,8,9,10\n",
    "      head.append([sequence[14],sequence[15],sequence[16],sequence[17],sequence[18],sequence[19],sequence[20],sequence[21]])\n",
    "\n",
    "\n",
    "    correct_data_set[0].append(left_upper_arm)\n",
    "    correct_data_set[1].append(left_lower_arm)\n",
    "    correct_data_set[2].append(left_hand)\n",
    "\n",
    "    correct_data_set[3].append(right_upper_arm)\n",
    "    correct_data_set[4].append(right_lower_arm)\n",
    "    correct_data_set[5].append(right_hand)\n",
    "\n",
    "    correct_data_set[6].append(left_upper_leg)\n",
    "    correct_data_set[7].append(left_lower_leg)\n",
    "    correct_data_set[8].append(left_feet)\n",
    "\n",
    "    correct_data_set[9].append(right_upper_leg)\n",
    "    correct_data_set[10].append(right_lower_leg)\n",
    "    correct_data_set[11].append(right_feet)\n",
    "\n",
    "    correct_data_set[12].append(body)\n",
    "    correct_data_set[13].append(head)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  for x in range(len(correct_data_set)):\n",
    "    correct_data_set[x] = np.array(correct_data_set[x])\n",
    "\n",
    "  # for x in range(len(noise_data_set)):\n",
    "  #   noise_data_set[x] = np.array(noise_data_set[x])\n",
    "\n",
    "  print('len of correct data set ---->',len(correct_data_set[0]))\n",
    "  print('len of noise data set ---->',len(noise_data_set[0]))\n",
    "\n",
    "  correct_data_set_label = np.ones(len(combined_inputs))\n",
    "  noise_data_set_label = np.zeros(len(aug_noise_data5))\n",
    "\n",
    "  # for x in range(len(correct_data_set)):\n",
    "  #   rand_batches[x]=concatenate_randomize_batches(correct_data_set[x],correct_data_set_label,noise_data_set[x],noise_data_set_label)\n",
    "\n",
    "\n",
    "\n",
    "  loop = int(len(aug_noise_data5)/len(combined_inputs))\n",
    "\n",
    "\n",
    "  data_set_name=['left_upper_arm','left_lower_arm','left_hand','right_upper_arm','right_lower_arm','right_hand','left_upper_leg','left_lower_leg','left_feet','right_upper_leg','right_lower_leg','right_feet','head','body']\n",
    "  for x in range(len(data_set_name)-1):\n",
    "    print('progress -> ',x,'/',len(data_set_name)-1)\n",
    "\n",
    "\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_accuracy = 0.0\n",
    "    best_model = None\n",
    "\n",
    "    for y in range(loop):\n",
    "\n",
    "      noise_data_set=[]\n",
    "\n",
    "\n",
    "    #------------------------------ generating noise ---------------------------------------\n",
    "      for exercise in aug_noise_data5[len(correct_data_set[x])*y:len(correct_data_set[x]) + (len(correct_data_set[x])*y)]:\n",
    "      # for exercise in aug_noise_data5[0:450]:\n",
    "        # print('batchin noise ->',len(correct_data_set[x])*y,'<==--==>',len(correct_data_set[x]) + (len(correct_data_set[x])*y))\n",
    "\n",
    "\n",
    "        left_upper_arm = []\n",
    "        left_lower_arm = []\n",
    "        left_hand = []\n",
    "        right_upper_arm = []\n",
    "        right_lower_arm = []\n",
    "        right_hand = []\n",
    "        left_upper_leg = []\n",
    "        left_lower_leg = []\n",
    "        left_feet = []\n",
    "        right_upper_leg = []\n",
    "        right_lower_leg = []\n",
    "        right_feet = []\n",
    "        head =[]\n",
    "        body = []\n",
    "\n",
    "\n",
    "\n",
    "        for sequence in exercise:\n",
    "          # 11,13\n",
    "          # print(sequence[22])\n",
    "          if x == 0:\n",
    "            left_upper_arm.append([sequence[22],sequence[23],sequence[26],sequence[27]])\n",
    "          # 13,15\n",
    "          if x == 1:\n",
    "            left_lower_arm.append([sequence[26],sequence[27],sequence[30],sequence[31]])\n",
    "          # 15,17,19,21\n",
    "          if x == 2:\n",
    "            left_hand.append([sequence[30],sequence[31],sequence[34],sequence[35],sequence[38],sequence[39],sequence[42],sequence[43]])\n",
    "\n",
    "          # 12,14\n",
    "          if x == 3:\n",
    "            right_upper_arm.append([sequence[24],sequence[25],sequence[28],sequence[29]])\n",
    "          # 14,16\n",
    "          if x == 4:\n",
    "            right_lower_arm.append([sequence[28],sequence[29],sequence[32],sequence[33]])\n",
    "          # 16,18,20,22\n",
    "          if x == 5:\n",
    "            right_hand.append([sequence[32],sequence[33],sequence[36],sequence[37],sequence[40],sequence[41],sequence[44],sequence[45]])\n",
    "\n",
    "          # 23,25\n",
    "          if x == 6:\n",
    "            left_upper_leg.append([sequence[46],sequence[47],sequence[50],sequence[51]])\n",
    "          # 25,27\n",
    "          if x == 7:\n",
    "            left_lower_leg.append([sequence[50],sequence[51],sequence[54],sequence[55]])\n",
    "          # 27,29,31\n",
    "          if x == 8:\n",
    "            left_feet.append([sequence[54],sequence[55],sequence[58],sequence[59],sequence[62],sequence[63]])\n",
    "\n",
    "          # 24,26\n",
    "          if x == 9:\n",
    "            right_upper_leg.append([sequence[48],sequence[49],sequence[52],sequence[53]])\n",
    "          # 26,28\n",
    "          if x == 10:\n",
    "            right_lower_leg.append([sequence[52],sequence[53],sequence[56],sequence[57]])\n",
    "          # 28,30,32\n",
    "          if x == 11:\n",
    "            right_feet.append([sequence[56],sequence[57],sequence[60],sequence[61],sequence[64],sequence[65]])\n",
    "\n",
    "          # 11,12,23,24\n",
    "          if x == 12:\n",
    "            body.append([sequence[22],sequence[23],sequence[24],sequence[25],sequence[46],sequence[47],sequence[48],sequence[49]])\n",
    "          # 7,8,9,10\n",
    "          if x == 13:\n",
    "            head.append([sequence[14],sequence[15],sequence[16],sequence[17],sequence[18],sequence[19],sequence[20],sequence[21]])\n",
    "\n",
    "        if x == 0:\n",
    "          noise_data_set.append(left_upper_arm)\n",
    "        if x == 1:\n",
    "          noise_data_set.append(left_lower_arm)\n",
    "        if x == 2:\n",
    "          noise_data_set.append(left_hand)\n",
    "\n",
    "        if x == 3:\n",
    "          noise_data_set.append(right_upper_arm)\n",
    "        if x == 4:\n",
    "          noise_data_set.append(right_lower_arm)\n",
    "        if x == 5:\n",
    "          noise_data_set.append(right_hand)\n",
    "\n",
    "        if x == 6:\n",
    "          noise_data_set.append(left_upper_leg)\n",
    "        if x == 7:\n",
    "          noise_data_set.append(left_lower_leg)\n",
    "        if x == 8:\n",
    "          noise_data_set.append(left_feet)\n",
    "\n",
    "\n",
    "        if x == 9:\n",
    "          noise_data_set.append(right_upper_leg)\n",
    "        if x == 10:\n",
    "          noise_data_set.append(right_lower_leg)\n",
    "        if x == 11:\n",
    "          noise_data_set.append(right_feet)\n",
    "\n",
    "\n",
    "        if x == 12:\n",
    "          noise_data_set.append(body)\n",
    "        if x == 13:\n",
    "          noise_data_set.append(head)\n",
    "\n",
    "# ===================================================================================================================================================\n",
    "\n",
    "      print('correct->',len(correct_data_set[x]),'  incorrect->',len(noise_data_set[x]))\n",
    "\n",
    "      rand_batches=concatenate_randomize_batches(correct_data_set[x],correct_data_set_label,noise_data_set,noise_data_set_label)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "      print('loop -> ',y,'/',loop)\n",
    "      X_train, X_test, y_train, y_test = train_test_split(rand_batches[0], rand_batches[1], test_size=0.2, random_state=42)\n",
    "\n",
    "      # history = model.fit(X_train[y*len(rand_batches[x][0]):len(rand_batches[x][0])*(y+1)], y_train[y*len(rand_batches[x][1]):len(rand_batches[x][1])*(y+1)], epochs=200, batch_size =128 , validation_data=(X_test[y*len(rand_batches[x][0]):len(rand_batches[x][0])*(y+1)], y_test[y*len(rand_batches[x][1]):len(rand_batches[x][1])*(y+1)]), callbacks=[custom_early_stopping])\n",
    "\n",
    "      if y == 0:\n",
    "        model_base_modifier = 10\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(model_base_modifier+len(aug4[0]), return_sequences=True, activation='relu',  input_shape=(len(correct_data_set[x][0]), len(correct_data_set[x][0][0]))))\n",
    "        model.add(LSTM(model_base_modifier+len(aug4[0]) + int(len(aug4[0]) - int(len(aug4[0]) * .4)), return_sequences=True,  activation='relu' ))\n",
    "        model.add(Bidirectional(LSTM(model_base_modifier+len(aug4[0]) - int(len(aug4[0]) - int(len(aug4[0]) * .5)), return_sequences=True, dropout=0.4, recurrent_dropout=0.4, activation='relu')))\n",
    "        model.add(LSTM(model_base_modifier+len(aug4[0]) - int(len(aug4[0]) - int(len(aug4[0]) * .4)), return_sequences=False,  activation='relu'))\n",
    "        # model.add(BatchNormalization())\n",
    "        model.add(Dense(model_base_modifier+len(aug4[0]) - int(len(aug4[0]) - int(len(aug4[0]) * .4)), activation='relu'))\n",
    "\n",
    "        model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "        custom_early_stopping = CustomEarlyStopping(accuracy_threshold=0.97, loss_threshold=0.05)\n",
    "        model.compile(optimizer = 'Adam' , loss = 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "      history = model.fit(X_train, y_train, epochs=150, batch_size =128 , validation_data=(X_test, y_test), callbacks=[custom_early_stopping])\n",
    "\n",
    "      val_loss = min(history.history['val_loss'])\n",
    "      val_accuracy = max(history.history['val_accuracy'])\n",
    "\n",
    "      if val_loss < best_val_loss and val_accuracy > best_val_accuracy:\n",
    "        best_model = model.get_weights()\n",
    "        best_val_loss = val_loss\n",
    "        best_val_accuracy = val_accuracy\n",
    "\n",
    "      model.set_weights(best_model)\n",
    "\n",
    "\n",
    "\n",
    "    model.save('testingModel')\n",
    "\n",
    "\n",
    "    # X_train = X_train[x].astype(np.float32)\n",
    "    # X_test = X_test[x].astype(np.float32)\n",
    "\n",
    "    # convert_tf_to_tflite('/content/testingModel',[1,len(X_train[0]),len(X_train[0][0])], X_test,data_set_name[x],id_num,val_loss,val_accuracy)\n",
    "    # convert_tf_to_tflite('/content/testingModel',[1,len(correct_data_set[x][0]),len(correct_data_set[x][0][0])], X_test,data_set_name[x],id_num,val_loss,val_accuracy)\n",
    "\n",
    "\n",
    "    X_train = X_train.astype(np.float32)\n",
    "    X_test = X_test.astype(np.float32)\n",
    "    model.save('testingModel')\n",
    "\n",
    "    print('param 1 ->',len(noise_data_set[0]) )\n",
    "    print('param 1 ->',len(noise_data_set[0][0]))\n",
    "    print('current x ->',x)\n",
    "    convert_tf_to_tflite('/content/testingModel',[1,len(noise_data_set[0]),len(noise_data_set[0][0])], X_test,data_set_name[x],id_num,best_val_loss,best_val_accuracy)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    clear_output(wait=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# uncomment this to run the individual landmarks training training(this means that it can determine individual parts of the body if spefic parts are incorrect or correct)\n",
    "# streamlined_process('/content/drive/MyDrive/Colab Notebooks/correct_new_2.txt','/content/drive/MyDrive/Colab Notebooks/wrong_new_2.txt')\n",
    "3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\CLARK\\Documents\\fitguidef\\pythonScriptTrain\\trainScript.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CLARK/Documents/fitguidef/pythonScriptTrain/trainScript.ipynb#W4sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m black_background \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(image_size, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39muint8)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CLARK/Documents/fitguidef/pythonScriptTrain/trainScript.ipynb#W4sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(temp1) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CLARK/Documents/fitguidef/pythonScriptTrain/trainScript.ipynb#W4sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39m# Convert coordinates to integers only when drawing\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/CLARK/Documents/fitguidef/pythonScriptTrain/trainScript.ipynb#W4sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     start_point \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\u001b[39mmap\u001b[39;49m(\u001b[39mint\u001b[39;49m, skeletal_coordinates[i]))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CLARK/Documents/fitguidef/pythonScriptTrain/trainScript.ipynb#W4sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     end_point \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mint\u001b[39m, skeletal_coordinates[i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m]))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CLARK/Documents/fitguidef/pythonScriptTrain/trainScript.ipynb#W4sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     cv2\u001b[39m.\u001b[39mline(black_background, start_point, end_point, (\u001b[39m255\u001b[39m, \u001b[39m255\u001b[39m, \u001b[39m255\u001b[39m), \u001b[39m2\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Mock skeletal coordinates (replace this with your actual data)\n",
    "base_data = txt_pre_process('firstExerciseFitguide(correct).txt',1,False,4)\n",
    "\n",
    "\n",
    "\n",
    "# skeletal_coordinates = np.array([[50, 50], [50, 100], [100, 100], [100, 50]])\n",
    "skeletal_coordinates = base_data[0][0][0]\n",
    "tempCtr = 0\n",
    "temp1=[]\n",
    "\n",
    "for x in range(33):\n",
    "    temp1.append([skeletal_coordinates[tempCtr],skeletal_coordinates[tempCtr+1]])\n",
    "    tempCtr = tempCtr + 2\n",
    "\n",
    "\n",
    "image_size = (200, 200, 3)\n",
    "black_background = np.zeros(image_size, dtype=np.uint8)\n",
    "\n",
    "for i in range(len(temp1) - 1):\n",
    "    # Convert coordinates to integers only when drawing\n",
    "    start_point = tuple(map(int, skeletal_coordinates[i]))\n",
    "    end_point = tuple(map(int, skeletal_coordinates[i + 1]))\n",
    "\n",
    "    cv2.line(black_background, start_point, end_point, (255, 255, 255), 2)\n",
    "\n",
    "# Create a black background image\n",
    "\n",
    "\n",
    "# Draw lines on the black background based on skeletal coordinates\n",
    "# for i in range(len(skeletal_coordinates) - 1):\n",
    "#     cv2.line(black_background, tuple(skeletal_coordinates[i]), tuple(skeletal_coordinates[i + 1]), (255, 255, 255), 2)\n",
    "\n",
    "# Display the image\n",
    "cv2.imshow('Skeletal Image', black_background)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.59652624, 0.02667674, 0.58421618, 0.00168468, 0.59267123, 0.00101575, 0.59962849, 0.0, 0.54127683, 0.00120362, 0.51851573, 0.00065259, 0.49172178, 0.00029337, 0.57099637, 0.00786889, 0.41215745, 0.01199619, 0.59732561, 0.04739432, 0.54323311, 0.0482398, 0.73056635, 0.13932864, 0.19639467, 0.14795567, 0.86508043, 0.31035553, 0.08993779, 0.32513696, 0.9544216, 0.47049416, 0.03687205, 0.48484136, 1.0, 0.52071026, 0.0, 0.53177139, 0.95038218, 0.52437813, 0.04261503, 0.53557986, 0.91838214, 0.50537482, 0.07422279, 0.51945688, 0.66757966, 0.4674395, 0.34961041, 0.47650171, 0.67116715, 0.71119287, 0.38583393, 0.70923163, 0.69751153, 0.91788413, 0.45857493, 0.9158063, 0.67752156, 0.94017888, 0.49370885, 0.93859664, 0.75245725, 0.99957656, 0.3469685, 1.0] ---- 0\n",
      "[0.60607397, 0.02864242, 0.60260547, 0.00267221, 0.61245055, 0.00228204, 0.62116926, 0.00160955, 0.55862231, 0.00137912, 0.5361607, 0.0005643, 0.50949435, 0.0, 0.5979642, 0.00893795, 0.42914406, 0.00988336, 0.60940093, 0.04877043, 0.55540959, 0.04924289, 0.74647659, 0.13464554, 0.22071441, 0.14189631, 0.88224742, 0.30934527, 0.11087524, 0.31727453, 0.95876999, 0.47356484, 0.0437211, 0.48249088, 1.0, 0.52393994, 0.0, 0.53045464, 0.94871017, 0.52700509, 0.05153056, 0.53355134, 0.91790247, 0.50884227, 0.08304762, 0.51909059, 0.68053486, 0.46199788, 0.37628513, 0.46692, 0.68551623, 0.7079692, 0.41577542, 0.70364314, 0.70720138, 0.91321088, 0.4783021, 0.91355947, 0.68676501, 0.93540225, 0.50687864, 0.93594724, 0.76332566, 0.99778645, 0.37241983, 1.0] ---- [0.59652624, 0.02667674, 0.58421618, 0.00168468, 0.59267123, 0.00101575, 0.59962849, 0.0, 0.54127683, 0.00120362, 0.51851573, 0.00065259, 0.49172178, 0.00029337, 0.57099637, 0.00786889, 0.41215745, 0.01199619, 0.59732561, 0.04739432, 0.54323311, 0.0482398, 0.73056635, 0.13932864, 0.19639467, 0.14795567, 0.86508043, 0.31035553, 0.08993779, 0.32513696, 0.9544216, 0.47049416, 0.03687205, 0.48484136, 1.0, 0.52071026, 0.0, 0.53177139, 0.95038218, 0.52437813, 0.04261503, 0.53557986, 0.91838214, 0.50537482, 0.07422279, 0.51945688, 0.66757966, 0.4674395, 0.34961041, 0.47650171, 0.67116715, 0.71119287, 0.38583393, 0.70923163, 0.69751153, 0.91788413, 0.45857493, 0.9158063, 0.67752156, 0.94017888, 0.49370885, 0.93859664, 0.75245725, 0.99957656, 0.3469685, 1.0]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'>=' not supported between instances of 'int' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\CLARK\\Documents\\fitguidef\\pythonScriptTrain\\trainScript.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CLARK/Documents/fitguidef/pythonScriptTrain/trainScript.ipynb#W5sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mfor\u001b[39;00m sequenceList \u001b[39min\u001b[39;00m base_data:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CLARK/Documents/fitguidef/pythonScriptTrain/trainScript.ipynb#W5sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39mprint\u001b[39m(sequenceList,\u001b[39m\"\u001b[39m\u001b[39m----\u001b[39m\u001b[39m\"\u001b[39m,highestSeq)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/CLARK/Documents/fitguidef/pythonScriptTrain/trainScript.ipynb#W5sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39;49m(sequenceList) \u001b[39m>\u001b[39;49m\u001b[39m=\u001b[39;49m highestSeq:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CLARK/Documents/fitguidef/pythonScriptTrain/trainScript.ipynb#W5sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m         highestSeq \u001b[39m=\u001b[39m sequenceList\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CLARK/Documents/fitguidef/pythonScriptTrain/trainScript.ipynb#W5sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mfor\u001b[39;00m sequenceList \u001b[39min\u001b[39;00m base_data[\u001b[39m0\u001b[39m]:\n",
      "\u001b[1;31mTypeError\u001b[0m: '>=' not supported between instances of 'int' and 'list'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "height = 500\n",
    "width = 200\n",
    "image_size = (height, width, 3)\n",
    "\n",
    "highestSeq = 0\n",
    "temp_H_seq = []\n",
    "\n",
    "black_background = np.zeros(image_size, dtype=np.uint8)\n",
    "\n",
    "\n",
    "base_data = txt_pre_process('firstExerciseFitguide(correct).txt', 1, False, 4)\n",
    "base_data = base_data[0][40]\n",
    "temp1=[]\n",
    "tempCtr = 0\n",
    "# print(\"len-->\",len(base_data))\n",
    "# print(len(base_data[0]))\n",
    "\n",
    "for sequenceList in base_data:\n",
    "    \n",
    "    print(sequenceList,\"----\",highestSeq)\n",
    "    if len(sequenceList) >= highestSeq:\n",
    "        highestSeq = sequenceList\n",
    "\n",
    "for sequenceList in base_data[0]:\n",
    "    if len(sequenceList) == highestSeq:\n",
    "        temp_H_seq.append(sequenceList)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for y in range(len(sequenceList)):\n",
    "    black_background = np.zeros(image_size, dtype=np.uint8)\n",
    "\n",
    "    tempCtr =0\n",
    "    temp1 = []\n",
    "    for x in range(33):\n",
    "        temp1.append([int(temp_H_seq[y][tempCtr]*width),int(temp_H_seq[y][tempCtr+1]*height)])\n",
    "        tempCtr = tempCtr + 2\n",
    "        print(len(temp1))\n",
    "\n",
    "    # left foot\n",
    "    cv2.line(black_background, temp1[32], temp1[30], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[30], temp1[28], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[32], temp1[28], (255, 255, 255), 2)\n",
    "\n",
    "    # right Foot\n",
    "    cv2.line(black_background, temp1[27], temp1[31], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[31], temp1[29], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[29], temp1[27], (255, 255, 255), 2)\n",
    "\n",
    "    # left leg\n",
    "    cv2.line(black_background, temp1[28], temp1[26], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[26], temp1[24], (255, 255, 255), 2)\n",
    "\n",
    "    # right leg\n",
    "    cv2.line(black_background, temp1[27], temp1[25], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[25], temp1[23], (255, 255, 255), 2)\n",
    "\n",
    "    # body\n",
    "    cv2.line(black_background, temp1[24], temp1[23], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[23], temp1[11], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[11], temp1[12], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[12], temp1[24], (255, 255, 255), 2)\n",
    "\n",
    "    # left arm\n",
    "    cv2.line(black_background, temp1[12], temp1[14], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[14], temp1[16], (255, 255, 255), 2)\n",
    "\n",
    "    # left hand\n",
    "    cv2.line(black_background, temp1[16], temp1[18], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[18], temp1[20], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[20], temp1[16], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[16], temp1[22], (255, 255, 255), 2)\n",
    "\n",
    "    # right arm\n",
    "    cv2.line(black_background, temp1[11], temp1[13], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[13], temp1[15], (255, 255, 255), 2)\n",
    "\n",
    "    # right hand\n",
    "    cv2.line(black_background, temp1[15], temp1[17], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[17], temp1[19], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[19], temp1[15], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[15], temp1[21], (255, 255, 255), 2)\n",
    "\n",
    "    # face\n",
    "    cv2.line(black_background, temp1[7], temp1[3], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[3], temp1[2], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[2], temp1[1], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[0], temp1[4], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[0], temp1[4], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[4], temp1[5], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[5], temp1[6], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[6], temp1[7], (255, 255, 255), 2)\n",
    "    cv2.line(black_background, temp1[7], temp1[8], (255, 255, 255), 2)\n",
    "\n",
    "    # mouth\n",
    "    cv2.line(black_background, temp1[10], temp1[9], (255, 255, 255), 2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    cv2.imshow('Skeletal Image', black_background)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\CLARK\\Documents\\fitguidef\\pythonScriptTrain\\trainScript.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CLARK/Documents/fitguidef/pythonScriptTrain/trainScript.ipynb#X10sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mfor\u001b[39;00m y \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(base_data)):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CLARK/Documents/fitguidef/pythonScriptTrain/trainScript.ipynb#X10sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m33\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/CLARK/Documents/fitguidef/pythonScriptTrain/trainScript.ipynb#X10sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         temp1\u001b[39m.\u001b[39mappend([\u001b[39mint\u001b[39;49m(base_data[y][tempCtr]\u001b[39m*\u001b[39;49mwidth),\u001b[39mint\u001b[39m(base_data[y][tempCtr\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m*\u001b[39mheight)])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CLARK/Documents/fitguidef/pythonScriptTrain/trainScript.ipynb#X10sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m         tempCtr \u001b[39m=\u001b[39m tempCtr \u001b[39m+\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CLARK/Documents/fitguidef/pythonScriptTrain/trainScript.ipynb#X10sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m         \u001b[39mprint\u001b[39m(temp1)\n",
      "\u001b[1;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'list'"
     ]
    }
   ],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# height = 500\n",
    "# width = 200\n",
    "# image_size = (height, width, 3)\n",
    "\n",
    "# black_background = np.zeros(image_size, dtype=np.uint8)\n",
    "\n",
    "\n",
    "# base_data = txt_pre_process('firstExerciseFitguide(correct).txt', 1, False, 4)\n",
    "# base_data = base_data[0]\n",
    "# temp1=[]\n",
    "# tempCtr = 0\n",
    "\n",
    "# print(len(base_data))\n",
    "\n",
    "\n",
    "# for y in range(len(base_data)):\n",
    "#     for x in range(33):\n",
    "#         temp1.append([int(base_data[y][tempCtr]*width),int(base_data[y][tempCtr+1]*height)])\n",
    "#         tempCtr = tempCtr + 2\n",
    "#         print(temp1)\n",
    "\n",
    "#     # left foot\n",
    "#     cv2.line(black_background, temp1[32], temp1[30], (255, 255, 255), 2)\n",
    "#     cv2.line(black_background, temp1[30], temp1[28], (255, 255, 255), 2)\n",
    "#     cv2.line(black_background, temp1[32], temp1[28], (255, 255, 255), 2)\n",
    "\n",
    "#     # right Foot\n",
    "#     cv2.line(black_background, temp1[27], temp1[31], (255, 255, 255), 2)\n",
    "#     cv2.line(black_background, temp1[31], temp1[29], (255, 255, 255), 2)\n",
    "#     cv2.line(black_background, temp1[29], temp1[27], (255, 255, 255), 2)\n",
    "\n",
    "#     # left leg\n",
    "#     cv2.line(black_background, temp1[28], temp1[26], (255, 255, 255), 2)\n",
    "#     cv2.line(black_background, temp1[26], temp1[24], (255, 255, 255), 2)\n",
    "\n",
    "#     # right leg\n",
    "#     cv2.line(black_background, temp1[27], temp1[25], (255, 255, 255), 2)\n",
    "#     cv2.line(black_background, temp1[25], temp1[23], (255, 255, 255), 2)\n",
    "\n",
    "#     # body\n",
    "#     cv2.line(black_background, temp1[24], temp1[23], (255, 255, 255), 2)\n",
    "#     cv2.line(black_background, temp1[23], temp1[11], (255, 255, 255), 2)\n",
    "#     cv2.line(black_background, temp1[11], temp1[12], (255, 255, 255), 2)\n",
    "#     cv2.line(black_background, temp1[12], temp1[24], (255, 255, 255), 2)\n",
    "\n",
    "#     # left arm\n",
    "#     cv2.line(black_background, temp1[12], temp1[14], (255, 255, 255), 2)\n",
    "#     cv2.line(black_background, temp1[14], temp1[16], (255, 255, 255), 2)\n",
    "\n",
    "#     # left hand\n",
    "#     cv2.line(black_background, temp1[17], temp1[18], (255, 255, 255), 2)\n",
    "#     cv2.line(black_background, temp1[18], temp1[20], (255, 255, 255), 2)\n",
    "#     cv2.line(black_background, temp1[20], temp1[16], (255, 255, 255), 2)\n",
    "#     cv2.line(black_background, temp1[16], temp1[23], (255, 255, 255), 2)\n",
    "\n",
    "#     # right arm\n",
    "#     cv2.line(black_background, temp1[11], temp1[13], (255, 255, 255), 2)\n",
    "#     cv2.line(black_background, temp1[13], temp1[15], (255, 255, 255), 2)\n",
    "\n",
    "#     # right hand\n",
    "#     cv2.line(black_background, temp1[15], temp1[17], (255, 255, 255), 2)\n",
    "#     cv2.line(black_background, temp1[17], temp1[19], (255, 255, 255), 2)\n",
    "#     cv2.line(black_background, temp1[19], temp1[15], (255, 255, 255), 2)\n",
    "#     cv2.line(black_background, temp1[15], temp1[21], (255, 255, 255), 2)\n",
    "\n",
    "#     # face\n",
    "#     cv2.line(black_background, temp1[7], temp1[3], (255, 255, 255), 2)\n",
    "#     cv2.line(black_background, temp1[3], temp1[2], (255, 255, 255), 2)\n",
    "#     cv2.line(black_background, temp1[2], temp1[1], (255, 255, 255), 2)\n",
    "#     cv2.line(black_background, temp1[0], temp1[4], (255, 255, 255), 2)\n",
    "#     cv2.line(black_background, temp1[0], temp1[4], (255, 255, 255), 2)\n",
    "#     cv2.line(black_background, temp1[4], temp1[5], (255, 255, 255), 2)\n",
    "#     cv2.line(black_background, temp1[5], temp1[6], (255, 255, 255), 2)\n",
    "#     cv2.line(black_background, temp1[6], temp1[7], (255, 255, 255), 2)\n",
    "#     cv2.line(black_background, temp1[7], temp1[8], (255, 255, 255), 2)\n",
    "\n",
    "#     # mouth\n",
    "#     cv2.line(black_background, temp1[10], temp1[9], (255, 255, 255), 2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     cv2.imshow('Skeletal Image', black_background)\n",
    "#     cv2.waitKey(0)\n",
    "#     cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'base_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\CLARK\\Documents\\fitguidef\\pythonScriptTrain\\trainScript.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/CLARK/Documents/fitguidef/pythonScriptTrain/trainScript.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Assuming the coordinates range from 0 to 1\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/CLARK/Documents/fitguidef/pythonScriptTrain/trainScript.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m skeletal_coordinates \u001b[39m=\u001b[39m base_data[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/CLARK/Documents/fitguidef/pythonScriptTrain/trainScript.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Scale relative coordinates to pixel coordinates\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/CLARK/Documents/fitguidef/pythonScriptTrain/trainScript.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m scaled_coordinates \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(skeletal_coordinates) \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39marray(image_size[:\u001b[39m2\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'base_data' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assuming the coordinates range from 0 to 1\n",
    "skeletal_coordinates = base_data[0][0][0]\n",
    "\n",
    "\n",
    "# Scale relative coordinates to pixel coordinates\n",
    "scaled_coordinates = np.array(skeletal_coordinates) * np.array(image_size[:2])\n",
    "\n",
    "tempCtr = 0\n",
    "temp1=[]\n",
    "\n",
    "for x in range(33):\n",
    "    temp1.append([scaled_coordinates[tempCtr],scaled_coordinates[tempCtr+1]])\n",
    "    tempCtr = tempCtr + 2\n",
    "\n",
    "# Convert to integers for drawing\n",
    "scaled_coordinates = scaled_coordinates.astype(int)\n",
    "\n",
    "# Create a black background image\n",
    "image_size = (200, 200, 3)\n",
    "black_background = np.zeros(image_size, dtype=np.uint8)\n",
    "\n",
    "# Draw lines on the black background based on scaled coordinates\n",
    "for i in range(len(scaled_coordinates) - 1):\n",
    "    start_point = tuple(scaled_coordinates[i])\n",
    "    end_point = tuple(scaled_coordinates[i + 1])\n",
    "\n",
    "    cv2.line(black_background, start_point, end_point, (255, 255, 255), 2)\n",
    "\n",
    "# Display the image\n",
    "cv2.imshow('Skeletal Image', black_background)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediapipe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
