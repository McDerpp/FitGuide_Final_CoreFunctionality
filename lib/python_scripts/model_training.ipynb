{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Bidirectional, BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard,ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "import copy\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "# import mediapipe as mp\n",
    "from IPython.display import clear_output\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from tensorflow_model_optimization.quantization.keras import quantize_model\n",
    "from collections import Counter\n",
    "import random as rand\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def txt_pre_process(txt_file,label,simplify=False,simplify_level=14 ):\n",
    "    label_array = []\n",
    "    temp_feature_data = []\n",
    "    temp_sequence_data = []\n",
    "    batch_data = []\n",
    "\n",
    "    with open(str(txt_file), 'r') as file:\n",
    "\n",
    "        for line in file:\n",
    "            values = line.strip().split('|')\n",
    "\n",
    "            temp_feature_data = []\n",
    "\n",
    "            for value in values:\n",
    "                float_value = str(value)\n",
    "\n",
    "                #FIRST PART OF THE SEQUENCE\n",
    "                if float_value == 'START':\n",
    "                    temp_sequence_data=[]\n",
    "\n",
    "                elif float_value == 'END':\n",
    "                    batch_data.append(temp_sequence_data)\n",
    "                    label_array.append(label)\n",
    "\n",
    "\n",
    "                elif float_value != '' and float_value != 'START':\n",
    "                    if simplify:\n",
    "                        float_value = round(float(value),simplify_level)\n",
    "                    else:\n",
    "                        float_value = float(value)\n",
    "                    temp_feature_data.append(float_value)\n",
    "\n",
    "            if temp_feature_data!=[]:\n",
    "                temp_sequence_data.append(temp_feature_data)\n",
    "\n",
    "    label_array = np.array(label_array)\n",
    "    return [batch_data,label_array]\n",
    "\n",
    "#--------------------------------------------------------------------------- paddingV1 --------------------------------------------------------------------------------\n",
    "# padding can be improved probably...by using sequence\n",
    "# minor issue:\n",
    "# > is whether sequences had exceeded the intended number of sequences but is still right (it was performed right but slower(by an acceptable margin)) - not resolved\n",
    "#    = temporary fix was just to truncate everything if it had exceeded the intended number of sequence for the sake of running it for now\n",
    "#    = a reliable solution in theory could be that to randomly truncate in between the first and end sequence, in this way relevant data can be captured\n",
    "def padding(pre_processed_input,optional_maxLength=0):\n",
    "    padded_sequences = []\n",
    "    if optional_maxLength != 0:\n",
    "        max_length = optional_maxLength\n",
    "    else:\n",
    "        max_length = max(len(sequence) for sequence in pre_processed_input)\n",
    "\n",
    "    for sequence in pre_processed_input:\n",
    "        padding_length = max_length - len(sequence)\n",
    "        if padding_length >= 0:\n",
    "            padded_sequence = np.pad(sequence, ((0, padding_length), (0, 0)), mode='constant')\n",
    "\n",
    "        else:\n",
    "            padded_sequence = sequence[:max_length]\n",
    "        padded_sequences.append(padded_sequence)\n",
    "    padded_sequences = np.array(padded_sequences)\n",
    "\n",
    "    return padded_sequences\n",
    "\n",
    "#--------------------------------------------------------------------------- paddingV1 --------------------------------------------------------------------------------\n",
    "\n",
    "# this is to merge correct executions and wrong executions and randomize their input and label\n",
    "# positions of input and its corresponding label are the same\n",
    "# introducing noise/wrong input makes the model more robust\n",
    "def concatenate_randomize_batches(base_input,base_label,concat_input,concat_label):\n",
    "    combined_inputs = np.concatenate((base_input,concat_input), axis = 0)\n",
    "    combined_label = np.concatenate((base_label,concat_label), axis = 0)\n",
    "    indices = np.random.permutation(len(combined_inputs))\n",
    "    randomized_inputs = combined_inputs[indices]\n",
    "    randomized_label = combined_label[indices]\n",
    "    return [randomized_inputs,randomized_label]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def tally_sequence(sequence_array):\n",
    "    tally_number = []\n",
    "    tally_ctr = []\n",
    "\n",
    "    for x in sequence_array:\n",
    "        temp = len(x)\n",
    "        if temp not in tally_number:\n",
    "            tally_number.append(temp)\n",
    "            tally_ctr.append(1)\n",
    "        else:\n",
    "            for y in range(len(tally_number)) :\n",
    "                if temp == tally_number[y]:\n",
    "                    tally_ctr[y] = tally_ctr[y] + 1\n",
    "\n",
    "    tally_max = 0\n",
    "    tally_number_arranged = []\n",
    "    tally_ctr_arranged = []\n",
    "\n",
    "    for x in range(len(tally_number)):\n",
    "        # print(len(tally_ctr))\n",
    "        tally_max = max(tally_ctr)\n",
    "        for y in range(len(tally_number)):\n",
    "            if tally_ctr[y] == tally_max:\n",
    "                tally_number_arranged.append(tally_number[y])\n",
    "                tally_ctr_arranged.append(tally_ctr[y])\n",
    "                tally_ctr.pop(y)\n",
    "                tally_number.pop(y)\n",
    "                break\n",
    "\n",
    "    total_ctr = 0\n",
    "    for x in tally_ctr:\n",
    "        total_ctr = total_ctr + x\n",
    "\n",
    "\n",
    "    for x in range(len(tally_number_arranged)):\n",
    "        print(tally_number_arranged[x],'-->',tally_ctr_arranged[x])\n",
    "\n",
    "\n",
    "# outlier detection and removal (currently being used)\n",
    "def common_length_sequence(sequences_array,threshold = 2):\n",
    "    temp = []\n",
    "\n",
    "    data = [len(seq) for seq in sequences_array]\n",
    "    data_frequency = Counter(data)\n",
    "    most_common_data = data_frequency.most_common()\n",
    "    outlier_frequencies = [value for value, freq in data_frequency.items() if freq < threshold]\n",
    "    most_common_values = [value for value, freq in most_common_data if freq >= threshold]\n",
    "\n",
    "    print(\"Most Common Data Points:\", most_common_values)\n",
    "    print(\"Outlier Frequencies:\", outlier_frequencies)\n",
    "\n",
    "    for x in sequences_array:\n",
    "        if len(x) in most_common_values:\n",
    "            temp.append(x)\n",
    "    print('-------------------applied frequency outlier detection-------------------')\n",
    "    print(\"original num -> \", len(sequences_array))\n",
    "    print(\"current num -> \", len(temp))\n",
    "    print(\"removed num -> \", len(sequences_array) - len(temp))\n",
    "    return temp\n",
    "\n",
    "# outlier detection and removal (currently being used)\n",
    "def apply_z_score(sequences_array,z_score_threshold = 1):\n",
    "    data_points = []\n",
    "    included_datapoints = []\n",
    "    updated_sequences =[]\n",
    "\n",
    "    for x in sequences_array:\n",
    "        temp = len(x)\n",
    "        if temp not in data_points:\n",
    "            data_points.append(temp)\n",
    "\n",
    "    data = np.array(data_points)\n",
    "    mean_value = np.mean(data)\n",
    "    standard_deviation = np.std(data)\n",
    "    z_scores = (data - mean_value) / standard_deviation\n",
    "    for x in range(len(z_scores)):\n",
    "        if np.abs(z_scores[x]) <= z_score_threshold:\n",
    "            included_datapoints.append(data[x])\n",
    "\n",
    "\n",
    "    for x in sequences_array:\n",
    "        if len(x) in included_datapoints:\n",
    "            updated_sequences.append(x)\n",
    "    print('-------------------applied z-score outlier detection-------------------')\n",
    "    print(\"datapoints included -> \", included_datapoints)\n",
    "    print(\"original num -> \", len(sequences_array))\n",
    "    print(\"current num -> \", len(updated_sequences))\n",
    "    print(\"removed num -> \", len(sequences_array) - len(updated_sequences))\n",
    "\n",
    "    return updated_sequences\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def paddingV2(sequences_array_input,optional_maxlength = 0):\n",
    "    sequences_array = copy.deepcopy(sequences_array_input)\n",
    "\n",
    "\n",
    "    output = []\n",
    "    max_length = 0\n",
    "    if optional_maxlength == 0:\n",
    "        max_length = max(len(sequence) for sequence in sequences_array)\n",
    "        expanded_max_length = int(max_length+ ((max_length) * .10))\n",
    "    else:\n",
    "        expanded_max_length = optional_maxlength\n",
    "\n",
    "    # sequence = np.array(sequences_array)\n",
    "\n",
    "    print(expanded_max_length)\n",
    "\n",
    "\n",
    "    padding_length_before = 0\n",
    "    padding_length_after = 0\n",
    "\n",
    "    for seq in sequences_array:\n",
    "        # print(seq)\n",
    "        for x in range(expanded_max_length-len(seq)+1):\n",
    "            padding_length_before = x\n",
    "            padding_length_after = expanded_max_length - len(seq) - x\n",
    "            padded_sequence = np.pad(seq, ((padding_length_before, padding_length_after),(0,0)), mode='constant')\n",
    "            output.append(padded_sequence)\n",
    "\n",
    "            # print(padded_sequence)\n",
    "    print('------------------------applied paddingV2------------------------')\n",
    "    print('max_length -> ', max_length)\n",
    "    print('expanded_max_length -> ', expanded_max_length)\n",
    "    print('original num set of sequences -> ', len(sequences_array))\n",
    "    print('final num set of sequences -> ', len(output))\n",
    "\n",
    "    output = np.array(output)\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def convert_tf_to_tflite(tf_model,input_shape,test_dataset,name,id_number,validation_loss,validation_accuracy):\n",
    "  model = tf.keras.models.load_model(tf_model)\n",
    "\n",
    "  run_model = tf.function(lambda x: model(x))\n",
    "  # This is important, let's fix the input size.\n",
    "  BATCH_SIZE = input_shape[0]\n",
    "  STEPS = input_shape[1]\n",
    "  INPUT_SIZE = input_shape[2]\n",
    "  concrete_func = run_model.get_concrete_function(\n",
    "      tf.TensorSpec([BATCH_SIZE, STEPS, INPUT_SIZE], model.inputs[0].dtype))\n",
    "\n",
    "  # model directory.\n",
    "  MODEL_DIR = \"keras_lstm\"\n",
    "  model.save(MODEL_DIR, save_format=\"tf\", signatures=concrete_func)\n",
    "\n",
    "  converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR)\n",
    "  tflite_model = converter.convert()\n",
    "\n",
    "\n",
    "  # Run the model with TensorFlow to get expected results.\n",
    "  TEST_CASES = 10\n",
    "\n",
    "  # Run the model with TensorFlow Lite\n",
    "  interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "  interpreter.allocate_tensors()\n",
    "  input_details = interpreter.get_input_details()\n",
    "  output_details = interpreter.get_output_details()\n",
    "\n",
    "  for i in range(TEST_CASES):\n",
    "    expected = model.predict(test_dataset[i:i+1])\n",
    "    interpreter.set_tensor(input_details[0][\"index\"], test_dataset[i:i+1, :, :])\n",
    "    interpreter.invoke()\n",
    "    result = interpreter.get_tensor(output_details[0][\"index\"])\n",
    "\n",
    "    # Assert if the result of TFLite model is consistent with the TF model.\n",
    "    np.testing.assert_almost_equal(expected, result, decimal=5)\n",
    "    print(\"Done. The result of TensorFlow matches the result of TensorFlow Lite.\")\n",
    "\n",
    "    interpreter.reset_all_variables()\n",
    "\n",
    "\n",
    "\n",
    "  temp = 'converted_model_'\n",
    "\n",
    "  temp3 = temp + str(name) + id_number + \"(loss_\"+ str(round(validation_loss,3)) +\")\" + \"(acc_\"+  str(round(validation_accuracy,3 )) + \")\" + '.tflite'\n",
    "  # Save the TFLite model to a file\n",
    "  with open(temp3, \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "  # with open(\"converted_model.tflite\", \"wb\") as f:\n",
    "  #     f.write(tflite_model)\n",
    "\n",
    "\n",
    "# this data augmentation replaces padded index with random inputs\n",
    "def populate_0_input(correct_data_input,noise_data_input):\n",
    "    correct_data = copy.deepcopy(correct_data_input)\n",
    "    noise_data = copy.deepcopy(noise_data_input)\n",
    "\n",
    "    print(len(correct_data))\n",
    "    index = 10\n",
    "    temp = []\n",
    "    temp_compilation = []\n",
    "    ctr = 0\n",
    "    rand_modifier =0\n",
    "\n",
    "    for set_sequence in tqdm(correct_data, desc=\"data_aug_coor_sensitivity\", leave=True):\n",
    "        rand_modifier = rand.randint(0,len(noise_data))\n",
    "\n",
    "        for x in range(len(set_sequence)):\n",
    "            ctr = ctr + 1\n",
    "            if set_sequence[x][0] == 0:\n",
    "                temp.append(noise_data[rand_modifier-1][rand.randint(0,len(noise_data[rand_modifier-1])-1)])\n",
    "\n",
    "            else:\n",
    "                temp.append(set_sequence[x])\n",
    "\n",
    "        temp_compilation.append(temp)\n",
    "        temp =[]\n",
    "\n",
    "\n",
    "    return temp_compilation\n",
    "\n",
    "\n",
    "\n",
    "class CustomEarlyStopping(Callback):\n",
    "  def __init__(self, accuracy_threshold=0.95, loss_threshold=0.10):\n",
    "      super(CustomEarlyStopping, self).__init__()\n",
    "      self.accuracy_threshold = accuracy_threshold\n",
    "      self.loss_threshold = loss_threshold\n",
    "\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "      if logs is None:\n",
    "          logs = {}\n",
    "\n",
    "      if logs.get('val_accuracy') is None or logs.get('val_loss') is None:\n",
    "          return\n",
    "\n",
    "      if logs.get('val_accuracy') >= self.accuracy_threshold and logs.get('val_loss') <= self.loss_threshold:\n",
    "          self.model.stop_training = True\n",
    "          print(f\"\\nTraining stopped as validation accuracy reached {logs.get('val_accuracy'):.4f} \"\n",
    "                f\"and validation loss reached {logs.get('val_loss'):.4f}\")\n",
    "\n",
    "\n",
    "# this data augmentation augments data to simulate if the sequence were follow,whether there is unecessary movements in between.\n",
    "# this would also reinforce the model to detect certain action that were classified as correct instead of incorrect\n",
    "\n",
    "def data_aug_sensitivity(sequence_array_list_input,noise_sequence_list_input,num_data_aug = 3,num_aug_in_1_seq = 3,noise_seq_len = 2):\n",
    "  sequence_array_list = copy.deepcopy(sequence_array_list_input)\n",
    "  noise_sequence_list = copy.deepcopy(noise_sequence_list_input)\n",
    "\n",
    "  compile = []\n",
    "  temp_seq = []\n",
    "  temp_storage = []\n",
    "  temp_rand = []\n",
    "  num = 0\n",
    "  ctr1111 = 0\n",
    "  temp_rand2 = 0\n",
    "  temp_rand3 = 0\n",
    "\n",
    "# per sequences\n",
    "  for sequence in tqdm(sequence_array_list, desc=\"data_aug_seq_sensitivity\", leave=True):\n",
    "    # loops for the number of data augmentation per sequence\n",
    "    for ctr in range(num_data_aug):\n",
    "      # loops for the amount of number of augmentation in the sequence(loops to get random index)\n",
    "      while len(temp_rand)!=num_aug_in_1_seq:\n",
    "        num = rand.randint(0,len(sequence)-1)\n",
    "        if num in temp_rand:\n",
    "          continue\n",
    "        else:\n",
    "          temp_rand.append(num)\n",
    "\n",
    "      #actual augmentation of the sequence\n",
    "      temp_seq = sequence.copy()\n",
    "      # store in a temp variable and to be edited\n",
    "\n",
    "      # number of augmentation to be done in a sequence\n",
    "      for ctr1 in range(len(temp_rand)):\n",
    "\n",
    "        # number of sequence to be expanded(index + number of noise_seq_len)\n",
    "        for ctr2 in range(noise_seq_len):\n",
    "          temp_rand2 = rand.randint(0,len(noise_sequence_list)-1)\n",
    "          temp_rand3 = rand.randint(0,len(noise_sequence_list[0])-1)\n",
    "\n",
    "          if (temp_rand[ctr1] + ctr2) < len(temp_seq):\n",
    "            temp_seq[temp_rand[ctr1] + ctr2] = noise_sequence_list[temp_rand2][temp_rand3]\n",
    "\n",
    "\n",
    "          else:\n",
    "            continue\n",
    "\n",
    "\n",
    "      compile.append(temp_seq)\n",
    "      temp_seq = []\n",
    "      temp_rand = []\n",
    "\n",
    "\n",
    "  return compile\n",
    "\n",
    "\n",
    "def data_aug_coor_sensitivity(sequence_array_list_input,num_to_aug=2,num_coor_edit=3,num_sequence_edit=2):\n",
    "  sequence_array_list = copy.deepcopy(sequence_array_list_input)\n",
    "\n",
    "  compile = []\n",
    "  temp = []\n",
    "  rand_coor = []\n",
    "\n",
    "\n",
    "  for ctr in tqdm(range(num_to_aug), desc=\"data_aug_coor_sensitivity\", leave=True):\n",
    "    for sequence in sequence_array_list:\n",
    "      for ctr3 in range(num_sequence_edit):\n",
    "        what_sequence = rand.randint(0,len(sequence)-1)\n",
    "        for ctr2 in range(num_coor_edit):\n",
    "          what_coor = rand.randint(0,len(sequence[0])-1)\n",
    "          rand_coor = rand.randint(0,9999999999)\n",
    "          rand_coor = rand_coor / (10 ** len(str(rand_coor)))\n",
    "          sequence[what_sequence][what_coor]=rand_coor\n",
    "      compile.append(sequence)\n",
    "  return compile\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp =[[[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5]],[[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5]],[[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5]]]\n",
    "temp_noise =[[[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0]],[[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0]],[[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0]]]\n",
    "print(len(temp))\n",
    "aug = data_aug_sensitivity(temp,temp_noise,2,3,2)\n",
    "\n",
    "# aug = data_aug_coor_sensitivity(temp)\n",
    "# def data_aug_sensitivity(sequence_array_list,noise_sequence_list,num_data_aug = 3,num_aug_in_1_seq = 3,noise_seq_len = 2):\n",
    "for x in range(len(temp)):\n",
    "  print('')\n",
    "  print('original -> ',temp[x])\n",
    "  print('======================')\n",
    "  print('aug -> ',aug[x*2],'------------->',x*2)\n",
    "  print('aug -> ',aug[x*2+1],'----------->',x*2+1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('qweqwe')\n",
    "print(len(aug))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_to_aug=2\n",
    "num_coor_edit=3\n",
    "num_sequence_edit=2\n",
    "print('test')\n",
    "temp = [[[1,2,3,4],[1,2,3,4],[1,2,3,4]],[[1,2,3,4],[1,2,3,4],[1,2,3,4]],[[1,2,3,4],[1,2,3,4],[1,2,3,4]]]\n",
    "compile = []\n",
    "\n",
    "rand_coor = []\n",
    "for ctr in range(num_to_aug):\n",
    "  print('aug-----------')\n",
    "  for sequence in temp:\n",
    "    for ctr3 in range(num_sequence_edit):\n",
    "      what_sequence = rand.randint(0,len(sequence)-1)\n",
    "      for ctr2 in range(num_coor_edit):\n",
    "        what_coor = rand.randint(0,len(sequence[0])-1)\n",
    "        rand_coor = rand.randint(0,9999999999)\n",
    "        rand_coor = rand_coor / (10 ** len(str(rand_coor)))\n",
    "\n",
    "        sequence[what_sequence][what_coor]=rand_coor\n",
    "      print(sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 0 1  -> 0\n",
    "# 2 3  -> 1\n",
    "# 4 5  -> 2\n",
    "# 6 7  -> 3\n",
    "# 8 9  -> 4\n",
    "# 10 11  -> 5\n",
    "# 12 13  -> 6\n",
    "# 14 15  -> 7\n",
    "# 16 17  -> 8\n",
    "# 18 19  -> 9\n",
    "# 20 21  -> 10\n",
    "# 22 23  -> 11\n",
    "# 24 25  -> 12\n",
    "# 26 27  -> 13\n",
    "# 28 29  -> 14\n",
    "# 30 31  -> 15\n",
    "# 32 33  -> 16\n",
    "# 34 35  -> 17\n",
    "# 36 37  -> 18\n",
    "# 38 39  -> 19\n",
    "# 40 41  -> 20\n",
    "# 42 43  -> 21\n",
    "# 44 45  -> 22\n",
    "# 46 47  -> 23\n",
    "# 48 49  -> 24\n",
    "# 50 51  -> 25\n",
    "# 52 53  -> 26\n",
    "# 54 55  -> 27\n",
    "# 56 57  -> 28\n",
    "# 58 59  -> 29\n",
    "# 60 61  -> 30\n",
    "# 62 63  -> 31\n",
    "# 64 65  -> 32\n",
    "\n",
    "# # ----------------------------------------------------------------------------\n",
    "#   left_upper_arm_sequence = []\n",
    "#   left_lower_arm_sequence = []\n",
    "#   left_hand_sequence = []\n",
    "\n",
    "#   right_upper_arm_sequence = []\n",
    "#   right_lower_arm_sequence = []\n",
    "#   right_hand_sequence = []\n",
    "\n",
    "#   left_upper_leg_sequence = []\n",
    "#   left_lower_leg_sequence = []\n",
    "#   left_feet_sequence = []\n",
    "\n",
    "#   right_upper_leg_sequence = []\n",
    "#   right_lower_leg_sequence = []\n",
    "#   right_feet_sequence = []\n",
    "\n",
    "#   body_sequence = []\n",
    "#   head_sequence = []\n",
    "\n",
    "#   # ------------------------------------------------------------------------------\n",
    "#   left_upper_arm_sequence_noise = []\n",
    "#   left_lower_arm_sequence_noise = []\n",
    "#   left_hand_sequence_noise = []\n",
    "\n",
    "#   right_upper_arm_sequence_noise = []\n",
    "#   right_lower_arm_sequence_noise = []\n",
    "#   right_hand_sequence_noise = []\n",
    "\n",
    "#   left_upper_leg_sequence_noise = []\n",
    "#   left_lower_leg_sequence_noise = []\n",
    "#   left_feet_sequence_noise = []\n",
    "\n",
    "#   right_upper_leg_sequence_noise = []\n",
    "#   right_lower_leg_sequence_noise = []\n",
    "#   right_feet_sequence_noise = []\n",
    "\n",
    "#   body_sequence_noise = []\n",
    "#   head_sequence_noise = []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is as a whole exercise\n",
    "def streamlined_process(correct_execution,noise_data):\n",
    "  id_num = str(rand.randint(1000,9999))\n",
    "  base_data = txt_pre_process(correct_execution,1,False,4)\n",
    "  base_data_noise = txt_pre_process(noise_data,0,False,4)\n",
    "\n",
    "  loop = 0\n",
    "\n",
    "  temp_correct_start=[]\n",
    "  temp_correct_wrong=[]\n",
    "\n",
    "  temp_wrong_store_train = []\n",
    "\n",
    "  data = base_data[0]\n",
    "  data_noise = base_data_noise[0]\n",
    "\n",
    "\n",
    "\n",
    "  print('')\n",
    "  print('----------------------------correct data augmentation ----------------------------')\n",
    "  aug = common_length_sequence(data)\n",
    "  aug2 = apply_z_score(aug,1)\n",
    "  aug3 = paddingV2(aug2)\n",
    "  aug4 = populate_0_input(aug3,data_noise)\n",
    "  aug4 = np.array(aug4)\n",
    "  aug4 = aug4.reshape(-1,len(aug3[0]),len(aug3[0][0]))\n",
    "  combined_inputs = np.concatenate((aug4,aug3), axis = 0)\n",
    "  combined_inputs = aug4\n",
    "  print('concat -> ', len(combined_inputs))\n",
    "\n",
    "\n",
    "  print('')\n",
    "  print('----------------------------data noise data augmentation ----------------------------')\n",
    "  aug_noise_data1 = paddingV2(data_noise,len(aug3[0]))\n",
    "  aug_noise_data2 = populate_0_input(aug_noise_data1,data_noise)\n",
    "  aug_noise_data3 = aug_noise_data2[0:len(combined_inputs)]\n",
    "  aug_noise_data4 = np.array(aug_noise_data3)\n",
    "  aug_noise_data5 = aug_noise_data4.reshape(-1,len(aug_noise_data4[0]),len(aug_noise_data4[0][0]))\n",
    "\n",
    "  # aug_noise_data6 = data_aug_sensitivity(aug4,aug_noise_data5,1,2,2)\n",
    "  aug_noise_data6 = data_aug_sensitivity(aug4,aug_noise_data5,1,2,int(len(combined_inputs)*.35))\n",
    "  aug_noise_data5 = np.concatenate((aug_noise_data6,aug_noise_data5), axis = 0)\n",
    "\n",
    "  print('aug_noise_data5--->',aug_noise_data5.shape)\n",
    "\n",
    "\n",
    "  loop = int(len(aug_noise_data5)/len(combined_inputs))\n",
    "\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(len(aug4[0]), return_sequences=True, activation='relu',  input_shape=(len(aug4[0]), len(aug4[0][0]))))\n",
    "  model.add(LSTM(len(aug4[0]) + int(len(aug4[0]) - int(len(aug4[0]) * .4)), return_sequences=True,  activation='relu'))\n",
    "  model.add(Bidirectional(LSTM(len(aug4[0]) - int(len(aug4[0]) - int(len(aug4[0]) * .4)), return_sequences=True, dropout=0.3, recurrent_dropout=0.3, activation='relu')))\n",
    "  model.add(LSTM(len(aug4[0]) - int(len(aug4[0]) - int(len(aug4[0]) * .4)), return_sequences=False, activation='relu'))\n",
    "  # model.add(BatchNormalization())\n",
    "  model.add(Dense(len(aug4[0]) - int(len(aug4[0]) - int(len(aug4[0]) * .3)), activation='relu'))\n",
    "  model.add(Dense(1,activation='sigmoid'))\n",
    "  custom_early_stopping = CustomEarlyStopping(accuracy_threshold=0.97, loss_threshold=0.05)\n",
    "  model.compile(optimizer = 'Adam' , loss = 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  for x in range(loop):\n",
    "    temp_wrong_store_train = aug_noise_data5[x*len(combined_inputs):len(combined_inputs)*(x+1)]\n",
    "\n",
    "    aug3_label = np.ones(len(combined_inputs))\n",
    "    aug_noise_label = np.zeros(len(temp_wrong_store_train))\n",
    "\n",
    "    rand_batches=concatenate_randomize_batches(combined_inputs,aug3_label,temp_wrong_store_train,aug_noise_label)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(rand_batches[0], rand_batches[1], test_size=0.3, random_state=42)\n",
    "\n",
    "    history=model.fit(X_train, y_train, epochs=200, batch_size =256  , validation_data=(X_test, y_test), callbacks=[custom_early_stopping])\n",
    "\n",
    "    val_loss = min(history.history['val_loss'])\n",
    "    val_accuracy = max(history.history['val_accuracy'])\n",
    "    temp_wrong_store_train=[]\n",
    "\n",
    "  X_train = X_train.astype(np.float32)\n",
    "  X_test = X_test.astype(np.float32)\n",
    "  model.save('testingModel')\n",
    "  convert_tf_to_tflite('/content/testingModel',[1,len(aug3[0]),len(aug3[0][0])], X_test,'whole_model',id_num,val_loss,val_accuracy)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "streamlined_process('/content/drive/MyDrive/Colab Notebooks/correct_new_2.txt','/content/drive/MyDrive/Colab Notebooks/wrong_new_2.txt')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for the individual models\n",
    "# this is experimental\n",
    "def streamlined_process(correct_execution,noise_data,loop=5):\n",
    "  id_num = str(rand.randint(1000,9999))\n",
    "  base_data = txt_pre_process(correct_execution,1,False,4)\n",
    "  base_data_noise = txt_pre_process(noise_data,0,False,4)\n",
    "\n",
    "  temp_correct_start=[]\n",
    "  temp_correct_wrong=[]\n",
    "\n",
    "  data = base_data[0]\n",
    "  data_noise = base_data_noise[0]\n",
    "\n",
    "\n",
    "\n",
    "  print('')\n",
    "  print('----------------------------correct data augmentation ----------------------------')\n",
    "  aug = common_length_sequence(data)\n",
    "  aug2 = apply_z_score(aug,1)\n",
    "  aug3 = paddingV2(aug2)\n",
    "  aug4 = populate_0_input(aug3,data_noise)\n",
    "  aug4 = np.array(aug4)\n",
    "  aug4 = aug4.reshape(-1,len(aug3[0]),len(aug3[0][0]))\n",
    "  combined_inputs = np.concatenate((aug4,aug3), axis = 0)\n",
    "  combined_inputs= aug4\n",
    "  print('concat -> ', len(combined_inputs))\n",
    "\n",
    "\n",
    "\n",
    "  print('')\n",
    "  print('----------------------------data noise data augmentation ----------------------------')\n",
    "  aug_noise_data1 = paddingV2(data_noise,len(aug3[0]))\n",
    "  aug_noise_data2 = populate_0_input(aug_noise_data1,data_noise)\n",
    "  aug_noise_data3 = aug_noise_data2[0:len(combined_inputs)]\n",
    "  aug_noise_data4 = np.array(aug_noise_data3)\n",
    "  aug_noise_data5 = aug_noise_data4.reshape(-1,len(aug_noise_data4[0]),len(aug_noise_data4[0][0]))\n",
    "\n",
    "  aug_noise_data6 = data_aug_sensitivity(aug4,aug_noise_data5,1,1,int(len(combined_inputs)*.35))\n",
    "  aug_noise_data5 = np.concatenate((aug_noise_data6,aug_noise_data5), axis = 0)\n",
    "  # aug_noise_data6 = data_aug_coor_sensitivity(aug4)\n",
    "  # aug_noise_data5 = np.concatenate((aug_noise_data6,aug_noise_data5), axis = 0)\n",
    "\n",
    "\n",
    "  correct_data_set = []\n",
    "  noise_data_set = []\n",
    "  rand_batches = []\n",
    "\n",
    "  print('testtesttesttest=======>',len(aug_noise_data5))\n",
    "\n",
    "\n",
    "  for x in range(14):\n",
    "    correct_data_set.append([])\n",
    "    noise_data_set.append([])\n",
    "    rand_batches.append([])\n",
    "\n",
    "\n",
    "  for exercise in combined_inputs:\n",
    "\n",
    "    left_upper_arm = []\n",
    "    left_lower_arm = []\n",
    "    left_hand = []\n",
    "    right_upper_arm = []\n",
    "    right_lower_arm = []\n",
    "    right_hand = []\n",
    "    left_upper_leg = []\n",
    "    left_lower_leg = []\n",
    "    left_feet = []\n",
    "    right_upper_leg = []\n",
    "    right_lower_leg = []\n",
    "    right_feet = []\n",
    "    head =[]\n",
    "    body = []\n",
    "\n",
    "\n",
    "\n",
    "#------------------------ generating the correct input of certain part------------------------------\n",
    "    for sequence in exercise:\n",
    "      # 11,13\n",
    "      left_upper_arm.append([sequence[22],sequence[23],sequence[26],sequence[27]])\n",
    "      # 13,15\n",
    "      left_lower_arm.append([sequence[26],sequence[27],sequence[30],sequence[31]])\n",
    "      # 15,17,19,21\n",
    "      left_hand.append([sequence[30],sequence[31],sequence[34],sequence[35],sequence[38],sequence[39],sequence[42],sequence[43]])\n",
    "\n",
    "      # 12,14\n",
    "      right_upper_arm.append([sequence[24],sequence[25],sequence[28],sequence[29]])\n",
    "      # 14,16\n",
    "      right_lower_arm.append([sequence[28],sequence[29],sequence[32],sequence[33]])\n",
    "      # 16,18,20,22\n",
    "      right_hand.append([sequence[32],sequence[33],sequence[36],sequence[37],sequence[40],sequence[41],sequence[44],sequence[45]])\n",
    "\n",
    "      # 23,25\n",
    "      left_upper_leg.append([sequence[46],sequence[47],sequence[50],sequence[51]])\n",
    "      # 25,27\n",
    "      left_lower_leg.append([sequence[50],sequence[51],sequence[54],sequence[55]])\n",
    "      # 27,29,31\n",
    "      left_feet.append([sequence[54],sequence[55],sequence[58],sequence[59],sequence[62],sequence[63]])\n",
    "\n",
    "      # 24,26\n",
    "      right_upper_leg.append([sequence[48],sequence[49],sequence[52],sequence[53]])\n",
    "      # 26,28\n",
    "      right_lower_leg.append([sequence[52],sequence[53],sequence[56],sequence[57]])\n",
    "      # 28,30,32\n",
    "      right_feet.append([sequence[56],sequence[57],sequence[60],sequence[61],sequence[64],sequence[65]])\n",
    "\n",
    "      # 11,12,23,24\n",
    "      body.append([sequence[22],sequence[23],sequence[24],sequence[25],sequence[46],sequence[47],sequence[48],sequence[49]])\n",
    "      # 7,8,9,10\n",
    "      head.append([sequence[14],sequence[15],sequence[16],sequence[17],sequence[18],sequence[19],sequence[20],sequence[21]])\n",
    "\n",
    "\n",
    "    correct_data_set[0].append(left_upper_arm)\n",
    "    correct_data_set[1].append(left_lower_arm)\n",
    "    correct_data_set[2].append(left_hand)\n",
    "\n",
    "    correct_data_set[3].append(right_upper_arm)\n",
    "    correct_data_set[4].append(right_lower_arm)\n",
    "    correct_data_set[5].append(right_hand)\n",
    "\n",
    "    correct_data_set[6].append(left_upper_leg)\n",
    "    correct_data_set[7].append(left_lower_leg)\n",
    "    correct_data_set[8].append(left_feet)\n",
    "\n",
    "    correct_data_set[9].append(right_upper_leg)\n",
    "    correct_data_set[10].append(right_lower_leg)\n",
    "    correct_data_set[11].append(right_feet)\n",
    "\n",
    "    correct_data_set[12].append(body)\n",
    "    correct_data_set[13].append(head)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  for x in range(len(correct_data_set)):\n",
    "    correct_data_set[x] = np.array(correct_data_set[x])\n",
    "\n",
    "  # for x in range(len(noise_data_set)):\n",
    "  #   noise_data_set[x] = np.array(noise_data_set[x])\n",
    "\n",
    "  print('len of correct data set ---->',len(correct_data_set[0]))\n",
    "  print('len of noise data set ---->',len(noise_data_set[0]))\n",
    "\n",
    "  correct_data_set_label = np.ones(len(combined_inputs))\n",
    "  noise_data_set_label = np.zeros(len(aug_noise_data5))\n",
    "\n",
    "  # for x in range(len(correct_data_set)):\n",
    "  #   rand_batches[x]=concatenate_randomize_batches(correct_data_set[x],correct_data_set_label,noise_data_set[x],noise_data_set_label)\n",
    "\n",
    "\n",
    "\n",
    "  loop = int(len(aug_noise_data5)/len(combined_inputs))\n",
    "\n",
    "\n",
    "  data_set_name=['left_upper_arm','left_lower_arm','left_hand','right_upper_arm','right_lower_arm','right_hand','left_upper_leg','left_lower_leg','left_feet','right_upper_leg','right_lower_leg','right_feet','head','body']\n",
    "  for x in range(len(data_set_name)-1):\n",
    "    print('progress -> ',x,'/',len(data_set_name)-1)\n",
    "\n",
    "\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_accuracy = 0.0\n",
    "    best_model = None\n",
    "\n",
    "    for y in range(loop):\n",
    "\n",
    "      noise_data_set=[]\n",
    "\n",
    "\n",
    "    #------------------------------ generating noise ---------------------------------------\n",
    "      for exercise in aug_noise_data5[len(correct_data_set[x])*y:len(correct_data_set[x]) + (len(correct_data_set[x])*y)]:\n",
    "      # for exercise in aug_noise_data5[0:450]:\n",
    "        # print('batchin noise ->',len(correct_data_set[x])*y,'<==--==>',len(correct_data_set[x]) + (len(correct_data_set[x])*y))\n",
    "\n",
    "\n",
    "        left_upper_arm = []\n",
    "        left_lower_arm = []\n",
    "        left_hand = []\n",
    "        right_upper_arm = []\n",
    "        right_lower_arm = []\n",
    "        right_hand = []\n",
    "        left_upper_leg = []\n",
    "        left_lower_leg = []\n",
    "        left_feet = []\n",
    "        right_upper_leg = []\n",
    "        right_lower_leg = []\n",
    "        right_feet = []\n",
    "        head =[]\n",
    "        body = []\n",
    "\n",
    "\n",
    "\n",
    "        for sequence in exercise:\n",
    "          # 11,13\n",
    "          # print(sequence[22])\n",
    "          if x == 0:\n",
    "            left_upper_arm.append([sequence[22],sequence[23],sequence[26],sequence[27]])\n",
    "          # 13,15\n",
    "          if x == 1:\n",
    "            left_lower_arm.append([sequence[26],sequence[27],sequence[30],sequence[31]])\n",
    "          # 15,17,19,21\n",
    "          if x == 2:\n",
    "            left_hand.append([sequence[30],sequence[31],sequence[34],sequence[35],sequence[38],sequence[39],sequence[42],sequence[43]])\n",
    "\n",
    "          # 12,14\n",
    "          if x == 3:\n",
    "            right_upper_arm.append([sequence[24],sequence[25],sequence[28],sequence[29]])\n",
    "          # 14,16\n",
    "          if x == 4:\n",
    "            right_lower_arm.append([sequence[28],sequence[29],sequence[32],sequence[33]])\n",
    "          # 16,18,20,22\n",
    "          if x == 5:\n",
    "            right_hand.append([sequence[32],sequence[33],sequence[36],sequence[37],sequence[40],sequence[41],sequence[44],sequence[45]])\n",
    "\n",
    "          # 23,25\n",
    "          if x == 6:\n",
    "            left_upper_leg.append([sequence[46],sequence[47],sequence[50],sequence[51]])\n",
    "          # 25,27\n",
    "          if x == 7:\n",
    "            left_lower_leg.append([sequence[50],sequence[51],sequence[54],sequence[55]])\n",
    "          # 27,29,31\n",
    "          if x == 8:\n",
    "            left_feet.append([sequence[54],sequence[55],sequence[58],sequence[59],sequence[62],sequence[63]])\n",
    "\n",
    "          # 24,26\n",
    "          if x == 9:\n",
    "            right_upper_leg.append([sequence[48],sequence[49],sequence[52],sequence[53]])\n",
    "          # 26,28\n",
    "          if x == 10:\n",
    "            right_lower_leg.append([sequence[52],sequence[53],sequence[56],sequence[57]])\n",
    "          # 28,30,32\n",
    "          if x == 11:\n",
    "            right_feet.append([sequence[56],sequence[57],sequence[60],sequence[61],sequence[64],sequence[65]])\n",
    "\n",
    "          # 11,12,23,24\n",
    "          if x == 12:\n",
    "            body.append([sequence[22],sequence[23],sequence[24],sequence[25],sequence[46],sequence[47],sequence[48],sequence[49]])\n",
    "          # 7,8,9,10\n",
    "          if x == 13:\n",
    "            head.append([sequence[14],sequence[15],sequence[16],sequence[17],sequence[18],sequence[19],sequence[20],sequence[21]])\n",
    "\n",
    "        if x == 0:\n",
    "          noise_data_set.append(left_upper_arm)\n",
    "        if x == 1:\n",
    "          noise_data_set.append(left_lower_arm)\n",
    "        if x == 2:\n",
    "          noise_data_set.append(left_hand)\n",
    "\n",
    "        if x == 3:\n",
    "          noise_data_set.append(right_upper_arm)\n",
    "        if x == 4:\n",
    "          noise_data_set.append(right_lower_arm)\n",
    "        if x == 5:\n",
    "          noise_data_set.append(right_hand)\n",
    "\n",
    "        if x == 6:\n",
    "          noise_data_set.append(left_upper_leg)\n",
    "        if x == 7:\n",
    "          noise_data_set.append(left_lower_leg)\n",
    "        if x == 8:\n",
    "          noise_data_set.append(left_feet)\n",
    "\n",
    "\n",
    "        if x == 9:\n",
    "          noise_data_set.append(right_upper_leg)\n",
    "        if x == 10:\n",
    "          noise_data_set.append(right_lower_leg)\n",
    "        if x == 11:\n",
    "          noise_data_set.append(right_feet)\n",
    "\n",
    "\n",
    "        if x == 12:\n",
    "          noise_data_set.append(body)\n",
    "        if x == 13:\n",
    "          noise_data_set.append(head)\n",
    "\n",
    "# ===================================================================================================================================================\n",
    "\n",
    "      print('correct->',len(correct_data_set[x]),'  incorrect->',len(noise_data_set[x]))\n",
    "\n",
    "      rand_batches=concatenate_randomize_batches(correct_data_set[x],correct_data_set_label,noise_data_set,noise_data_set_label)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "      print('loop -> ',y,'/',loop)\n",
    "      X_train, X_test, y_train, y_test = train_test_split(rand_batches[0], rand_batches[1], test_size=0.2, random_state=42)\n",
    "\n",
    "      # history = model.fit(X_train[y*len(rand_batches[x][0]):len(rand_batches[x][0])*(y+1)], y_train[y*len(rand_batches[x][1]):len(rand_batches[x][1])*(y+1)], epochs=200, batch_size =128 , validation_data=(X_test[y*len(rand_batches[x][0]):len(rand_batches[x][0])*(y+1)], y_test[y*len(rand_batches[x][1]):len(rand_batches[x][1])*(y+1)]), callbacks=[custom_early_stopping])\n",
    "\n",
    "      if y == 0:\n",
    "        model_base_modifier = 10\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(model_base_modifier+len(aug4[0]), return_sequences=True, activation='relu',  input_shape=(len(correct_data_set[x][0]), len(correct_data_set[x][0][0]))))\n",
    "        model.add(LSTM(model_base_modifier+len(aug4[0]) + int(len(aug4[0]) - int(len(aug4[0]) * .4)), return_sequences=True,  activation='relu' ))\n",
    "        model.add(Bidirectional(LSTM(model_base_modifier+len(aug4[0]) - int(len(aug4[0]) - int(len(aug4[0]) * .5)), return_sequences=True, dropout=0.4, recurrent_dropout=0.4, activation='relu')))\n",
    "        model.add(LSTM(model_base_modifier+len(aug4[0]) - int(len(aug4[0]) - int(len(aug4[0]) * .4)), return_sequences=False,  activation='relu'))\n",
    "        # model.add(BatchNormalization())\n",
    "        model.add(Dense(model_base_modifier+len(aug4[0]) - int(len(aug4[0]) - int(len(aug4[0]) * .4)), activation='relu'))\n",
    "\n",
    "        model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "        custom_early_stopping = CustomEarlyStopping(accuracy_threshold=0.97, loss_threshold=0.05)\n",
    "        model.compile(optimizer = 'Adam' , loss = 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "      history = model.fit(X_train, y_train, epochs=150, batch_size =128 , validation_data=(X_test, y_test), callbacks=[custom_early_stopping])\n",
    "\n",
    "      val_loss = min(history.history['val_loss'])\n",
    "      val_accuracy = max(history.history['val_accuracy'])\n",
    "\n",
    "      if val_loss < best_val_loss and val_accuracy > best_val_accuracy:\n",
    "        best_model = model.get_weights()\n",
    "        best_val_loss = val_loss\n",
    "        best_val_accuracy = val_accuracy\n",
    "\n",
    "      model.set_weights(best_model)\n",
    "\n",
    "\n",
    "\n",
    "    model.save('testingModel')\n",
    "\n",
    "\n",
    "    # X_train = X_train[x].astype(np.float32)\n",
    "    # X_test = X_test[x].astype(np.float32)\n",
    "\n",
    "    # convert_tf_to_tflite('/content/testingModel',[1,len(X_train[0]),len(X_train[0][0])], X_test,data_set_name[x],id_num,val_loss,val_accuracy)\n",
    "    # convert_tf_to_tflite('/content/testingModel',[1,len(correct_data_set[x][0]),len(correct_data_set[x][0][0])], X_test,data_set_name[x],id_num,val_loss,val_accuracy)\n",
    "\n",
    "\n",
    "    X_train = X_train.astype(np.float32)\n",
    "    X_test = X_test.astype(np.float32)\n",
    "    model.save('testingModel')\n",
    "\n",
    "    print('param 1 ->',len(noise_data_set[0]) )\n",
    "    print('param 1 ->',len(noise_data_set[0][0]))\n",
    "    print('current x ->',x)\n",
    "    convert_tf_to_tflite('/content/testingModel',[1,len(noise_data_set[0]),len(noise_data_set[0][0])], X_test,data_set_name[x],id_num,best_val_loss,best_val_accuracy)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    clear_output(wait=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "streamlined_process('/content/drive/MyDrive/Colab Notebooks/correct_new_2.txt','/content/drive/MyDrive/Colab Notebooks/wrong_new_2.txt')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
